<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Chapter2 模型评估与选择</title>
    <url>/2019/08/26/Chapter-2-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/</url>
    <content><![CDATA[<p>本文内容：</p>
<ul>
<li>经验误差与过拟合</li>
<li>评估方法<ul>
<li>留出法</li>
<li>交叉验证法</li>
<li>自助法</li>
<li>调参与最终模型</li>
</ul>
</li>
<li>性能度量<ul>
<li>错误率与精度</li>
<li>查准率、查全率与F1</li>
<li>ROC与AUC</li>
<li>代价敏感错误率与代价曲线</li>
</ul>
</li>
<li>比较检验<ul>
<li>偏差与方差</li>
</ul>
</li>
</ul>
<span id="more"></span>
<h2 id="2-1-经验误差与过拟合"><a href="#2-1-经验误差与过拟合" class="headerlink" title="2.1 经验误差与过拟合"></a>2.1 经验误差与过拟合</h2><p><strong>误差</strong>（error）：学习器的实际预测输出与样本的真实值之间的差异。在训练集上的误差被称为“<em>训练误差</em>”（training error）或“<em>经验误差</em>”（empirical error），在新样本上的误差称为“<em>泛化误差</em>”（generalization error）。</p>
<p>众所周知，在模型训练中，我们需要“泛化误差”尽可能的小，因此，在多数情况下，我们会倾向于训练出最小化“经验误差”的学习器，即“经验误差”$\rightarrow$0，对训练集的精度达到100%的情况。然而，事实上在多数情况下，这类模型的性能并不好，原因在于模型存在过拟合的情况。</p>
<p><strong>过拟合</strong>（overfitting）：对训练集数据训练过度，导致模型将一些训练集的自身特点作为了所有潜在样本的一般性质学习。与之相对的是“<em>欠拟合</em>”（underfitting），即对训练集的一般性质尚未学好。</p>
<p>欠拟合比较容易克服，在决策树学习中扩展分支；在神经网络中增加训练轮数等。过拟合则十分复杂，许多因素可能会导致过拟合。</p>
<p><strong>过拟合是机器学习中面临的重要障碍。</strong> 各类学习算法必然带有一些针对过拟合的措施，但是必须认识到的是：<em>过拟合是无法彻底避免的</em>。</p>
<p>理解这一点需要明白，机器学习解决的问题通常都是NP难甚至更难，如果可以彻底避免过拟合，仅通过经验误差最小化就能得到最优解，就意味着我们证明了“P=NP”，因此，过拟合显然是无法完全避免的。</p>
<h2 id="2-2-评估方法"><a href="#2-2-评估方法" class="headerlink" title="2.2 评估方法"></a>2.2 评估方法</h2><p>在得到学习器后，通常使用<em>测试集</em>（testing set）进行实验，以实验得到的“<em>测试误差</em>”（testing error）作为泛化误差的近似，从而对其泛化误差进行评估并进而作出选择。</p>
<p>需要注意的是，<strong>测试集应尽量与训练集互斥</strong>，从而避免过于“乐观”的测试结果。（<em>例：考试出原题</em>）</p>
<p>而我们仅有一个数据集$D$，就需要对数据集进行处理，从中产生测试集$S$和训练集$T$。</p>
<h3 id="2-2-1-留出法"><a href="#2-2-1-留出法" class="headerlink" title="2.2.1 留出法"></a>2.2.1 留出法</h3><p><strong>留出法</strong>（hold-out）：直接将数据集$D$划分为两个互斥的集合，一个作为训练集$T$，另一个作为测试集$S$。</p>
<p>需要注意的是，在划分过程中，不能破坏原本数据集的分布方式。这一过程类似于<em>采样</em>（sampling）中的“<em>分层采样</em>”（stratified）过程。</p>
<p>其次，因为在同一类别中抽样时的不同选择也会导致不同的训练/测试集，所以在使用留出法时，为了使结果更加可靠，往往会采用若干次随机划分、重复进行实验评估后取平均值作为留出法的结果。</p>
<p>此外，还需注意不能因为划分测试集导致训练集不足，欠拟合的情况发生。<em>一般地，我们将数据集的$\frac{2}{3}$~$\frac{4}{5}$作为训练集，余下部分为测试集。</em></p>
<h3 id="2-2-2-交叉验证法"><a href="#2-2-2-交叉验证法" class="headerlink" title="2.2.2 交叉验证法"></a>2.2.2 交叉验证法</h3><p><strong>交叉验证法</strong>（cross validation）：确定划分参数k，将数据集$D$划分为k个互斥集合，且每个集合大小相同或相似。每个子集$D_i$中的元素需保证依旧符合总体的分布规律。以k-1个子集的集合作为训练集，余下的子集作为测试集，重复k次，将所得结果取k次均值。（此方法又称为<em>k折交叉验证法</em>）</p>
<p><strong>留一法</strong>（Leave-One-Out，简称LOO）：在将每个元素看作一个子集时，作交叉验证，即为留一法。相较于常规交叉验证法，留一法的训练集更接近数据集$D$，但其有较大的计算开销，并且留一法结果不总是比常规交叉验证结果更准确。</p>
<h3 id="2-2-3-自助法"><a href="#2-2-3-自助法" class="headerlink" title="2.2.3 自助法"></a>2.2.3 自助法</h3><p><strong>自助法</strong>（bootstrapping）：在样本容量为m的数据集$D$中，利用<em>自助采样法</em>（bootstrap sampling）从$D$中抽取m个样本，构成集合$D’$。以$D’$为训练集，$D\setminus D’$为测试集，达到可以使用m个训练样本作为训练集。<em>自助采样法</em>，即有放回采样。</p>
<p>自助法的原理如下：</p>
<script type="math/tex; mode=display">\lim _{m \to \infty} (1-\frac{1}{m})^m = \frac{1}{e} \approx 0.368</script><p>构造m个样本的训练集$D’$后，余下部分$D \setminus D’$大约为数据集的$\frac{1}{3}$，以此作为测试集的测试结果被称为“<em>包外估计</em>”（out-of-bag estimate）。</p>
<p>显而易见，在数据集较小或需要多个不同训练集进行集成学习时自助法相对比较有用；但对于较大的数据集，划分训练集和测试集对结果影响较小，再使用自助法不仅作用甚微反而会因为提取训练集时导致训练集的分布与原分布不符，进而引入估计偏差。</p>
<h3 id="2-2-4-调参与最终模型"><a href="#2-2-4-调参与最终模型" class="headerlink" title="2.2.4 调参与最终模型"></a>2.2.4 调参与最终模型</h3><p><strong>调参</strong>（parameter tuning）：对算法参数进行设定的过程。</p>
<p>机器学习中的参数分为两类：</p>
<ol>
<li><strong>超参数</strong> 即算法的参数，供人为选择，通常数目在10以内。</li>
<li><strong>模型的参数</strong> 通过机器学习产生的参数，数目较多。</li>
</ol>
<p>显然，调参的过程与算法选择类似，因此，同样需要从数据集$D$中得到两个集合，此时为训练集和<em>验证集</em>（validation set），利用验证集上的性能进行模型选择或者调参。</p>
<h2 id="2-3-性能度量"><a href="#2-3-性能度量" class="headerlink" title="2.3 性能度量"></a>2.3 性能度量</h2><p><strong>性能度量</strong>（performance measure）：衡量模型泛化能力的标准。针对不同的模型，需要使用不同的性能度量。例：回归最常使用“均方误差”（mean squared error，简称MSE）作为性能度量。</p>
<p>样例集D上的MSE</p>
<script type="math/tex; mode=display">E(f;D) = \frac{1}{m}\sum^m_{i=1}(f(x_i)-y_i)^2</script><p>对于数据分布$\mathbb{D}$和概率密度函数$p(\cdot)$，MSE为</p>
<script type="math/tex; mode=display">E(f;\mathbb{D})=\int _{x \sim \mathbb{D}} (f(x)-y)^2p(x)dx</script><p>以下主要介绍分类任务的性能度量：</p>
<h3 id="2-3-1-错误率与精度"><a href="#2-3-1-错误率与精度" class="headerlink" title="2.3.1 错误率与精度"></a>2.3.1 错误率与精度</h3><p>分类任务中<strong>最常用</strong>的两种度量方式：<em>错误率、精度</em>。</p>
<p>错误率（error rate）：分类错误的样本数占样本总数的比例。<br>精度（accuracy）：分类正确的样本数占样本总数的比例。</p>
<p>对于样例集$D$，分类错误率定义为</p>
<script type="math/tex; mode=display">E(f;D) = \frac{1}{m} \sum^{m}_{i=1} \mathbb{I}(f(x) \neq y_i)</script><p>精度定义为</p>
<script type="math/tex; mode=display">acc(f;D) = \frac{1}{m} \sum^{m}_{i=1} \mathbb{I}(f(x) = y_i) = 1 - E(f;D)</script><p>注：$\mathbb{I}(\cdot)$为指示函数$\cdot$为真时取1，假时取0。</p>
<h3 id="2-3-2-查准率、查全率与F1"><a href="#2-3-2-查准率、查全率与F1" class="headerlink" title="2.3.2 查准率、查全率与F1"></a>2.3.2 查准率、查全率与F1</h3><p>对于二分类问题，将样例根据其真实类别与模型预测类别的组合划分为四类：</p>
<ul>
<li><strong>TP</strong> 真正例（true positive）</li>
<li><strong>FP</strong> 假正例（false positive）</li>
<li><strong>TN</strong> 真反例（true negative）</li>
<li><strong>FN</strong> 假反例（false negative）</li>
</ul>
<p>分类结果的混淆矩阵（confusion matrix）为：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">真实情况\预测结果</th>
<th style="text-align:center">正例</th>
<th style="text-align:center">反例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">正例</td>
<td style="text-align:center">TP（真正例）</td>
<td style="text-align:center">FN（假反例）</td>
</tr>
<tr>
<td style="text-align:center">反例</td>
<td style="text-align:center">FP（假正例）</td>
<td style="text-align:center">TN（真反例）</td>
</tr>
</tbody>
</table>
</div>
<p>则查准率（precision）P和查全率（recall）R分别定义为</p>
<script type="math/tex; mode=display">P = \frac{TP}{TP+FP},</script><script type="math/tex; mode=display">R = \frac{TP}{TP+FN}.</script><p>而查准率与查全率是一对矛盾的度量。一般地，查准率高时，查全率往往偏低；查全率高时，查准率偏低。</p>
<p>根据模型的预测结果，以样本是正例的可能性高低，对样本进行排序，并按此顺序依次以样本作为正例进行预测，从而可以计算得到不断更新的查全率和查准率。将其变化趋势绘制为图像，得到<em>查全率-查准率曲线</em>，简称<strong>P-R曲线</strong>，图像为P-R图。</p>
<p>在利用P-R图比较学习器优劣时，若一个学习器的P-R曲线完全包住另一个学习器的曲线，则可断言前者的性能优于后者；若有交叉部分，则难以一般性地断言优劣。</p>
<p>为综合考虑查准率和查全率，构造BEP和F1度量，F1度量更加常用。</p>
<p><strong>平衡点</strong>（Break-Even Point，简称BEP）：BEP = 查准率 = 查全率</p>
<p><strong>F1度量</strong>：</p>
<script type="math/tex; mode=display">F1 = \frac{2 \times P \times R}{P + R} = \frac{2 \times TP}{样例总数 + TP - TN}</script><p>针对于对查准率和查全率重视程度不同的情况，构造F1度量更一般的形式——$F_\beta$</p>
<script type="math/tex; mode=display">F_\beta = \frac{(1+\beta^2) \times P \times R}{(\beta^2 \times P) + R}</script><p><em>注：$F1$度量来源于查准率P和查全率R的调和平均数</em></p>
<p>其中$\beta \gt 0$度量了查准率对查重率的相对重要性，$\beta = 1$时退化为标准$F1$；$\beta \gt 1$时查全率有更大影响；$\beta \lt 1$时查准率有更大影响。</p>
<p><strong>多个二分类混淆矩阵情况</strong> 对多个二分类混淆矩阵综合考虑其查全率和查准率的问题（例如：对每次得到一个混淆矩阵的问题进行多次训练；在多个数据集上进行训练，希望估计算法的平均性能；执行多分类任务，每两两组合形成一个混淆矩阵），大致分为两种方法：</p>
<p><strong>macro-$F$1</strong> 首先计算各混淆矩阵的查准率和查全率，记作$(P_1,R_1)，(P_2,R_2)，\dots，(P_n,R_n)$，再分别计算查准率和查全率的均值，称为“<em>宏查准率</em>”（macro-P）和“<em>宏查全率</em>”（macro-R），并计算出对应的“宏$F1$”（macro-$F1$）</p>
<script type="math/tex; mode=display">macro-P = \frac{1}{n}\sum^n_{i=1}Pi,</script><script type="math/tex; mode=display">macro-R = \frac{1}{n}\sum^n_{i=1}Ri,</script><script type="math/tex; mode=display">macro-F1 = \frac{2 \times macro-P \times macro-R}{macro-P + macro-R}.</script><p><strong>micro-$F$1</strong> 首先计算各混淆矩阵的均值，即求出$\overline {TP}，\overline {T}，\overline {FP}，$</p>
<h3 id="2-3-3-ROC与AUC"><a href="#2-3-3-ROC与AUC" class="headerlink" title="2.3.3 ROC与AUC"></a>2.3.3 ROC与AUC</h3><p>在学习器对测试数据预测时，一般是产生一个实值或概率值，再与一个<em>分类阈值</em>（threshold）比较，以此决定最终的判断结果。等价于将测试结果实值排序，再规定一个<em>截断点</em>（cut point），截断点前后即为正例和反例。</p>
<p>而这个排序的质量，就反映出学习器的期望泛化性能（在一般情况下的性能），常用ROC曲线反应这一指标。</p>
<p><strong>ROC曲线</strong>（Receiver Operating Characteristic）：受试者工作曲线，和P-R曲线的绘制方法类似，使用TPR和FPR代替P-R曲线中的P和R即可得到ROC曲线。</p>
<p><strong>真正例率</strong>（True Positive Rate，简称TPR）：</p>
<script type="math/tex; mode=display">TPR = \frac{TP}{TP+FN}</script><p>注：<em>其中 TP+FN=样本中所有正例=最大正例个数</em></p>
<p><strong>假正例率</strong>（False Positive Rate，简称FPR）：</p>
<script type="math/tex; mode=display">FPR = \frac{FP}{TN+FP}</script><p>注：<em>其中 TN+FP=样本中所有反例=最大反例个数</em></p>
<p>使用ROC对学习器的泛化性能进行比较时，类似于P-R曲线，若一个学习器的曲线完全被另一个学习器的曲线包含，则可断定后者性能优于前者；若两曲线有重叠部分，则需要使用AUC（Area Under ROC Curve）进行判定。</p>
<h3 id="2-3-4-代价敏感错误率与代价曲线"><a href="#2-3-4-代价敏感错误率与代价曲线" class="headerlink" title="2.3.4 代价敏感错误率与代价曲线"></a>2.3.4 代价敏感错误率与代价曲线</h3><p>为衡量不同错误所造成的不同损失，可为错误赋予“<em>非均等代价</em>”（unequal cost）。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">真实类别\预测类别</th>
<th style="text-align:center">第0类</th>
<th style="text-align:center">第1类</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">第0类</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$cost_{01}$</td>
</tr>
<tr>
<td style="text-align:center">第1类</td>
<td style="text-align:center">$cost_{10}$</td>
<td style="text-align:center">0</td>
</tr>
</tbody>
</table>
</div>
<p>以二分类事件为例，设定以上的“<em>代价矩阵</em>”（cost matrix）。可以通过改变$cost<em>{01}$和$cost</em>{10}$的值，描述不同错误的不同损失情况。</p>
<p>由此，引入一系列代价敏感的指标，如：代价敏感错误率，以$D^{+}$为正例子集$D^{-}$为反例子集，第0类为正例，第1类为反例，则</p>
<script type="math/tex; mode=display">E(f;D;cost) = \frac{1}{m}(\sum_{x_i \in D^+} \mathbb{I}(f(x_i) \neq y_i) \times cost_{01} + \sum_{x_i \in D^-} \mathbb{I}(f(x_i) \neq y_i) \times cost_{10})</script><p>需要注意的是，在代价敏感情况下，不能直接用ROC曲线判断两学习器的泛化性能，需要引入“<em>代价曲线</em>”（cost curve）进行判断。</p>
<p><strong>代价曲线</strong>（cost curve）：横轴为取值在[0,1]区间的正例概率代价</p>
<script type="math/tex; mode=display">P(+)cost = \frac{p \times cost_{01}}{p \times cost_{01} + (1-p) \times cost_{10}}</script><p>p是样例为正例的概率；纵轴是取值为[0,1]的归一化代价</p>
<script type="math/tex; mode=display">cost_{norm} = \frac{FNR \times p \times cost_{01} + FPR \times (1-p) \times cost_{10}}{p \times cost_{01} + (1-p) \times cost_{10}}</script><h2 id="2-4-比较检验"><a href="#2-4-比较检验" class="headerlink" title="2.4 比较检验"></a>2.4 比较检验</h2><p>机器学习中性能比较涉及以下因素：</p>
<ol>
<li>测试性能无法完全代表泛化性能</li>
<li>测试性能与测试集的选取有非常大的关系</li>
<li>许多机器学习算法存在一定随机性</li>
</ol>
<p>因此，在机器学习中比较两学习器的性能时需要用到统计假设检验（hypothesis test）的方法对上述因素进行规避，以得到较为准确的结论。</p>
<p>—<em>由于内容较为繁琐，此处略去</em>—</p>
<h3 id="偏差与方差"><a href="#偏差与方差" class="headerlink" title="偏差与方差"></a>偏差与方差</h3><p>通过“<em>偏差-方差分解</em>”（bias-variance decomposition）可以对一个学习算法泛化性能进行解释。</p>
<p>对测试样本$x$，令$y_D$为$x$在数据集中的标记，$y$为$x$的真实标记，$f(x;D)$为训练集$D$上学得模型$f$在$x$上的预测输出。以回归预测为例，学习算法的期望预测为</p>
<script type="math/tex; mode=display">\bar f(x) = E_D[f(x;D)],</script><p>使用样本数相同的不同训练集产生的方差为</p>
<script type="math/tex; mode=display">var(x) = \mathbb{E}_D[(f(x;D) - \bar f(x))^2],</script><p>噪声为</p>
<script type="math/tex; mode=display">\varepsilon ^2 = \mathbb{E}_D[(y_D-y)^2]</script><p>期望输出与真实标记的差别称为偏差（bias），即</p>
<script type="math/tex; mode=display">bias^2(x) = (\bar f(x) - y)^2q</script><p>为便于讨论就，假定噪声期望为零（噪声与学习算法无关），则$\mathbb{E}_D[(y_D-y)^2]=0$，对算法期望泛化误差分解得</p>
<script type="math/tex; mode=display">E(f;D)=\mathbb{E}_D[(f(x;D)-y_D)^2]=bias^2(x)+var(x)+\varepsilon^2</script><p>所以，泛化误差可以分解为偏差、方差与噪声之和。通过偏差-方差分解可以得知，偏差度量了学习算法预测值与真是值之间的差异，刻画学习算法本身的拟合能力；方差度量了同样大小的训练集的变动导致的学习性能的变化，刻画数据扰动所造成的影响；噪声表达了在当前任务上任何算法所能达到的下界，刻画了学习问题本身的难度。</p>
<p>对于一个学习算法，在偏差-方差分解中可知，想要提升其性能，需要对数据进行更好的拟合（减少偏差）；数据扰动的影响小（减小方差）。然而，在正常情况下，存在<em>偏差-方差窘境</em>（bias-variance dilemma）</p>
<p><strong>偏差-方差窘境</strong> 在欠拟合时，拟合程度不高致使偏差较大，数据扰动能力弱，泛化误差由偏差主导；过拟合时，拟合程度过高致使方差较大，数据扰动能力强，泛化误差由方差主导。</p>
]]></content>
      <tags>
        <tag>《机器学习》</tag>
      </tags>
  </entry>
  <entry>
    <title>Chapter10 降维与度量学习</title>
    <url>/2019/10/05/Chapter10%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>本文内容：</p>
<ul>
<li>kk近邻学习</li>
<li>低维嵌入</li>
<li>主成分分析<ul>
<li>最近重构性</li>
<li>最大可分性</li>
</ul>
</li>
<li>核化线性降维</li>
<li>流形学习<ul>
<li>等度量映射</li>
<li>局部线性嵌入</li>
</ul>
</li>
<li>度量学习</li>
</ul>
<span id="more"></span>
<h2 id="10-1-k-近邻学习"><a href="#10-1-k-近邻学习" class="headerlink" title="10.1 $k$近邻学习"></a>10.1 $k$近邻学习</h2><p><strong>$k$近邻</strong>（$k$-Nearest Neighbor，简称kNN）：是一种常用的监督学习方法。其在给定测试样本的情况下，基于某种距离度量找出训练集中与其最靠近的$k$个“近邻”的样本点信息进行预测。通常，对分类任务采用“投票法”；在回归任务中使用“平均法”；还可基于距离进行加权，应用于投票或回归。</p>
<p><strong>懒惰学习</strong>（lazy learning）：在训练阶段仅仅是把样本保存起来，训练时间开销为零，在收到测试样本后再进行处理。</p>
<p><strong>急切学习</strong>（eager learning）：在训练阶段就对样本进行学习处理的方法。</p>
<p>$k$近邻学习是“<em>懒惰学习</em>”（lazy learning）的著名代表。因为它与其他学习方法相比有一个明显的不同之处：它没有显式的训练过程。</p>
<p><img src="https://i.loli.net/2019/10/02/wjOdJsx6cg5qbv2.png" alt="$k$近邻分类器示意图"></p>
<p>显然，$k$是一个重要参数，显著决定分类结果。同时，采用不同的距离计算方法，“近邻”可能会有显著差别，从而会导致分类结果显著不同。</p>
<p>假定距离计算是恰当的情况下，讨论最邻近分类器（1NN，即$k=1$）在二分类任务上的性能。</p>
<p>对给定测试样本$x$，有最近邻样本$z$，则最近邻分类器出错的概率即$x$的标记与$z$不同的概率</p>
<script type="math/tex; mode=display">P(err) = 1 - \sum_{c \in \gamma}P(c|x)P(c|z)</script><p>假设样本独立同分布，且对任意$x$和任意正小数$\delta$，在$x$附近$\delta$距离范围内总能找到一个训练样本$z$。令$c^*=\argmax_{c \in \gamma}P(c|x)$表示贝叶斯最优分类的结果，则</p>
<script type="math/tex; mode=display">\begin{aligned} P(err) &= 1 - \sum_{c \in \gamma}P(c|x)P(c|z) \\ &\simeq 1 - \sum_{c \in \gamma}P^2(c|x) \\ &\leqslant 1 - P^2(c^* |x)\\ &= \left ( 1 + P(c^* |x) \right ) \left ( 1 - P(c^* |x) \right ) \\ &\leqslant 2\times \left ( 1 - P(c^*|x) \right ) \end{aligned}</script><p>于是，可以得出结论：虽然$k$近邻分类器简单，但其错误率不会超过最优贝叶斯分类器错误率的2倍。</p>
<h2 id="10-2-低维嵌入"><a href="#10-2-低维嵌入" class="headerlink" title="10.2 低维嵌入"></a>10.2 低维嵌入</h2><p>在上节对$k$近邻分类器性能的分析中，对样本作出假设：对任意$x$和任意正小数$\delta$，在$x$附近$\delta$范围内总能找到一个训练样本$z$，即样本密度足够大，这被称为<em>密采样</em>（density sample）。在现实中，对均匀分布的样本点归一化后，若假定$\delta = 0.001$，即需要1000个样本点，但这仅仅是对于单属性条件而言，若在20个属性的条件下，则需要样本点的个数即为$(10^3)^{20} = 10^{60}$，显然，这是无法接受的。</p>
<p>所以，发生这种在高维情形下出现的数据样本稀疏、距离计算困难的等问题时，需要辅以方法缓解“<em>维数灾难</em>”（curse of dimensionality）。其中，一个重要的途径是<em>降维</em>（dimension reduction），亦称“维数约简”。</p>
<p>降维将通过某些数学变换将原始高维属性空间转变为一个低维“<em>子空间</em>”（subspace），从而使子空间中样本密度大幅提高，同时简化了距离计算。这针对于需要面对高维的数据样本，却仅有低维分布与其学习任务密切相关的情况，此时被称为高维空间中的一个低维“<em>嵌入</em>”（embedding）。</p>
<p><strong>多维缩放</strong>（Multiple Dimension Scaling，简称MDS）：一种经典的降维方法，可以在低维空间中保持原始空间中样本之间的距离。以下作简单介绍：</p>
<p>假定$m$个样本在原始空间中的距离矩阵维$D \in R^{m\times m}$，其第$i$行第$j$个元素$\text{dist}<em>{ij}$为样本$x_i$到$x_j$的距离。目标是获得样本在$d’$维空间中的表示$Z \in R^{d’ \times m}$，$d’ \leqslant d$，且任意两样本在$d’$维空间中的欧氏距离等于原始空间中的距离，即$\left | z_i - z_j \right | =\text{dist}</em>{ij}$。</p>
<p>令$B=Z^TZ \in R^{m\times m}$，$B$是样本降维后对应的内积矩阵，$b_{ij} = z_i^Tz_j$，从而有</p>
<script type="math/tex; mode=display">\begin{aligned} \text{dist}_{ij}^2 &= \left \| z_i \right \|^2 + \left \| z_j \right \|^2 - 2 z_i^T z_j \\ &= b_{ii} + b_{jj} - 2b_{ij} \end{aligned}</script><p>为便于后续讨论，中心化降维后的样本$Z$，即有$\sum^m<em>{i=1}z_i=0$。显然，此时矩阵$B$的行和列之后均为零，$\sum^m</em>{j=1} b<em>{ij} = \sum^m</em>{i=1} b_{ij} = 0$，则可知</p>
<script type="math/tex; mode=display">\begin{aligned} \sum^m_{i=1} \text{dist}_ {ij}^2 = \sum^m_{i=1}(b_{ii} + b_{jj} - 2b_{ij}) = \text{tr}(B) + mb_{jj} \\ \sum^m_{j=1} \text{dist}_ {ij}^2 = \sum^m_{j=1}(b_{ii} + b_{jj} - 2b_{ij}) = \text{tr}(B) + mb_{ii} \\ \sum^m_{i=1}\sum^m_{j=1} \text{dist}_ {ij}^2 = \sum^m_{i=1}\sum^m_{j=1}(b_{ii} + b_{jj} - 2b_{ij}) = 2m\text{tr}(B) \end{aligned}</script><p>进一步可知</p>
<script type="math/tex; mode=display">\begin{aligned} \text{dist}_ {i\cdot}^2 = \frac{1}{m} \sum^m_{j=1}\text{dist}_ {ij}^2 = \frac{\text{tr}(B)}{m} + b_{jj} \\ \text{dist}_ {\cdot j}^2 = \frac{1}{m}\sum^m_{i=1}\text{dist}_ {ij}^2 = \frac{\text{tr}(B)}{m} + b_{ii} \\ \text{dist}_ {\cdot \cdot}^2 = \frac{1}{m^2}\sum^m_{i=1}\sum^m_{j=1}\text{dist}_ {ij}^2 = 2\frac{\text{tr}(B)}{m} \end{aligned}</script><p>所以有</p>
<script type="math/tex; mode=display">b_{ij} = \frac{1}{2} (\text{dist}_ {i\cdot}^2+\text{dist}_ {\cdot j}^2) - \text{dist}_ {\cdot \cdot}^2 - \text{dist}_ {ij}^2</script><p>由上式即可通过距离矩阵$D$求得内积矩阵$B$。</p>
<p>对矩阵$B$进行<em>特征值分解</em>（eigenvalue decomposition），$B=V\Lambda V^T$，其中$\Lambda=\text{diag}(\lambda<em>1,\lambda_2,\dots,\lambda_d)$为特征值构成的对角矩阵，$\lambda_1 \geqslant \lambda_2 \geqslant \dots \geqslant \lambda_d$，$V$为特征向量矩阵。假定其中有$d^*$个非零特征值，构成对角矩阵$\Lambda</em> <em>=\text{diag}(\lambda<em>1,\lambda_2, \dots,\lambda</em>{d^ </em>})$，$V_ *$表示相应的特征向量矩阵，则$Z$为</p>
<script type="math/tex; mode=display">Z = \Lambda_*^{\frac{1}{2}}V_*^T \in R^{d^* \times m}</script><p>在现实应用中为了有效降维，通常不会要求距离的严格相等，因此，可取$d’ \ll d$个最大特征值构成对角矩阵$\tilde{\Lambda} = \text{diag}(\lambda<em>1,\lambda_2,\dots,\lambda</em>{d’})$，$\tilde{V}$是相应的特征向量矩阵，则$Z$为</p>
<script type="math/tex; mode=display">Z = \tilde{\Lambda}^{\frac{1}{2}}\tilde{V}^T \in R^{d'\times m}</script><p>一般而言，为得到原始子空间，最简单的方法是对原始高维空间进行线性变换，对给定$d$维空间中的样本$x = (x_1,x_2,\dots,x_m) \in R^{d \times m}$，变换后得到$d’ \leqslant d$维空间中的样本</p>
<script type="math/tex; mode=display">Z = W^T X</script><p>其中，$W\in R^{d\times d’}$是变换矩阵，$Z \in R^{d’ \times m}$是降维后的矩阵。变化矩阵可以看作$d’$个$d$维基向量，$z<em>i=W^Tx_i$是第$i$个样本与这$d’$个基向量分别作内积而得到的$d’$维属性基向量，即$z_i$是原属性$x_i$在新坐标系${w_1,w_2,\dots,w</em>{d’}}$中的坐标向量，若$w_i$和$w_j$（$i \neq j$）正交，则新坐标系是一个正交坐标系，$W$为正交变换。这种基于线性变换的降维方法称为线性降维方法。</p>
<h2 id="10-3-主成分分析"><a href="#10-3-主成分分析" class="headerlink" title="10.3 主成分分析"></a>10.3 主成分分析</h2><p><strong>主成分分析</strong>（Principal Component Analysis，简称PCA）：是最常用的一种降维方法。使用一个超平面对正交属性空间内的所有样本点进行恰当的表达，即将所有样本点投影到一个超平面上，这个超平面大概需要如下性质：</p>
<ul>
<li>最近重构性：样本点到这个超平面的距离足够近；</li>
<li>最大可分性：样本点在这个超平面上的投影尽可能分开；</li>
</ul>
<p>基于以上两个性质，可分别得到主成分分析的两种等价推导。</p>
<h3 id="最近重构性"><a href="#最近重构性" class="headerlink" title="最近重构性"></a>最近重构性</h3><p>假定数据已中心化，则有$\sum^m<em>{i=1}x_i=0$，假定变换后的新坐标系为${w_1,w_2,\dots,w_d}$，其中$w_i$是标准正交基向量，$\left | w_i \right |_2 = 1$，$w_i^Tw_j=0$（$i\neq j$）。丢弃新坐标系中的部分坐标，使维度降到$d’ &lt; d$，则样本点$x_i$在$d’$维上的投影为$z=(z</em>{i1},z<em>{i2},\dots,z</em>{id’})$，其中$z<em>{ij}=w_j^Tx_i$是$x_i$在低维$j$下的投影，基于$z_i$重构$x_i$可得到$\hat x_i = \sum^{d’}</em>{j=1}z _{ij}w_j$。</p>
<p>考虑整个样本集，原样本点与其基于投影重构的样本点$\hat x_i$之间的距离为</p>
<script type="math/tex; mode=display">\begin{aligned} \sum^m_{i=1} \left \| \sum^{d'}_{j=1}z _{ij}w_j-x_i \right \|^2_2 &= \sum^m_{i=1} \left \| Wz_i - x_i \right \|_2^2 \\ &= \sum^m _{i=1}(Wz_i)^TWz_i - 2\sum^m_{i=1}z_i^TW^Tx_i + \sum^m _{i=1}x_i^Tx_i \\ &= \sum^m _{i=1}z_i^TW^TWz_i - 2\sum^m _{i=1}z_i^TW^TW^{-1T}z_i + \text{const} \\ &= -\sum^m _{i=1}z_i^Tz_i + \text{const} \\ &\propto -\text{tr}\left (W^T \left (\sum^m _{i=1}x_i^Tx_i \right )W \right ) \end{aligned}</script><p>其中$W = {w<em>1,w_2,\dots,w_d}$，考虑$w_j$是标准正交基，$\frac{1}{m-1}\sum^m</em>{i=1} x_ix_i^T$是协方差矩阵，则根据上式可得主成分分析的优化目标为</p>
<script type="math/tex; mode=display">\begin{aligned} \min _{W} \ \  -\text{tr}(W^TXX^TW) \\ \text{s.t.  } W^TW=I. \end{aligned}</script><h3 id="最大可分性"><a href="#最大可分性" class="headerlink" title="最大可分性"></a>最大可分性</h3><p>使样本点$x_i$在新空间超平面上的投影$W^Tx_i$的方差最大化，投影后的样本点的协方差阵为$\sum_i W^Tx_ix_I^TW$，于是优化目标为</p>
<script type="math/tex; mode=display">\begin{aligned} \max _{W} \ \  \text{tr}(W^TXX^TW) \\ \text{s.t.  } W^TW=1. \end{aligned}</script><p>显然，与前文最近重构法所得结论等价。</p>
<p>利用拉格朗日乘子法得</p>
<script type="math/tex; mode=display">XX^Tw_i = \lambda_i w_i</script><p>之后，对协方差阵$XX^T$进行特征值分解，使$\lambda<em>1 \geqslant \lambda_2 \geqslant,\dots,\geqslant \lambda_d$，取前$d’$个特征值对应的特征向量，构成$W^*=(w_1,w_2,\dots,w</em>{d’})$，即取得PCA的解。</p>
<p>PCA仅需保留$W^*$与样本的均值向量即可通过简单的向量减法和矩阵-向量乘法将原数据集降维。在降维过程中有$(d-d’)$个最小的特征值被舍弃，一方面，舍弃这部分信息可使样本的采样密度增大（重要动机）；另一方面，这些小的特征值通常与噪声相关，舍弃它们在一定程度上可以起到降噪的作用。</p>
<h2 id="10-4-核化线性降维"><a href="#10-4-核化线性降维" class="headerlink" title="10.4 核化线性降维"></a>10.4 核化线性降维</h2><p>在线性降维方法中，假设从高维空间到低维空间的函数映射是线性的，但在现实任务中，很多情况都需要非线性映射才能找到恰当的低维嵌入。对于这种情况，如果直接使用线性降维方法，将会丢失原本的低维结构。原本采样的低维空间称为“<em>本真</em>”（intrinsic）低维空间。</p>
<p>非线性降维的一种常见方法，即是基于核技巧对线性降维方法进行<em>核化</em>（kernelized）。以下以<em>核主成分分析</em>（Kernelized Principle Component Analysis，简称KPCA}）为例：</p>
<p>假定将高维特征空间中的数据投影到由$W=(w_1,w_2,\dots,w_d)$确定的超平面上，则对于$w_j$由PCA中引入拉格朗日乘子的式子有</p>
<script type="math/tex; mode=display">\left ( \sum^m_{i=1}z_iz_i^T \right ) w_j = \lambda_jw_j</script><p>其中$z_i$是样本点$x_i$在高维特征空间中的像。易知</p>
<script type="math/tex; mode=display">\begin{aligned} w_j &= \frac{1}{\lambda_j}\left ( \sum^m_{i=1} z_iz_i^T \right )w_j = \sum^m_{i=1}z_i \frac{z_i^Tw_j}{\lambda_j} \\ &= \sum^m_{i=1}z_i\alpha_i^j \end{aligned}</script><p>其中$\alpha_i^j = \frac{1}{\lambda_j}z_i^Tw_j$是$\alpha_i$的第$j$个分量。假定$z_i$是由原始属性空间中的样本点$x_i$通过映射$\phi$产生，即$z_i = \phi(x_i)$，$i=1,2,\dots,m$。</p>
<ul>
<li><p>如果$\phi$可以被显式地表达出来，则通过它将样本映射至高维特征空间，再在特征空间中实施PCA即可。则有<script type="math/tex">\begin{aligned} \left ( \sum^m_{i=1}\phi(x_i)\phi(x_i)^T \right ) w_j = \lambda_jw_j \\ w_j=\sum^m_{i=1} \phi(x_i)\alpha_i^j \end{aligned}</script></p>
</li>
<li><p>但在一般情况下，难以显式表达出$\phi$的具体形式，所以引入核函数<script type="math/tex">\kappa(x_i,x_j) = \phi(x_i)^T\phi(x_j)</script>进一步有<script type="math/tex">K\alpha^j=\lambda_j\alpha^j</script>其中$K$为$\kappa$对应的核矩阵，$(K)_{ij}=\kappa(x_i,x_j)$，$\alpha_j=(\alpha^j_1;\alpha^j_2;\dots;\alpha^j_m)$。显然，对上式进行特征值分解，即可取得相对应的特征向量。</p>
</li>
</ul>
<p>对新样本$x$，其投影后的第$j$（$j=1,2,\dots,d’$）维坐标为</p>
<script type="math/tex; mode=display">\begin{aligned} z_j &= w^T_j\kappa(x) = \sum^m_{i=1}\alpha_i^j\kappa(x_i)^T\kappa(x) \\ &= \sum^m_{i=1}\alpha_i^j\kappa(x_i,x) \end{aligned}</script><p>其中$\alpha_i$已经过规范化。从最终式中可以看出，为计算投影后的坐标需要对所有样本进行求和，所以它的计算开销较大。</p>
<h2 id="10-5-流形学习"><a href="#10-5-流形学习" class="headerlink" title="10.5 流形学习"></a>10.5 流形学习</h2><p><strong>流形学习</strong>（manifold learning）：是一类借鉴了拓扑流形概念的降维方法。“流形”是指在局部与欧氏空间同胚的空间，即它在局部具有欧氏空间的性质，可以用欧氏距离进行距离计算。</p>
<p>由此对降维带来启发：可以在局部建立降维映射关系，这样一来，即便其在原始空间内的分布再复杂，只要在局部仍具有欧氏空间性质，依旧可以利用推广到全局的局部映射关系完成映射。</p>
<h3 id="10-5-1-等度量映射"><a href="#10-5-1-等度量映射" class="headerlink" title="10.5.1 等度量映射"></a>10.5.1 等度量映射</h3><p><strong>等度量映射</strong>（Isometric Mapping，简称Isomap）：基本出发点是，认为在低维嵌入高维后，由于高维空间中的直线距离在低维空间中是不可达的，所以高维空间中的直线距离存在误导性。主张以“<em>测地线</em>”（geodesic）作为低维嵌入流形的两点间距离，即在曲面上两点间的距离，同时是两点之间的本真距离。</p>
<p>计算测地线距离，需要在流形的局部寻找近邻点，通过近邻点间的距离可用欧氏距离计算的性质，建立一个近邻连接图。在近邻连接图上计算得到的最短距离即为测地线距离。</p>
<p>对于近邻连接图上的测地线距离，可利用Dijktra算法或Floyd算法计算。在得到距离后，可通过MDS方法对样本点进行降维处理，得到样本点的低维坐标。最后，以样本点的高维坐标为输入，低维坐标为输出，训练一个回归学习器用于对新样本的低维空间进行预测。显然，这仅是权宜之计，但目前还没有更好的方法。</p>
<p>对近邻图的构建有两种方法：</p>
<ul>
<li>指定近邻点个数，指定最近的$k$个样本点为近邻点，得到的近邻图为$k$近邻图；</li>
<li>指定距离阈值$\epsilon$，距离小于$\epsilon$的样本点即为近邻点，得到的近邻图为$\epsilon$近邻图。</li>
</ul>
<p>以上两种方法均存在不足，因此使用时需谨慎，避免给后续计算造成误导。</p>
<h3 id="10-5-2-局部线性嵌入"><a href="#10-5-2-局部线性嵌入" class="headerlink" title="10.5.2 局部线性嵌入"></a>10.5.2 局部线性嵌入</h3><p><strong>局部线性嵌入</strong>（Locally Linear Embedding，简称LLE）：LLE试图保持邻域内样本点之间的线性关系。</p>
<p>LLE先为每个样本$x$找到其近邻下标集合$Q_i$，然后计算出基于$Q_i$的样本点对$x_i$进行线性重构的系数$w_i$：</p>
<script type="math/tex; mode=display">\begin{aligned}\min_{w_1,w_2,\dots,w_m} &\sum^m_{i=1}\left \| x_i - \sum_{j\in Q_i} w_{ij}x_j \right \|^2_2  \\ &\text{s.t.  } \sum_{j \in Q_i} w_{ij} = 1 \end{aligned}</script><p>其中，$x<em>i$和$x_j$均为已知，令$C</em>{jk} = (x<em>i - x_j)^T(x_i-x_k)$，$w</em>{ij}$有闭式解</p>
<script type="math/tex; mode=display">w_{ij} = \frac{\sum_{k \in Q_i} C_{jk}^{-1}}{\sum_{l,s \in Q_i} C_{ls}^{-1}}</script><p>由于目标是保持其邻域内样本点之间的线性关系。所以LLE在低维空间中保持$w_i$不变，那么，$x_i$对应的低维空间坐标即可通过下式求解：</p>
<script type="math/tex; mode=display">\min_{z_1,z_2,\dots,z_m} \sum^m_{i=1} \left \| z_i - \sum_{j \in Q_i} w_{ij}z_j \right \|^2_2</script><p>不难发现，该式与求解线性重构系数$w<em>i$的目标同形，唯一的区别在于需要确定的对象。于是，可令$Z = (z_1,z_2,\dots,z_m) \in R^{d’ \times m}$，$(W)</em>{ij} = w _{ij}$，</p>
<script type="math/tex; mode=display">M = (I - W)^T(I - W)</script><p>则可将目标重写为</p>
<script type="math/tex; mode=display">\begin{aligned} &\min_Z \text{  tr} (ZMZ^T) \\ &\text{  s.t.  } ZZ^T = I \end{aligned}</script><p>显然，可以通过特征值分解求解，$M$最小的$d’$个特征值对应的特征向量组成的矩阵即为$Z^T$。</p>
<h2 id="10-6-度量学习"><a href="#10-6-度量学习" class="headerlink" title="10.6 度量学习"></a>10.6 度量学习</h2><p>前面对数据进行降维处理的主要目的还是在于，减少数据的维度避免维数灾难，从而方便距离的计算。那么，如果可以直接进行距离学习，找到一个合适的距离度量，即不必再对数据进行降维了。这就是<em>度量学习</em>（metric learning）的基本动机。</p>
<p>如果要进行度量学习，必须要先有一个固定的距离度量表达形式，目前还没有接触过附带参数的距离度量表达式，因此它们无法通过对数据集的学习进行改进，所以需要先做一个推广。</p>
<p>对两个$d$维样本$x_i$和$x_j$，它们之间的平方欧氏距离可以写作</p>
<script type="math/tex; mode=display">\text{dist}^2 _{ed}(x_i,x_j) = \left \| x_i - x_j \right \|^2_2 = \sum^d_{k=1} dist^2_{ij,k}</script><p>其中，$dist^2_{ij,k}$表示$x_i$与$x_j$在第$k$维上的距离。若假定不同属性的重要性不同，则可引入权重变量$w$，从而得到</p>
<script type="math/tex; mode=display">\begin{aligned} \text{dist}^2 _{wed}(x_i,x_j) &= \left \| x_i - x_j \right \|^2_2 = \sum^d_{k=1} w_k \cdot dist^2_{ij,k} \\ &= (x_i - x_j)^TW(x_i - x_j) \end{aligned}</script><p>其中，$w<em>{k} \geqslant 0$，$W = \text{diag}(w)$是一个对角矩阵，$(W)</em>{ii} = w_i$。</p>
<p>式中的$W$可由学习得到，但需要注意的是，$W$的非对角线元素均为零，意味着其属性之间无关，但在现实问题中，往往无法达到这一条件。因此，将式中的$W$矩阵替换成一个普通的半正定对称矩阵$M$，这样就得到了<em>马氏距离</em>（Mahalanobis distance）（在标准的马氏距离中$M$为协方差矩阵的逆，即$M=\Sigma^{-1}$，但在度量学习中其被赋予了更大的灵活性）</p>
<script type="math/tex; mode=display">\text{dist}_{\text{mah}}^2(x_i,x_j) = \left \| x_i - x_j \right \|^2_{M} = (x_i - x_j)^T M(x_i - x_j)</script><p>其中$M$亦称“度量矩阵”，度量学习即是对$M$进行学习。需要注意的是，为了保持距离的非负且对称，$M$必须是（半）正定矩阵，即必有正交基$P$使得$M$能写为$M=PP^T$。</p>
<p>之后，需要将$M$直接嵌入到近邻分类器的评价指标中，以方便对$M$进行学习。例，在<em>近邻成分分析</em>（Neighbourhood Component Analysis）中使用的方法。</p>
<p>在近邻成分分析中，我们将在判别时普遍使用的多数投票法，替换为引入距离权重的概率投票法。对于任意样本$x_j$，他对$x_i$的分类结果影响的概率为</p>
<script type="math/tex; mode=display">p_{ij} = \frac{\exp(-\left \| x_i - x_j \right \|^2_M)}{\sum_l \exp(-\left \| x_i - x_l \right \|^2_M)}</script><p>显然，当$i=j$时，$p_{ij}$最大，$x_j$对$x_i$的影响随距离增大而减小。以<em>留一法</em>（LOO）正确率的最大化为目标，则它被自身之外的所有样本正确分类的概率为</p>
<script type="math/tex; mode=display">p_i = \sum_{j \in \Omega_i} p_{ij}</script><p>其中，$\Omega_i$表示与$x_i$属于同类别的样本的下标集合。则在整个样本集上的留一法正确率为</p>
<script type="math/tex; mode=display">\sum^m_{i=1}p_i = \sum^m_{i=1}\sum_{j \in \Omega_i} p_{ij}</script><p>结合$p_{ij}$的公式和$M=PP^T$，可导出NCA的优化目标为</p>
<script type="math/tex; mode=display">\min_P 1 - \sum^m_{i=1}\sum_{j \in \Omega_i} \frac{\exp(-\left \| x_i - x_j \right \|^2_M)}{\sum_l \exp(-\left \| x_i - x_l \right \|^2_M)}</script><p>对于这个无约束的优化问题，可以利用随机梯度算法或共轭梯度算法等方法对其求解，即可得到最大化近邻分类器LOO正确率的距离度量矩阵$M$。</p>
<p>除了可以使用监督学习目标中的错误率作为度量学习的优化目标以外，还可以在度量学习中引入领域知识。例如，如果已知某些样本相似、某些样本不相似，则可以根据此定义“<em>必连</em>”（must-link）约束集合$\mathcal{M}$与“<em>勿连</em>”（connot-link）约束集合$\mathcal{C}$。于是，可以通过$(x_i,x_j) \in \mathcal{M}$表示相似，$(x_i,x_j) \in \mathcal{C}$表示不相似。根据相似的样本之间距离较小，不相似的样本之间距离较大的原则，可以构建出关于度量矩阵$M$的凸优化问题：</p>
<script type="math/tex; mode=display">\begin{aligned} &\min_M \sum_{(x_i,x_j) \in \mathcal{M}} \left \| x_i - x_j \right \|^2_M \\ &\text{  s.t.  } \sum_{(x_i,x_k) \in \mathcal{C}} \left \| x_i - x_k \right \|^2_M \geqslant 1, \\ &\ \ \ \text{   } \text{   } \ \ \ \ M \succeq 0. \end{aligned}</script><p>其中约束$M \succeq 0$表示度量矩阵$M$必须是半正定的。上式的意义在于：要求在与不相似样本间的距离不小于1的前提下，使相似样本间的距离尽可能小。</p>
<p>不同度量学习方法对好的度量矩阵$M$的要求不同，如果求得的$M$是一个低秩矩阵，则通过对$M$的特征值分解，总能找到一组正交基，其正交基数目等于矩阵$M$的秩$\text{rank}(M)$，小于原属性数$d$。所以，此时度量学习学得的结果，可以衍生出一个降维矩阵$P \in R^{d \times \text{rank}(M)}$，可用于降维。不过需要注意的是，不能依赖此类方法进行降维，因为通常情况下不要求度量矩阵$M$是低秩的。</p>
]]></content>
      <tags>
        <tag>《机器学习》</tag>
      </tags>
  </entry>
  <entry>
    <title>Chapter11 特征选择与稀疏学习</title>
    <url>/2019/10/17/Chapter11%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>本文内容：</p>
<ul>
<li>子集搜索与评价<ul>
<li>子集搜索</li>
<li>子集评价</li>
</ul>
</li>
<li>过滤式选择</li>
<li>包裹式选择</li>
<li>嵌入式选择与正则化</li>
<li>稀疏表示与字典学习</li>
<li>压缩感知<ul>
<li>限定等距性</li>
</ul>
</li>
<li>附录<ul>
<li>L-Lipschitz条件</li>
<li>$L_0$范数</li>
</ul>
</li>
</ul>
<span id="more"></span>
<h2 id="11-1-子集搜索与评价"><a href="#11-1-子集搜索与评价" class="headerlink" title="11.1 子集搜索与评价"></a>11.1 子集搜索与评价</h2><p>对一个学习任务而言，给定数据集，其中包含一种或多种属性，我们将属性称为“<em>特征</em>”（feature），对当前学习任务有用的属性称为“<em>相关特征</em>”（relevant feature）、无用的称为“<em>无关特征</em>”（irrelevant feature）。从给定的特征集合中选择出相关特征子集的过程，称为“<em>特征选择</em>”（feature selection）</p>
<p>特征选择的作用可大概分为两方面：一方面，选择出重要特征，可以在一定程度上避免维数灾难；另一方面，去除不相关因素可以降低学习任务的难度。因此，特征选择是一个重要的<em>数据预处理</em>，通常在获得数据之后，训练学习器之前进行。</p>
<p>但需要注意的是，特征选择过程必须确保不丢失重要特征，否则后续学习过程会因为缺失重要信息而无法获得好的性能。对给定的数据集，如果学习任务不同，相关特征也就不尽相同，所以，所以谓的“相关特征”和“无关特征”的概念都是相对的。</p>
<p>此外，有一类特征称为“<em>冗余特征</em>”（redundant feature），它们所包含的信息能从其他特征中推演得到，即具有完全相关的特征。一般情况下，去除冗余特征不会对结果产生影响，还可以减轻学习过程的负担；但在冗余属性是结果的“中间概念”时，则会降低学习任务的难度。为简化讨论，本文假定数据中不涉及冗余特征，且初始数据集中包含了所有重要特征。</p>
<p>要从初始的特征集中选取出一个包含所有重要信息的特征子集，如果没有任何领域知识作为先验假设，只能对所有特征子集进行遍历。但在计算上确是不可行的，因为一旦面对特征个数较多的情况，就会遇到组合爆炸的问题。</p>
<p>可行的方法是，寻找一个特征子集，对其进行评价。根据评价结果生成下一个特征子集直到无法找到更好的特征子集。</p>
<p>因此，可将特征子集选择拆分为两个环节：1. 子集搜索；2. 子集评价。</p>
<h3 id="子集搜索"><a href="#子集搜索" class="headerlink" title="子集搜索"></a>子集搜索</h3><p>从单特征子集开始，选择最优的特征，每轮增加一个当前情况下的最优特征，即贪心算法。在没有优于上一轮特征子集时停止生成候选子集，并选取上一轮的特征子集为特征选择结果。这样逐渐增加的策略被称为“<em>前向</em>”（forward）搜索。类似的，从后向前，逐渐减少特征的策略称为“<em>后向</em>”（backward）搜索。此外，可结合前向和后向搜索，每一轮逐渐增加选定相关特征，同时减少无关特征，这样的策略被称为“<em>双向</em>”（bidirectional）搜索。（分别对应于回归分析中的前向回归、后向回归和逐步回归）显然，这种贪心策略有可能会陷入局部最小，但若不进行穷举这种情况就无法避免。</p>
<h3 id="子集评价"><a href="#子集评价" class="headerlink" title="子集评价"></a>子集评价</h3><p>假定给定样本属性均为离散型的数据集$D$，且$D$中第$i$类样本所占比例为$p_i = (i=1,2,\dots,|\gamma|)$。对属性集$A$，假定根据$A$的取值将$D$分成了$V$个子集${D^1,D^2,\dots,D^V}$，每个子集中的样本在$A$上的取值相同，于是我们可以计算$A$的信息增益</p>
<script type="math/tex; mode=display">\text{Gain}(A) = \text{Ent}(D) - \sum^{V}_{v=1}\frac{|D^v|}{|D|} \text{Ent}(D^v)</script><p>其中信息熵定义为</p>
<script type="math/tex; mode=display">\text{Ent}(D) = -\sum^{|\gamma|}_{k=1} p_k\log p_k</script><p>信息增益$\text{Gain(A)}$越大，意味着特征子集$A$包含的有助于分类的信息越多。于是，可以使用信息增益作为特征子集的评价准则。</p>
<p>如果仅考察属性集$A$对$D$的划分能力，难以反映出其划分对学习任务的特殊性。于是，更一般地，对属性子集$A$对$D$的划分，和样本标记信息$Y$对$D$的真实划分，计算这两者之间的划分差异，就能对$A$进行评价。这是利用信息熵对特征子集作出的评价，对任意可以判断二者之间划分差异的机制都可以用于特征评价。（例如 8.4.2节中的多样性度量方法）</p>
<p>结合子集搜索与子集评价，即可得到特征选择方法，特别是前向搜索与信息熵结合，这就与决策树算法非常相似。</p>
<p>常见的特征选择方法可大致分为三类：<em>过滤式</em>（filter）、<em>包裹式</em>（wrapper）、<em>嵌入式</em>（embedding）。</p>
<h2 id="11-2-过滤式选择"><a href="#11-2-过滤式选择" class="headerlink" title="11.2 过滤式选择"></a>11.2 过滤式选择</h2><p>过滤式方法先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关。这相当于先用特征选择过程对初始特征进行过滤，再用过滤后的特征来训练模型。</p>
<p><strong>Relief</strong>（Relevant Feature）：是一种著名的过滤式特征选择方法，该方法设计了一个“相关统计量”来度量特征的重要性。该统计量是一个向量，其每个分量分别对应于一个初始特征，而特征子集的重要性则是由子集中每个特征所对应的相关统计量分量之和来决定。于是，最终只需指定一个阈值$\tau$，然后选择比$\tau$大的相关统计量分量对应的特征即可；也可指定欲选取的特征个数$k$，然后选择相关统计量分量最大的$k$个特征。</p>
<p>显然，Relief的关键是如何确定相关统计量。给定训练集${(x<em>1,y_1),(x_2,y_2),\dots,(x_m,y_m)}$，对每个示例$x_i$，Relief先在$x_i$的同类样本中寻找其最近邻$x</em>{i,\text{nh}}$，称为“<em>猜中近邻</em>”（near-hit），再从$x<em>i$的异类样本中寻找其最近邻$x</em>{i,\text{nm}}$，称为“<em>猜错近邻</em>”（near-miss）。然后，相关统计量对应于属性$j$的分量为</p>
<script type="math/tex; mode=display">\delta^j = \sum_i -\text{diff}(x_i^j,x_{i,nh}^j)^2 + \text{diff}(x_i^j,x_{i,nm}^j)^2</script><p>其中，$x_a^j$表示样本$x_a$在属性$j$上的取值，$\text{diff}(x_a^j,x_b^j)$取决于属性$j$的类型。若属性$j$是离散型，则$x^j_a = x^j_b$时$\text{diff}(x_a^j,x_b^j)=0$，否则为1；若属性$j$是连续型，则$\text{diff}(x_a^j,x_b^j) = |x^j_a-x^j_b|$，注意$x_a^j$和$x_b^j$已规范化到$[0,1]$区间。</p>
<p>结合上式分析，如果$x<em>i$与猜中近邻$x</em>{i,nh}$在属性$j$上的距离小于$x<em>i$与猜错近邻$x</em>{i,nm}$的距离，则说明属性$j$对区分同类与异类样本是有益的，于是增大属性$j$所对应的统计量分量；反之，若$x<em>i$与猜中近邻$x</em>{i,nh}$在属性$j$上的距离大于$x<em>i$与猜错近邻$x</em>{i,nm}$的距离，则说明属性$j$起负面作用，于是减小属性$j$所对应的统计量分量。最后，对基于不同样本得到的估计结果进行平均，就得到各属性的相关统计量分量，分量值越大，则对应属性的分类能力就越强。</p>
<p>上式中的$i$指出了用于平均的样本下标。实际上Relief只需在数据集的采样上而不必在整个数据集上估计相关统计量。显然，Relief的时间开销随采样次数以及原始特征数线性增长，因此是一个运行效率很高的过滤式特征算法。</p>
<p>Relief是为二分类问题设计的，其扩展变体Relief-F能处理多分类问题。假定数据集$D$中的样本来自$|\gamma|$各类别。对示例$x<em>i$，若它属于第$k$类（$k \in {1,2,\dots,|\gamma|}$），则Relief-F先在第$k$类之外的每个类中找到一个$x_i$的最近邻示例作为猜错近邻，记为$x</em>{i,l,m}$（$l = 1,2,\dots,|\gamma|$；$l \neq k$）。于是，相关统计量对应于属性$j$的分量为</p>
<script type="math/tex; mode=display">\delta^j = \sum_i -\text{diff}(x_i^j,x_{i,nh}^j)^2 + \sum_{l \neq k} \left (p_l \times \text{diff}(x_i^j,x_{i,nm}^j)^2 \right )</script><p>其中，$p_l$为第$l$类样本在数据集$D$中所占的比例。</p>
<h2 id="11-3-包裹式选择"><a href="#11-3-包裹式选择" class="headerlink" title="11.3 包裹式选择"></a>11.3 包裹式选择</h2><p>包裹式特征选择直接把最终将要使用的学习器的性能作为特征子集的评价准则，即目的就是为给定的学习器选择最有利于性能、量身定做的特征子集。</p>
<p>一方面，由于包裹式选择直接对给定学习器的性能进行优化，其性能表现要优于过滤式；另一方面，包裹式选择需要对学习器进行多次训练，所以时间复杂度较高。</p>
<p><strong>LVW</strong>（Las Vegas Wrapper）：一个典型的包裹式特征选择方法，其利用拉斯维加斯方法来进行子集搜索，以最终分类器的误差为特征子集评价准则。以下为LVW算法的描述</p>
<hr>
<p>输入：数据集$D$;<br>$~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~$特征集A;
$~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~$学习算法$\pounds$;<br>$~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~$停止条件控制参数$T$;
过程：
$E = \infty$;
$d = |A|$;
$A^* = A$;
$t = 0$;
**while** $t < T$ **do**
$~</script>~<script type="math/tex">~</script>~$随机产生特征子集$A’$;<br>$~<script type="math/tex">~</script>~<script type="math/tex">~</script>d’ = |A’|$;<br>$~<script type="math/tex">~</script>~<script type="math/tex">~</script>E’ = \text{CrossValidation}(\pounds(D^{A’}))$;<br>$~<script type="math/tex">~</script>~<script type="math/tex">~$**if**($E' < E$) $\vee$ (($E' = E$) $\wedge$ (d' < d)) **then**
$~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">t=0$;
$~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">E=E'$;
$~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">d=d'$;
$~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">A^*=A'$;
$~</script>~<script type="math/tex">~</script>~$<strong>else</strong><br>$~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~$t=t+1;
$~</script>~<script type="math/tex">~</script>~$<strong>end if</strong><br><strong>end while</strong></p>
<hr>
<p>需要注意的是，LVW的每次评价都需要重新训练学习器，且其特征子集搜索是随机的。所以一般需要为其设置一个最大未改变最优选择的轮数$T$，但如果在初始特征数很多、$T$设置很大时，若存在时间限制，则有可能无法给出解。</p>
<h2 id="10-4-嵌入式选择与正则化"><a href="#10-4-嵌入式选择与正则化" class="headerlink" title="10.4 嵌入式选择与正则化"></a>10.4 嵌入式选择与正则化</h2><p>区别于过滤式和包裹式清楚地将特征选择和学习器训练过程分开，嵌入式特征选择和学习器训练过程融为一体。</p>
<p>对给定的数据集$D={(x_1, y_1), (x_2, y_2), \dots, (x_m, y_m)}$，其中$x \in R$，$y \in R$。考虑最简单的线性回归模型，以平方误差为损失函数，则优化回归目标为</p>
<script type="math/tex; mode=display">\min_w \sum^m_{i=1} (y_i - w^Tx_i)^2</script><p>在样本特征很多而样本数相对较少时，上式易陷入过拟合，为缓解过拟合问题，为其引入正则化项。若使用$L_2$范数正则化，则有</p>
<script type="math/tex; mode=display">\min_w \sum^m_{i=1}(y_i - w^Tx_i)^2 + \lambda \left \| w \right \|^2_2</script><p>其中正则化参数$\lambda &gt; 0$，上式称为“<em>岭回归</em>”（ridge regression），引入$L_2$范数正则化了以后，能显著地降低过拟合的风险。</p>
<p>如果将$L_2$范数替换为$L_1$范数，即</p>
<script type="math/tex; mode=display">\min_w \sum(y_i - w^Tx_i)^2 +\lambda \left \| w \right \|_1</script><p>此时得到的式子被称为<em>LASSO</em>（Least Absolute Shrinkage and Selection Operation）</p>
<p>并且，$L_1$范数比起$L_2$范数还有另外的优势：它更易于获得“<em>稀疏</em>”（sparse）解，即所得解$w$具有更少的非零分量。可通过图像来理解这一点：</p>
<p>在取得稀疏解$w$之后，可以发现，此时仅使用了一部分初始特征。所以，也就实现了嵌入式的特征选择方法。</p>
<p>对$L_1$正则化问题的求解，可使用<em>近端梯度下降</em>（Proximal Gradient Descent）。令$\triangledown$表示微分算子，对优化目标</p>
<script type="math/tex; mode=display">\min_x f(x) + \lambda\left \| x \right \|_1</script><p>$f(x)$可导，$\triangledown$满足L-Lipschitz条件$^{[1]}$，即存在常数$L &gt; 0$，使得</p>
<script type="math/tex; mode=display">\left \| \triangledown f(x') - \triangledown f(x) \right \|^2_2 \ \ (\forall x, x')</script><p>在$x_k$附近将$f(x)$通过泰勒展开式近似为</p>
<script type="math/tex; mode=display">\begin{aligned} \hat f(x) &= f(x_k) + \left \| x-x_k \right \|f'(x_k) + \frac{\left \|x-x_k \right \|^2}{2}f''(x_k) \\ &\simeq f(x_k) + \left \langle \triangledown f(x_k), x - x_k \right \rangle + \frac{L}{2}\left \| x-x_k \right \|^2 \\ &= \frac{L}{2}\left \| x - \left ( x_k - \frac{1}{L}\triangledown f(x_k) \right )\right \|^2_2 + \text{const} \end{aligned}</script><p>其中$\text{const}$是与$x$无关的常数，$\left \langle \cdot, \cdot \right \rangle$表示内积。显然上式的最小值在如下$x_{k+1}$获得：</p>
<script type="math/tex; mode=display">x_{k+1} = x_k - \frac{1}{L}\triangledown f(x_k)</script><p>于是，通过梯度下降法对$f(x)$进行最小化时，每一步梯度下降迭代实际上等价于最小化二次函数$\hat f(x)$，将这个思想扩展到我们的优化目标中，可以得到</p>
<script type="math/tex; mode=display">x_{k+1} = \argmin_x \frac{L}{2}\left \| x-\left ( x_k - \frac{1}{L}\triangledown f(x_k)\right ) \right \|^2_2 + \lambda \left \|x \right \|_1</script><p>即在每一步对$f(x)$进行梯度下降迭代的同时，考虑对$L_1$范数的最小化。</p>
<p>对于上式，可先计算$z = x_k - \frac{1}{L}\triangledown f(x_k)$，然后求解</p>
<script type="math/tex; mode=display">x_{k+1} = \argmin_x \frac{L}{2}\left \| x-z \right \|^2_2 + \lambda \left \|x \right \|_1</script><p>令$x^i$表示$x$的第$i$个分量，将上式按分量展开可看出，其中不存在$x^ix^j(i \neq j)$这样的项，即$x$的各个分量互不影响，于是有闭式解</p>
<script type="math/tex; mode=display">x^i_{k+1}\begin{cases} z^i - \lambda / L, & \lambda / L < z^i; \\ 0, & |z^i| \leqslant \lambda / L; \\ z^i + \lambda / L, & z^i < -\lambda / L \end{cases}</script><p>其中$x^i<em>{k+1}$与$z^i$分别是$x</em>{k+1}$与$z$的第$i$个分量。因此，通过PGD能使LASSO和其他基于$L_1$范数最小化的方法得以快速求解。</p>
<h2 id="11-5-稀疏表示与字典学习"><a href="#11-5-稀疏表示与字典学习" class="headerlink" title="11.5 稀疏表示与字典学习"></a>11.5 稀疏表示与字典学习</h2><p>将数据集$D$考虑成一个矩阵，每行对应于一个样本，每列对应于一个特征。特征选择的目的是，将学习任务限制在仅与学习任务有关的列构成的矩阵上，以降低学习任务的难度，可以涉及的计算和存储开销，学得模型的可解释性也会提高。</p>
<p>针对于类似文档中汉字的稀疏矩阵，矩阵每一行都有大量零元素，并且针对不同的文档，零元素的出现往往很不相同。</p>
<p>样本具有这样的稀疏形式时，将使大多数问题变得线性可分。同时，由于稀疏矩阵有一些高效的存储方法，也就不会造成存储上的巨大负担。</p>
<p>对给定数据集$D$，此时$D$稠密的，将其转化为<em>稀疏表示</em>（sparse representation）形式时，需要注意做到“恰当稀疏”，避免“过度稀疏”。而在一般的学习任务中并没有将稠密矩阵转化为稀疏表示的“字典”或“<em>码书</em>”（codebook），因而需要学习出一个这样的字典，这个过程被称为“<em>字典学习</em>”（dictionary learning）或“<em>码书学习</em>”（codebook learning），也被称为<em>稀疏编码</em>（sparse coding）。（二者在侧重上略有不同）</p>
<p>给定数据集${x_1, x_2, \dots, x_m}$，字典学习最简单的形式为</p>
<script type="math/tex; mode=display">\min_{B, \alpha_i} \sum^m_{i=1} \left \| x_i - B\alpha_i \right \|^2_2 + \lambda \sum^m_{i=1}\left \| \alpha_i \right \|_1</script><p>其中，$B \in R^{d \times k}$为字典矩阵，$k$称为字典的词汇量，通常由用户指定，$\alpha_i \in R^k$是样本$x_i \in R^d$的稀疏表示。对上式可采用变量交替优化的方式求解。显然，第一项是对重构的正则化，第二项是对稀疏性的正则化：</p>
<ol>
<li>固定字典$B$，将上式按分量展开，可看出其中不包含$a<em>i^ua_i^v$这样的交叉项，因此，类似对LASSO的求解，可求出对应于$x_i$的$\alpha_i$ $$\min</em>{\alpha_i} \left | x_i - B \alpha_i \right |^2_2 + \lambda\left | \alpha_i \right |_1$$</li>
<li>以$\alpha_i$为初值更新字典$B$（$\left | \cdot \right |_F$是Frobenius范数，即矩阵上的$L_2$范数）<script type="math/tex">\min_B \left \| X - BA \right \|^2_F</script></li>
</ol>
<p>有多种方法可对上式求解，常用的有基于逐列更新策略的KSVD。令$b_i$表示字典矩阵$B$的第$i$列，$\alpha^i$表示稀疏矩阵$A$的第$i$行，从而将上式重写为</p>
<script type="math/tex; mode=display">\begin{aligned} \min_B \left \| X-BA \right \|^2_F &= \min_{b_i} \left \| X-\sum^k_{j=1}b_j\alpha^j \right \|^2_F \\ &= \min_{b_i} \left \| \left ( X - \sum_{j\neq i}b_j \alpha^j \right ) - b_i \alpha^i \right \|^2_F \\ &= \min_{b_i} \left \| E_i - b_i \alpha^i \right \|^2_F \end{aligned}</script><p>进行逐列更新至第$i$列时，其他列是固定的，所以$E<em>i = X - \sum</em>{j \neq i} b_j \alpha^j$是固定的。于是，求解上式仅需对$E_i$进行奇异值分解以取得最大奇异值对应的正交向量。但是，如果直接对$E_i$进行奇异值分解会改变$b_i$和$\alpha^i$，将可能破坏第一步中$A$的稀疏性，因此KSVD对$E_i$和$\alpha^i$进行专门处理：$\alpha_i$仅保留非零元素，$E_i$仅保留$b_i$与$\alpha^i$的非零元素的乘积项，再进行奇异值分解，从而保持其稀疏性。</p>
<p>初始化字典矩阵$B$之后反复迭代上述两步，最终即可求得字典$B$和样本$x_i$的稀疏表示。过程中可通过设置词汇量$k$的大小来控制字典规模，从而影响稀疏程度。</p>
<h2 id="11-6-压缩感知"><a href="#11-6-压缩感知" class="headerlink" title="11.6 压缩感知"></a>11.6 压缩感知</h2><p><strong>奈奎斯特采样定理</strong>：令采样频率达到模拟信号最高频率的两倍，则采样后的数字信号就保留了模拟信号的全部信息。（意味着这是信号恢复的充分条件而非必要条件）</p>
<p>在实践中，为方便存储、传输，往往需要对数字信号进行压缩，这可能会损失一定的信息，同时传输时也容易出现丢包问题，从而进一步损失信息。这时，需要通过接收到的戴护具精确地重构出原信号，<em>压缩感知</em>（compressed sensing）为该类提供新思路。</p>
<p>假定有长度为$m$的离散信号$x$，以远小于奈奎斯特采样定理要求的采样率进行采样，得到长度为$n$的采样后信号$y$，$n \ll m$，即</p>
<script type="math/tex; mode=display">y = \Phi x</script><p>其中$\Phi \in R^{n \times m}$是对信号$x$的测量矩阵，确定采样的频率和如何将采样样本组成采样后的信号。在将测量值$y$和测量矩阵$\Phi$传输出去后，在一般情况下，无法仅通过它们还原出原始信号$x$。</p>
<p>那么，不妨设存在某线性变换$\Psi \in R^{m \times m}$，使得$x$可表示为$\Psi s$，$y$可表示为</p>
<script type="math/tex; mode=display">y = \Phi \Psi s = As</script><p>其中$A=\Phi \Psi \in R^{n \times m}$，所以只要能通过$y$恢复出$s$，即可通过$x = \Psi s$恢复出信号$x$。看起来问题没有得到简化，因为，$y = \Phi x$和$y = \Phi \Psi s$都是欠定的，无法轻易求出数值解。但对于后者而言，若其中$s$具有稀疏性，则这个问题就能得以很好地解决，此时式中的$\Psi$称为稀疏基，$A$就类似于字典。</p>
<p>压缩感知关注的是如何利用信号本身所具有的稀疏性，从部分观测样本中恢复原信号。通常认为，压缩感知分为“感知测量”和“重构恢复”两个阶段：</p>
<p><strong>感知测量</strong> 关注如何对原始信号进行处理以获得稀疏样本表示，涉及傅里叶变换、小波变换、字典学习和稀疏编码等。</p>
<p><strong>重构恢复</strong> 关注如何基于稀疏性从少量观测中恢复原信号，当谈压缩感知时，通常指该部分。</p>
<p>压缩感知相关理论比较复杂，下面仅简要介绍“<em>限定等距性</em>”（Restricted Isometry Property，简称RIP）。</p>
<h3 id="限定等距性"><a href="#限定等距性" class="headerlink" title="限定等距性"></a>限定等距性</h3><p>对大小为$n \times m$（$n \ll m$）的矩阵$A$，若存在常数$\delta_k \in (0,1)$，使得对任意向量$s$和$A$的所有子矩阵$A_k \in R^{n \times k}$有</p>
<script type="math/tex; mode=display">(1-\delta_k) \left \| s \right \|^2_2 \leqslant \left \| A_ks \right \|^2_2 \leqslant (1+\delta_k)\left \| s \right \|^2_2</script><p>则称$A$满足$k$限定等距性($k$-RIP)。此时可通过下面的优化问题近乎完美地从$y$中恢复出稀疏信号$s$，进而恢复出$x$：</p>
<script type="math/tex; mode=display">\begin{aligned} \min_s \left \| s \right \|_0 \\ \text{s.t.  } \ y = As \end{aligned}</script><p>但上式中涉及$L_0$范数$^{[2]}$最小化，$L_0$范数非凸，这是NP难问题。不过，在一定条件下，$L_0$范数最小化与$L_1$范数最小化具有共解，因此只需要关注</p>
<script type="math/tex; mode=display">\begin{aligned} \min_s \left \| s \right \|_1 \\ \text{s.t.  } \ y = As \end{aligned}</script><p>这样，压缩感知问题就可通过$L_1$范数最小化问题求解，例如上式可转化为LASSO的等价形式再通过近端梯度下降法求解，即使用“<em>基寻踪去噪</em>”（Basis Pursuit De-Noising）。</p>
<p>对“<em>协同过滤</em>”（colalborative filtering）任务，通过压缩感知恢复欠采样信号（即含缺失值样本）的前提条件之一是信号有稀疏表示。</p>
<p><em>矩阵补全</em>（matrix completion）技术可用于解决这个问题，其形式为</p>
<script type="math/tex; mode=display">\begin{aligned} &\min_x \text{rank}(X) \\ &\text{s.t.  } \ (X)_{ij} = (A)_{ij}, \ \ (i,j)\in \Omega \end{aligned}</script><p>其中$X$是需要恢复的稀疏信号，$\text{rank}(X)$表示矩阵的秩。$A$是已观测信号，$\Omega$是$A$中已观测元素$(A)_{ij}$的下标集合。上式的约束中明确指出，恢复出的$X$矩阵与已观测到的对应元素相同。</p>
<p>但上式依旧是NP难问题，而$\text{rank}(X)$在集合${x \in R^{m \times n}: \left | X \right |^2_F \leqslant 1}$上的凸包是$X$的“<em>核范数</em>”（nuclear norm）：</p>
<script type="math/tex; mode=display">\left \| X \right \|_* = \sum^{\min\{m,n\}}_{j=1} \sigma_j(X)</script><p>其中$\sigma_j(X)$表示$X$的奇异值，即矩阵的核范数为矩阵的奇异值之和，于是可通过最小化矩阵核范数来近似求解原问题，即</p>
<script type="math/tex; mode=display">\begin{aligned} &\min_X \left \| X \right \|_*  \\ &\text{s.t.  } (X)_{ij} = (A)_{ij}, \ \ (i,j) \in \Omega \end{aligned}</script><p>上式是一个凸优化问题，可通过<em>半正定规划</em>（Semi-Definite Programming，简称SDP）求解。理论研究表明，在满足一定条件时，若$A$的秩为$r$，$n \ll m$，则只需观察到$O(mrlog^2m)$个元素就能完美恢复出$A$。</p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="1-L-Lipschitz条件"><a href="#1-L-Lipschitz条件" class="headerlink" title="[1] L-Lipschitz条件"></a>[1] L-Lipschitz条件</h3><p>如果函数$\phi(x)$，在有限区间$[a,b]$上满足：</p>
<ul>
<li>当$x \in [a,b]$时，$\phi(x) \in [a,b]$</li>
<li>对任意的$x_1$，$x_2 \in [a,b]$，恒成立：$|\phi(x_1) - \phi(x_2)| \leqslant L|x_1 - x_2|$</li>
</ul>
<p>则称函数$\phi(x)$在区间$[a,b]$上满足Lipschitz条件，其中$L$被称为Lipschitz常数。</p>
<p>利普希茨连续条件是一个比一致连续更强的光滑性条件，直观而言，利普希茨连续函数限制了函数改变的速度，是函数斜率必小于利普希茨常数。</p>
<p>在微分方程理论中，利普希茨条件是初值条件下解的存在唯一性定理中的一个核心条件。</p>
<h3 id="2-L-0-范数"><a href="#2-L-0-范数" class="headerlink" title="[2] $L_0$范数"></a>[2] $L_0$范数</h3><p>$L_0$范数表示向量中非零元素的个数，即稀疏度。</p>
]]></content>
      <tags>
        <tag>《机器学习》</tag>
      </tags>
  </entry>
  <entry>
    <title>Chapter3 线性模型</title>
    <url>/2019/09/20/Chapter3%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>本文内容：</p>
<ul>
<li>基本形式</li>
<li>线性回归</li>
<li>对数几率回归<ul>
<li>对数几率回归应用</li>
</ul>
</li>
<li>线性判别分析</li>
<li>多分类学习</li>
<li>附录<ul>
<li>范数</li>
<li>瑞利熵和广义瑞利熵</li>
<li>矩阵的迹</li>
</ul>
</li>
</ul>
<span id="more"></span>
<h2 id="3-1-基本形式"><a href="#3-1-基本形式" class="headerlink" title="3.1 基本形式"></a>3.1 基本形式</h2><p><em>线性模型</em>（linear model）学得一个通过属性的线性组合进行预测的函数，用向量形式写成</p>
<script type="math/tex; mode=display">f(x) = \omega ^T x + b</script><p>即通过训练集，学得$\omega$和$b$即可得到此线性模型。</p>
<p>由于$\omega$直观体现了相应$x$的权重，因此线性模型具有很好的<em>可解释性</em>（comprehensibility）或<em>可理解性</em>（understandability）。</p>
<h2 id="3-2-线性回归"><a href="#3-2-线性回归" class="headerlink" title="3.2 线性回归"></a>3.2 线性回归</h2><ul>
<li>针对于离散型属性的处理：<ul>
<li>有序型，直接将离散值转化为连续值</li>
<li>无序型，对应有k个离散值的属性，转化为k阶(0,1)向量</li>
</ul>
</li>
</ul>
<p>在将离散型属性转化为连续型之后，一般使用最小二乘法对线性回归模型进行参数估计，即可得到所求线性回归模型。</p>
<p>需要注意的是，在一些特殊情况下，建立的线性回归模型中变量数可能会超过样例数，此时会解出多个$\omega$值，此时将由学习算法的归纳偏好决定，比较常见的做法是引入<em>正则化</em>（regularization）项。</p>
<p>此外，还可以用线性函数对非线性函数映射。一般地，对单调可微函数$g(\cdot)$（连续且充分光滑），令</p>
<script type="math/tex; mode=display">y = g^{-1}(\omega^Tx+b)</script><p>得到的模型称为“<em>广义线性模型</em>”（generalized linear model），其中函数$g(\cdot)$称为“<em>联系函数</em>”（link function）。</p>
<h2 id="3-3-对数几率回归"><a href="#3-3-对数几率回归" class="headerlink" title="3.3 对数几率回归"></a>3.3 对数几率回归</h2><p>在回归学习的情况下做分类任务时，需要构造单调可微函数，将产生的预测值转换为真实标记。对于二分类问题，即将$z=\omega^Tx+b$的实值$z$转化为0/1值问题，最理想的是“<em>单位越阶函数</em>”（unit-step function）</p>
<script type="math/tex; mode=display">y = \begin{cases}
0, & \text{ if } z<0; \\
0.5, & \text{ if } z=0; \\
1, & \text{ if } z>0.
\end{cases}</script><p>但由于上式函数不连续，因此需要一个近似它的单调可微的替代函数（surrogate function）。常用的替代函数是对数几率函数（logistic function）：</p>
<script type="math/tex; mode=display">y = \frac{1}{1+e^{-z}}</script><p>在应用该模型时，可以得到x，y之间关系为</p>
<script type="math/tex; mode=display">y=\frac{1}{1+e^{-(\omega^Tx+b)}}</script><p>则有</p>
<script type="math/tex; mode=display">\ln \frac{y}{1-y} = \omega ^Tx+b</script><p>若视y为样本x为正例的可能性，则$\ln \frac{y}{1-y}$，反应的即为x的“<em>对数几率</em>”（log odds，亦称logit）。</p>
<p>所以该模型的本质是，用线性回归模型去逼近真实值的对数几率。因此，该模型称为“<em>对数几率回归</em>”（logistic regression，亦称logit regression）。回归指的仅是其内层算法逻辑，并非模型的目标，所以该模型是一种分类学习方法而非回归方法。其优点在于，对数据的分布不作要求；并且，其预测结果是对于类别的偏向概率；此外，该函数模型是在任意阶上可导的凸函数，可以使用数值优化算法求得最优解。</p>
<h3 id="对数几率回归模型应用"><a href="#对数几率回归模型应用" class="headerlink" title="对数几率回归模型应用"></a>对数几率回归模型应用</h3><p>在实际应用中，计算其参数需要使用<em>极大似然法</em>（maximum likelihood method）和<em>凸优化理论</em>实现，具体此处略过不表。</p>
<h2 id="3-4-线性判别分析"><a href="#3-4-线性判别分析" class="headerlink" title="3.4 线性判别分析"></a>3.4 线性判别分析</h2><p><strong>线性判别分析</strong>（Linear Discriminant Analysis，简称LDA）：一种经典的线性学习方法，亦称“Fisher判别分析”（严格来说，LDA比Fisher判别分析多了各类样本协方差矩阵相同且满秩的假设）。主要思想为，利用投影将训练集样例降维处理，同时根据类别的异同改变其相对距离，最后通过投影的位置对类别进行划分。</p>
<p>对给定数据集$D={(x_i,y_i)}^m_i=1$，$y_i \in {0.1}$，令$X_i$、$\mu _i$、$\Sigma_i$分别表示第i类示例的集合、均值向量、协方差矩阵。若将数据投影到直线$\omega$上，两类样本的中心在直线上的投影分别为$\omega^T\mu _0$和$\omega^T \mu _1$；若将样本全部投射到直线上，则两类样本的协方差分别为$\omega^T \Sigma _0\omega$和$\omega^T \Sigma _1\omega$，由于直线是一维空间，因此$\omega^T\mu _0$和$\omega^T \mu _1$、$\omega^T \Sigma _0\omega$和$\omega^T \Sigma _1\omega$均为实数。</p>
<p><strong>LDA的主要原理</strong>：为了使同类样例的投影点尽可能靠近，有以下方法：</p>
<ol>
<li>使同类样例投影点的协方差$\omega^T \Sigma _0\omega + \omega^T \Sigma _1\omega$尽可能小</li>
<li>使异类样例投影点尽可能远离，即使异类样例投影点中心之间的距离$\left | \omega^T\mu_0 - \omega^T\mu_1 \right |^2_2$尽可能大$^{[1]}$</li>
</ol>
<p>同时考虑以上条件，即可得到欲最大化目标</p>
<script type="math/tex; mode=display">\begin{aligned} J &= \frac{\left \| \omega^T\mu_0 - \omega^T\mu_1 \right \|^2_2}{\omega^T \Sigma _0\omega + \omega^T \Sigma _1\omega} \\ &=\frac{\omega^T(\mu_0-\mu_1)(\mu_0-\mu_1)^T\omega}{\omega^T(\Sigma_0+\Sigma_1)\omega} \end{aligned}</script><p>定义“<em>类内散度矩阵</em>”（within-class scatter matrix）</p>
<script type="math/tex; mode=display">S_\omega=\Sigma_0 + \Sigma_1</script><p>以及“<em>类间散度矩阵</em>”（between-class scatter matrix）</p>
<script type="math/tex; mode=display">S_b = (\mu_0 - \mu_1)(\mu_0 - \mu_1)^T</script><p>所以，前式可以化简为</p>
<script type="math/tex; mode=display">J = \frac{\omega^T S_{b}\omega}{\omega^T S_{\omega}\omega}</script><p>得到LDA的优化目标，同时也是$S<em>b$与$S</em>\omega$的“<em>广义瑞利熵</em>”（generalized Rayleigh quotient）$^{[2]}$。</p>
<p>值得一提的是，LDA存在贝叶斯决策理论的解释，当两类样例集同先验、满足高斯分布且协方差相等时，达到最优分类。</p>
<p><strong>LDA的多分类推广</strong> LDA可以推广到多分类问题中，假设存在N个类，第i类中样例数为$m_i$，样例总数为$m$，由此定义以下概念</p>
<p>全局散度矩阵$S_t$：</p>
<script type="math/tex; mode=display">S_t = S_b +S_\omega = \sum ^m_{i=1} (x_i-\mu)(x_i-\mu)^T</script><p>（其中$\mu$为所有样例的均值向量，即$\mu = \frac{1}{m}\sum ^m_{i=1}x_i$）</p>
<p>类内散度矩阵$S_\omega$：</p>
<script type="math/tex; mode=display">S_\omega = \sum^N_{i=1}S_{\omega_i},</script><script type="math/tex; mode=display">S_{\omega_i} = \sum^{m_i}_{k=1}(x_k-\mu)(x_k-\mu)^T</script><p>类间散度矩阵$S_b$：</p>
<script type="math/tex; mode=display">S_b = S_t-S_\omega = \sum^N_{i=1} m_i(\mu_i-\mu)(\mu_i-\mu)^T</script><p>显然，利用$S<em>t$、$S</em>\omega$、$S_b$中的任意两者，即可构造用于多分类任务的LDA模型。常见的一种是采用优化目标</p>
<script type="math/tex; mode=display">\max_W = \frac{tr(W^TS_bW)}{tr(W^TS_\omega W)}</script><p>（其中$W \in \mathbb{R}^{d \times (N-1)}$）</p>
<p>上式可通过引入广义特征值问题求解</p>
<script type="math/tex; mode=display">S_b W = \lambda S_\omega W</script><p>$W$的闭式解是$S^{-1}_\omega S_b$的$d’$个最大非零广义特征值所对应的向量组成的矩阵（$d’ \leqslant N-1$）</p>
<p>可将$W$看作一个投影矩阵，即多分类LDA将样本投影到$d’$维空间，一般情况下，$d’$远小于原有的属性数$d$。因此，LDA可以作为降维算法使用。</p>
<h2 id="3-5-多分类学习"><a href="#3-5-多分类学习" class="headerlink" title="3.5 多分类学习"></a>3.5 多分类学习</h2><p>对于多分类学习，部分情况下可以直接将二分类算法直接推广到多分类，但在大多数情况下，都是利用二分类学习器来解决多分类问题。</p>
<p>·<em>理解不足，内容且略</em></p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="1-范数"><a href="#1-范数" class="headerlink" title="[1] 范数"></a>[1] 范数</h3><p>文中$\left | \cdot \right |$为范数，具有长度概念的函数，常用的有</p>
<p>1-范数：</p>
<script type="math/tex; mode=display">\left \| x \right \|_1 = \sum^{n}_{i=1} \left | x_i \right |</script><p>2-范数：</p>
<script type="math/tex; mode=display">\left \| x \right \|_2 = (\sum^{n}_{i=1} x_i^2 )^{\frac{1}{2}}</script><p>$\infin$-范数：</p>
<script type="math/tex; mode=display">\left \| x \right \|_\infin = \max_i \left | x_i \right |</script><p>$p$-范数：</p>
<script type="math/tex; mode=display">\left \| x \right \|_p = (\sum^{n}_{i=1} \left | x_i \right |^p )^{\frac{1}{p}}</script><h3 id="2-瑞利熵和广义瑞利熵"><a href="#2-瑞利熵和广义瑞利熵" class="headerlink" title="[2] 瑞利熵和广义瑞利熵"></a>[2] 瑞利熵和广义瑞利熵</h3><p><strong>厄米特矩阵</strong>（Hermitian Matrix）：指共轭矩阵。</p>
<p><strong>瑞利熵</strong>（Raylei quotient）：</p>
<script type="math/tex; mode=display">R(A,x) = \frac{x^HAx}{x^Hx}</script><p>$x$为非零向量，A为$n\times n的Hermitian矩阵$。<br>需要注意的是，瑞利熵$R(A,x)$有一个重要性质，即它的最大值等于矩阵A的最大特征值，最小值等于矩阵A的最小特征值，即</p>
<script type="math/tex; mode=display">\lambda_{\min} \leqslant \frac{x^HAx}{x^Hx} \leqslant \lambda _{\max}</script><p><strong>广义瑞利熵</strong>（generalized Rayleigh quotient）：</p>
<script type="math/tex; mode=display">R(A,B,x) = \frac{x^HAx}{x^HBx}</script><p>B为正定矩阵，其余参数同瑞利熵。<br>令$x=B^{-\frac{1}{2}}x’$，则可将分母转化为</p>
<script type="math/tex; mode=display">x^HBx = x'^Hx</script><p>分子则转化为</p>
<script type="math/tex; mode=display">x^HAx = x'^HB^{-\frac{1}{2}}AB^{-\frac{1}{2}}x'</script><p>此时$R(A,B,x)$转化为</p>
<script type="math/tex; mode=display">R(A,B,x') = \frac{x'^HB^{-\frac{1}{2}}AB^{-\frac{1}{2}}x'}{x'^Hx'}</script><p>由瑞利熵性质知，广义瑞利熵的最大值即为$B^{-\frac{1}{2}}AB^{-\frac{1}{2}}$的最大特征值。</p>
<h3 id="3-矩阵的迹"><a href="#3-矩阵的迹" class="headerlink" title="[3] 矩阵的迹"></a>[3] 矩阵的迹</h3><p>指矩阵主对角线上的元素之和，也是所有特征值之和，使用tr(·)表示。</p>
<script type="math/tex; mode=display">tr(A) = \sum^n_{i=1} a_{ii}</script>]]></content>
      <tags>
        <tag>《机器学习》</tag>
      </tags>
  </entry>
  <entry>
    <title>Chapter4 决策树</title>
    <url>/2019/09/20/Chapter4%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<p>本文内容：</p>
<ul>
<li>基本流程</li>
<li>划分选择<ul>
<li>信息增益</li>
<li>增益率</li>
<li>基尼指数</li>
</ul>
</li>
<li>剪枝处理<ul>
<li>预剪枝</li>
<li>后剪枝</li>
</ul>
</li>
<li>连续与缺失值<ul>
<li>连续值处理</li>
<li>缺失值处理</li>
</ul>
</li>
<li>多变量决策树</li>
</ul>
<span id="more"></span>
<h2 id="4-1-基本流程"><a href="#4-1-基本流程" class="headerlink" title="4.1 基本流程"></a>4.1 基本流程</h2><p><strong>决策树</strong>（decision tree）：基于树结构进行决策的一种常见的机器学习算法。决策树学习的目的在于，产生一棵泛化能力强的决策树，其基本流程采用“<em>分而治之</em>”（divide-and-conquer）的策略（伪代码如下）。</p>
<figure class="highlight vb"><table><tr><td class="code"><pre><span class="line">输入：训练集 D = &#123;(x1,y1),(x2,y2),...,(xm,ym)&#125;;</span><br><span class="line">      属性集 A = &#123;a1,a2,...,ad&#125;.</span><br><span class="line">过程：函数TreeGenerate(D,A)</span><br><span class="line">生成结点node;</span><br><span class="line"><span class="keyword">if</span> D中样本全属于同一类别C <span class="keyword">then</span></span><br><span class="line">    将node标记为C类叶结点; <span class="keyword">return</span>;</span><br><span class="line"><span class="keyword">end</span> <span class="keyword">if</span></span><br><span class="line"><span class="keyword">if</span> A == 空集 <span class="built_in">OR</span> D中样本在A上取值相同 <span class="keyword">then</span></span><br><span class="line">    将node标记为叶结点，其类别标记为D中样本数最多的类; <span class="keyword">return</span>;</span><br><span class="line"><span class="keyword">end</span> <span class="keyword">if</span></span><br><span class="line">从A中选择最优划分属性a*;</span><br><span class="line"><span class="keyword">for</span> a*的每一个值av* <span class="keyword">do</span></span><br><span class="line">    为node生成一个分支; 令Dv表示D中在a*上取值为av*的样本子集;</span><br><span class="line">    <span class="keyword">if</span> Dv 为空 <span class="keyword">then</span></span><br><span class="line">        将分支结点标记为叶结点，其类别标记为D中样本最多的类; <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        以TreeGenerate(Dv, A\&#123;a*&#125;)为分支结点</span><br><span class="line">    <span class="keyword">end</span> <span class="keyword">if</span></span><br><span class="line"><span class="keyword">end</span> <span class="keyword">for</span></span><br><span class="line">输出：以node为根结点的一颗决策树</span><br></pre></td></tr></table></figure>
<h2 id="4-2-划分选择"><a href="#4-2-划分选择" class="headerlink" title="4.2 划分选择"></a>4.2 划分选择</h2><p>划分选择的方法是决策树学习的关键。一般而言，随着划分过程的进行，在决策树的分支结点所包含的样本应该是越来越接近同一类，其接近同一类的程度被称为“<em>纯度</em>”（purity）。</p>
<h3 id="4-2-1-信息增益"><a href="#4-2-1-信息增益" class="headerlink" title="4.2.1 信息增益"></a>4.2.1 信息增益</h3><p><strong>信息熵</strong>（information entropy）：是度量样本集合纯度最常用的一种指标，对样本集合$D$中的第$k$类样本所占的比例为$p_k(k=1,2,…,\left | \gamma \right |)$的信息熵为</p>
<script type="math/tex; mode=display">Ent(D) = -\sum^{\left | \gamma \right |}_{k=1} p_k \log_2{p_k}</script><p>（计算中约定：若$p=0$，则$p\log_2p = 0$）</p>
<p>显然，Ent(D)的值越小，集合D的纯度越高。（Ent(D)的最小值为0，最大值为$\log_2 {\left | \gamma \right |}$）</p>
<p>为衡量使用某一属性进行划分所获得的“纯度提升”，定义“<em>信息增益</em>”（information gain）.</p>
<p><strong>信息增益</strong>（information gain）：是度量使用某一属性进行划分所获得的“纯度提升”。假定离散属性$a$有$V$个可能的取值${a^1,a^2,…,a^V}$，若使用$a$来对样本集$D$进行划分，则会产生$V$个分支结点，其中第$v$个分支结点包含了$D$中所有在属性$a$上取值为$a_v$的样本，记为$D^v$。</p>
<script type="math/tex; mode=display">Gain(D,a) = Ent(D) - \sum^V_{v=1} \frac{\left | D^v \right |}{\left | D \right |}Ent(D^v)</script><p>其中$\frac{\left | D^v \right |}{\left | D \right |}$为分支结点的权重，即分支结点的样本数越多，影响越大。显然，信息增益越大，则使用属性$a$进行划分所获得的“纯度提升”越大。ID3决策树学习算法就是以信息增益为准则划分属性的。</p>
<h3 id="4-2-2-增益率"><a href="#4-2-2-增益率" class="headerlink" title="4.2.2 增益率"></a>4.2.2 增益率</h3><p>实际上，“信息增益”会对可取值较多的属性产生偏好，为减少这种不利影响，可以不直接使用“信息增益”，而是使用“<em>增益率</em>”（gain ratio）来选择最优划分属性。</p>
<script type="math/tex; mode=display">Gain\_ratio(D,a) = \frac{Gain(D,a)}{\mathbb{IV}(a)}</script><p>其中</p>
<script type="math/tex; mode=display">\mathbb{IV}(a) = -\sum^V_{v=1} \frac{|D^v|}{|D|} \log_2 ^\frac{|D^v|}{|D|}</script><p>称为属性a的“<em>固有值</em>”（intrinsic value），其值会随着属性a可取值数量的增大而增大。</p>
<p>需要注意的是，增益率会对可取值数目较少的属性产生偏好，因此通常不直接使用增益率。在C4.5算法中，先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。</p>
<h3 id="4-2-3-基尼指数"><a href="#4-2-3-基尼指数" class="headerlink" title="4.2.3 基尼指数"></a>4.2.3 基尼指数</h3><p>另一种较为常用的度量数据集纯度的方法——基尼指数：</p>
<script type="math/tex; mode=display">Gini(D) = \sum ^{|\gamma|}_{k=1} \sum_{k' \neq k} p_kp_{k'} = 1 - \sum ^{|\gamma|}_{k=1} p_k^2</script><p>直观地，Gini(D)反应了从数据集D中随机抽取两个样本，其类别标记不一致的概率，所以，Gini(D)越小，数据集D的纯度越高。</p>
<p>则属性a的基尼指数为</p>
<script type="math/tex; mode=display">Gini\_index(D,a) = \sum ^V_{v=1} \frac{\left | D^v \right |}{\left | D \right |} Gini(D^v)</script><p>所以，从A中选择基尼指数最小的属性作为最优划分属性。</p>
<h2 id="4-3-剪枝处理"><a href="#4-3-剪枝处理" class="headerlink" title="4.3 剪枝处理"></a>4.3 剪枝处理</h2><p><strong>剪枝</strong>（pruning）：决策树学习处理过拟合的主要手段。分为两种基本策略：预剪枝和后剪枝。</p>
<h3 id="4-3-1-预剪枝"><a href="#4-3-1-预剪枝" class="headerlink" title="4.3.1 预剪枝"></a>4.3.1 预剪枝</h3><p><strong>预剪枝</strong>（prepruning）：在决策树生成过程中，对每个结点在划分前进行估计，如果其无法使决策树的泛化性能提升，即将该结点标记为叶结点。</p>
<p>预剪枝的特性：</p>
<ol>
<li>可以减少分支的“展开”，一方面降低了过拟合的风险，另一方面减少了训练时间和测试时间的开销；</li>
<li>相对地，预剪枝基于贪心的本质，容易陷入局部最小的情况，可能会使预剪枝决策树欠拟合。</li>
</ol>
<h3 id="4-3-2-后剪枝"><a href="#4-3-2-后剪枝" class="headerlink" title="4.3.2 后剪枝"></a>4.3.2 后剪枝</h3><p><strong>后剪枝</strong>（post-pruning）：在生成决策树后，自底向上地对每个非叶结点进行检查，如果将其替换为叶结点，能提高决策树的泛化性能，则保留替换。</p>
<p>后剪枝的特性：</p>
<ol>
<li>由于后剪枝保留的分支更多，所以欠拟合的风险较小，泛化性能也要优于预剪枝决策树。</li>
<li>相对地，后剪枝过程是基于已生成的决策树的，而且需要对每个非叶结点逐一检查，大大增加了训练时间。</li>
</ol>
<h2 id="4-4-连续与缺失值"><a href="#4-4-连续与缺失值" class="headerlink" title="4.4 连续与缺失值"></a>4.4 连续与缺失值</h2><h3 id="4-4-1-连续值处理"><a href="#4-4-1-连续值处理" class="headerlink" title="4.4.1 连续值处理"></a>4.4.1 连续值处理</h3><p>在决策树中使用的一般为离散值，因此，若要在决策树中使用连续值，就需要使用连续值离散化技术对连续值进行处理。其中最简单的策略是采用<em>二分法</em>（bi-partition）（可见于C4.5决策树算法）。</p>
<p><strong>二分法</strong>（bi-partition）：对给定的样本集$D$和连续属性$a$，针对属性$a$对样本集进行排序，记为${a^1,a^2,…,a^n}$。将区间$[a^i,a^{i+1})$的中点作为候选划分点，即</p>
<script type="math/tex; mode=display">T_a = \{\frac{a^i+a^{i+1}}{2}|1 \leq i \leq n-1 \}</script><p>之后将候选点作为离散属性处理即可。</p>
<h3 id="4-4-2-缺失值处理"><a href="#4-4-2-缺失值处理" class="headerlink" title="4.4.2 缺失值处理"></a>4.4.2 缺失值处理</h3><p>在数据集中常出现缺失值，若直接舍去该样本将造成极大的信息浪费，因而需要对缺失值进行处理。</p>
<p>待解决的问题有：</p>
<ol>
<li>如何在属性值缺失的情况下进行划分属性选择；</li>
<li>给定划分属性，若样本在该属性上的值缺失，如何对该样本进行划分。</li>
</ol>
<p>给定训练集$D$和属性$a$，令$\tilde D$表示在属性$a$上没有缺失值的样本子集，那么根据上述问题1，我们仅可用$\tilde D$来判断属性$a$的优劣。为每个样本$x$赋予权重$\omega _x$，在决策树学习初始阶段权重均为1，定义</p>
<script type="math/tex; mode=display">\rho = \frac{\sum _{x \in \tilde D} \omega_x}{\sum _{x \in D} \omega_x}</script><script type="math/tex; mode=display">\tilde p_k = \frac{\sum _{x \in \tilde D_k} \omega_x}{\sum _{x \in \tilde D} \omega_x} \ \ (1 \leq k \leq |\gamma|)</script><script type="math/tex; mode=display">\tilde r_v = \frac{\sum _{x \in \tilde D^v} \omega_x}{\sum _{x \in \tilde D} \omega_x} \ \ (1 \leq v \leq V)</script><p>显然，$\rho$对应无缺失值样本在所有样本中占的比例，$\tilde p_k$对应k类的样本在无缺失值样本中占的比例，$\tilde r_v$对应在属性$a$上取值为$a^v$的样本在无缺失值样本中所占的比例。因此，信息增益函数化为</p>
<script type="math/tex; mode=display">\begin{aligned} Gain(D,a) &= \rho \times Gain(\tilde D,a) \\ &= \rho \times [Ent(\tilde D) - \sum ^V _{v=1}\tilde r_vEnt(\tilde D^v)] \end{aligned}</script><p>其中</p>
<script type="math/tex; mode=display">Ent(\tilde D) = - \sum ^{|\gamma|}_{k=1} \tilde p_k \log_2^{\tilde p_k}</script><p>至此仅解决了问题1，对于问题2，需要通过修改权值$\omega _x$实现。若样本$x$属性$a$的值已知，则正常处理；样本$x$属性$a$的值未知，则将$x$同时划入所有子节点，并将样本权值在与属性$a^v$对应的子节点中调整为$\tilde r_v \cdot \omega _x$。这相当于将同一个样本，以不同的概率划入到不同的子节点中去。</p>
<p>上述算法是C4.5中使用的缺失值处理算法。</p>
<h2 id="4-5-多变量决策树"><a href="#4-5-多变量决策树" class="headerlink" title="4.5 多变量决策树"></a>4.5 多变量决策树</h2><p>将单变量决策树直接推广到多变量时，$d$个属性的样本点对应了$d$维空间中的点，则对其分类即在该空间中寻找分类边界。</p>
<p>决策树所形成的分类边界有一个明显的特点：<em>轴平行</em>（axis-parallel），即它的分类边界由若干个和轴平行的线段组成。这使得它的分类边界具有较好的可解释性，但是相对地，其生成的决策树将十分复杂，预测需要进行大量的属性测试，预测时间开销将会很大。</p>
<p>因此，若使用“斜的”分类边界将大大简化决策树模型。“<em>多变量决策树</em>”（multivariate decision tree）即是能实现斜划分甚至更复杂划分的决策树。</p>
<p>以常见的斜划分多变量决策树为例，在其学习过程中，不是为每个非叶结点寻找一个最优划分属性，而是寻找最优划分属性组合即线性分类器。</p>
]]></content>
      <tags>
        <tag>《机器学习》</tag>
      </tags>
  </entry>
  <entry>
    <title>Chapter5 神经网络</title>
    <url>/2019/09/20/Chapter5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<p>本文内容：</p>
<ul>
<li>神经元模型</li>
<li>感知机与多层网络</li>
<li>误差逆传播算法</li>
<li>全局最小与局部最小</li>
</ul>
<span id="more"></span>
<h2 id="5-1-神经元模型"><a href="#5-1-神经元模型" class="headerlink" title="5.1 神经元模型"></a>5.1 神经元模型</h2><p><strong>神经网络</strong>（neural networks）：是由具有适应性的简单单元组成的广泛并行互联的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。在机器学习中的神经网络是指，神经网络学习，是机器学习与神经网络学科领域的交叉部分。</p>
<p><strong>神经元模型</strong>（neuron model）：即上述定义中的简单单元。在一个神经元兴奋时，会传递神经递质至其他神经元，神经递质改变其他神经元内的电位，在某神经元电位差达到“<em>阈值</em>”（threshold）时，该神经元就会兴奋，继续传递神经递质。</p>
<p><strong>M-P神经元模型</strong>：神经元接受n个来自其他神经元的输入信号，这些输入信号通过带权重的<em>连接</em>（connection）进行传递，神经元收到的总输入值将与神经元的阈值进行比较，然后通过“<em>激活函数</em>”（activetion function）处理产生神经元的输出。</p>
<p>理想的激活函数是阶跃函数，但其具有不连续、不光滑等不太好的性质，因而常用Sigmoid函数作为激活函数，其又被称作“<em>挤压函数</em>”（squashing function）。其中，对数几率函数，即logistic函数是Sigmoid函数的代表。</p>
<p>连接多个这样的神经元即形成了神经网络。神经网络可以看作一个包含许多参数的数学模型，由若干个函数构成，例如$y_j = f(\sum_i \omega_i x_i - \theta_j)$相互嵌套而得。</p>
<h2 id="5-2-感知机与多层网络"><a href="#5-2-感知机与多层网络" class="headerlink" title="5.2 感知机与多层网络"></a>5.2 感知机与多层网络</h2><p><strong>感知机</strong>（Perceptron）：由两层神经元构成，输入层接收外界信号后传递给输出层，输出层是M-P神经元，亦称“<em>阈值逻辑单元</em>”（threshold logic unit）</p>
<p>感知机可容易地实现与、或、非运算。对于$y = f(\sum_i \omega_i  x_i - \theta _i)$，假定$f$为阶跃函数</p>
<ul>
<li><p><strong>与</strong> $(x_1 \wedge x_2)$：令$\omega_1 = \omega_2 = 1$，$\theta = 2$，则$y = f(1 \cdot x_1 + 1 \cdot x_2 - 2)$，仅在$x_1 = x_2 =1$时，$y = 1$；</p>
</li>
<li><p><strong>或</strong> $(x_1 \vee x_2)$：令$\omega_1 = \omega_2 = 1$，$\theta = 1$，则$y = f(1 \cdot x_1 + 1 \cdot x_2 - 0.5)$，当$x_1 = 1$或$x_2 = 1$时，$y = 1$；</p>
</li>
<li><p><strong>非</strong> $(\urcorner x_1)$：令$\omega_1 = 0.6$，$\omega_2 = 0$，$\theta = -0.5$，则$y = f(-0.6 \cdot x_1 + 0 \cdot x_2 + 0.5)$，当$x_1 = 1$时，$y = 0$；当$x_1 = 0$时，$y = 1$.</p>
</li>
</ul>
<p>更一般地，给定训练集，权重$\omega<em>i(i=1,2,\dots,n)$以及阈值$\theta$可以通过学习得到。阈值$\theta$可看作一个固定输入为-1.0的“<em>哑结点</em>”（dummy node）所对应的连接权重$\omega </em>{n+1}$，以此将权重和阈值的问题统一为学习权重的问题。感知机的学习过程如下：</p>
<p>对于训练样例$(x,y)$，若当前感知机输出为$\hat y$，则感知机权重调整方法为</p>
<script type="math/tex; mode=display">\omega_i \leftarrow \omega_i + \Delta \omega_i,</script><script type="math/tex; mode=display">\Delta \omega_i = \eta(y - \hat y) x_i.</script><p>其中，$\eta \in (0,1)$称为学习率（learning rate），可大致看作向错误修正方向的步长。（通常设置为小整数，如0.1）</p>
<p>需要注意的是，感知机仅有输出层神经元进行激活函数处理，即只有一层<em>功能神经元</em>（functional neuron），学习能力十分有限。事实上，感知机仅能处理类似“与”、“或”、“非”的<em>线性可分</em>（linearly separable）问题。即若问题是线性可分的，感知机的学习过程一定会<em>收敛</em>（converge），求得适当的权向量$\omega = (\omega<em>1;\omega_2;…;\omega</em>{n+1})$，形成线性超平面将其分开；而如果是非线性可分问题（即便是最简单的异或问题），感知机的学习过程将会发生<em>震荡</em>（fluctuation），$\omega$难以稳定下来，不能求得合适解。</p>
<p>要解决非线性可分问题，需考虑使用多层功能神经元。如利用两层感知机就能解决异或问题，在输入层与输出层之间的一层神经元，被称为隐层或<em>隐含层</em>（hidden layer），隐含层和输出层都是拥有激活函数的功能神经元。</p>
<p>更一般地，常见的神经网络是每层神经元与下一层神经元互连，神经元之间不存在同层连接，也不存在跨层连接，这样的神经网络结构被称为“<em>多层前馈神经网络</em>”（multi-layer feedforward neural networks）。其中，输入层神经元接收外界输入，隐层与输出层神经元对信号进行处理，并由输出层神经元输出最终结果。</p>
<h2 id="5-3-误差逆传播算法"><a href="#5-3-误差逆传播算法" class="headerlink" title="5.3 误差逆传播算法"></a>5.3 误差逆传播算法</h2><p>如果对多层网络进行训练，显然无法继续沿用单层感知机中的学习规则（权重调整法则），需要更加强大的算法。<em>误差逆传播</em>算法就是其中最杰出的代表，它也是迄今最成功的神经网络学习算法。它不仅可以用于多层前馈神经网络，还可用于其他类型的神经网络，例如训练递归神经网络，但通常的“BP网络”是指用BP算法训练的多层前馈神经网络。</p>
<p><strong>误差逆传播算法</strong>（error BackPropagation algorithm，简称BP）：对训练集$D={(x<em>1,y_1),(x_2,y_2),…,(x_m,y_m)}$，其中$x_i \in R^d$，$y_i \in R^l$分别表示d个输入属性和l维输出变量，假设该BP网络中有q层隐层神经元，输出层第j个神经元的阈值为$\theta _j$，隐层第h个神经元阈值为$\gamma_h$，输入层第i个神经元与隐层第h个神经元的连接权为$v</em>{ih}$，隐层第h个神经元接收到的输入为$\alpha<em>h = \sum^d</em>{i=1} v<em>{ih}x_i$，输出层第j个神经元接收到的输入为$\beta_j = \sum^q</em>{h=1}w_{hj}b_h$，其中$b_h$为隐层第h个神经元的输出。并且，所有激活函数都使用Sigmoid函数。则需要确定的参数有$(d+l+1)q + l$个，即</p>
<p><img src="https://i.loli.net/2019/09/03/SAMJx54b6tFmOcv.png" alt="参数数量"></p>
<p>对训练例$(x_k,y_k)$，假定神经网络的输出为$\hat y_k = (y^k_1,y^k_2,..,y^k_l)$，即</p>
<script type="math/tex; mode=display">\hat y^k_j = f(\beta_j - \theta_j)</script><p>对任意参数$a$的更新估计式为</p>
<script type="math/tex; mode=display">a \leftarrow a + \Delta a</script><p>以隐层到输出层的连接权$w_{hj}$为例进行推导。BP算法基于<em>梯度下降</em>（gradient descent）的方法以目标的负梯度方向对参数进行调整。对均方误差$E_k$和给定的学习率$\eta$得到</p>
<script type="math/tex; mode=display">\Delta w_{hj} = -\eta \frac {\partial E_k}{\partial w_{hj}}</script><p>针对上式进行分解变换</p>
<script type="math/tex; mode=display">\frac {\partial E_k}{\partial w_{hj}} = \frac{\partial E_k}{\partial \hat y^k_j} \cdot \frac{\partial \hat y^k_j}{\partial \beta_j} \cdot \frac{\partial \beta_j}{\partial w_{hj}}</script><p>注意到$\beta<em>j = \sum^q</em>{h=1} w_{hj}b_h$，则有</p>
<script type="math/tex; mode=display">\frac{\partial \beta_j}{\partial w_{hj}} = b_h</script><p>剩余部分则设$g_j$</p>
<script type="math/tex; mode=display">\begin{aligned} g_j &= -\frac{\partial E_k}{\partial \hat y^k_j} \cdot \frac{\partial \hat y^k_j}{\partial \beta_j} \\ &= -(\hat y^k_j - y^k_j)f'(\beta_j - \theta_j) \\ &= \hat y^k_j(1 - \hat y^k_j)(y^k_j - \hat y^k_j) \end{aligned}</script><p>（其中用到了Sigmoid函数的性质$f’(x) = f(x)(1-f(x))$）</p>
<p>将结果反代得到</p>
<script type="math/tex; mode=display">\Delta w_{hj} = \eta g_j b_h</script><p>同理可得</p>
<script type="math/tex; mode=display">\Delta \theta_j = -\eta g_j,</script><script type="math/tex; mode=display">\Delta v_{ih} = \eta e_h x_i,</script><script type="math/tex; mode=display">\Delta \gamma _h = - \eta e_h.</script><p>其中</p>
<script type="math/tex; mode=display">e_h = -\frac{\partial E_k}{\partial b_h} \cdot \frac{\partial b_h}{\partial \alpha _h} = b_h(1 - b_h) \sum ^l_{j=1} w_{hj}g_j</script><p>确定变量的更新方式之后，即可按照流程构造神经网络算法。（训练集$D = {(x<em>k,y_k)}^m</em>{k=1}$，学习率$\eta$）</p>
<figure class="highlight vb"><table><tr><td class="code"><pre><span class="line">输入：训练集D；学习率eta</span><br><span class="line">过程：</span><br><span class="line">在(<span class="number">0</span>,<span class="number">1</span>)范围内随机初始化所有连接权和阈值</span><br><span class="line">repeat</span><br><span class="line">    <span class="keyword">for</span> all (xk,yk) <span class="keyword">in</span> D <span class="keyword">do</span></span><br><span class="line">        根据当前参数，计算当前样本的输出\hat y_k；</span><br><span class="line">        计算输出层的输出神经元的梯度项g_j；</span><br><span class="line">        计算隐层的输出神经元的梯度项e_h；</span><br><span class="line">        更新连接权和阈值</span><br><span class="line"><span class="keyword">end</span> <span class="keyword">for</span></span><br><span class="line"><span class="keyword">until</span> 达到停止条件</span><br><span class="line">输出：连接权与阈值确定的多层前馈神经网络。</span><br></pre></td></tr></table></figure>
<p>值得注意的是，以上考虑的使用基于单个的$E_k$的更新法则，将目标替换为使每个$E_k$达到最优。因此，如果针对于累积误差优化</p>
<script type="math/tex; mode=display">E = \frac{1}{m} \sum^m_{k=1}E_k</script><p>就得到了<em>累积误差逆传播</em>（accumulated error backpropagation）算法。以下对比<em>标准BP算法</em>与<em>累积BP算法</em>的特点：</p>
<p><strong>标准BP算法</strong>：一般来说，每次更新都只针对单个样例，参数更新的比较频繁，并且可能会对不同的样本产生“抵消”现象，导致如果要达到同样的累计误差极小点，需要进行更多次数的迭代；在累计误差下降到一定程度之后，相较之下，进一步下降依旧可以较快地得到更好的解，在样本集$D$非常大时更明显。</p>
<p><strong>累积BP算法</strong>（accumulated error backpropagation）：累积BP算法直接针对累积五擦汗最小化，在读取整个数据集$D$一遍后才对参数进行更新，参数更新频率低得多，但在累积误差下降到一定程度时，进一步下降过程将会极为缓慢。</p>
<hr>
<p><strong>缓解BP算法的过拟合</strong>：</p>
<ul>
<li><em>早停</em>（early stopping） 在训练过程中，不断测试训练集和验证集的误差，如果训练集误差减小，但验证集误差增高，则停止训练；</li>
<li><em>正则化</em> （regularization） 基本思想是在误差目标函数中增加一个用于描述网络复杂度的部分。例如：<script type="math/tex">E = \lambda \frac{1}{m} \sum^m_{k=1} E_k + (1 - \lambda ) \sum_i \omega_i^2</script>其中$\lambda \in (0,1)$用于对经验误差和网络复杂度的折中，常通过交叉验证法来估计。（增加连接权与阈值的平方这一项之后，算法将对较小的连接权和阈值产生偏好，从而使输出网络更加平滑，达到缓解过拟合的作用）</li>
</ul>
<h2 id="5-4-全局最小与局部最小"><a href="#5-4-全局最小与局部最小" class="headerlink" title="5.4 全局最小与局部最小"></a>5.4 全局最小与局部最小</h2><p>通常地，对于一个目标函数，在它的梯度上不断优化时，可以找到它的一个极小值点。如果该目标函数仅有一个极小值点，则该点即为全局最小值点，否则是局部最小点。因此，在梯度下降算法使用时，容易掉入局部最小点。有以下几种避免方法：</p>
<ul>
<li>使用多组不同的参数值初始化多个神经网络，按标准方法训练后，取其中误差最小的解作为最终参数。相当于从不同的起点出发，大概率可以收敛于不同的局部极小点，通过选择即可得出全局最小。</li>
<li>使用“<em>模拟退火</em>”（simulated annealing）技术。模拟退火技术允许在一定程度上接受较差的结果，从而有利于跳出局部最小。在爹地啊过程中，接受“次优解”的概率随着时间推移而降低，以保证算法稳定。</li>
<li>使用“<em>随机梯度下降</em>”（stochastic gradient descent），在标准梯度下降算法的基础上增加随机因素，从而使其在陷入局部最小点时，仍可能继续迭代跳出局部最小。</li>
<li>使用“<em>遗传算法</em>”（genetic algorithms）。</li>
</ul>
<p>需要注意的是，以上的方法虽然可以在一定程度上跳出局部最小，但由于都是启发式算法，没有理论保障。</p>
]]></content>
      <tags>
        <tag>《机器学习》</tag>
      </tags>
  </entry>
  <entry>
    <title>Chapter6 支持向量机</title>
    <url>/2019/09/20/Chapter6%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
    <content><![CDATA[<p>本文内容：</p>
<ul>
<li>间隔与支持向量</li>
<li>对偶问题</li>
<li>核函数</li>
<li>软间隔与正则化</li>
<li>支持向量回归</li>
<li>核方法</li>
<li>附录<ul>
<li>拉格朗日乘数法</li>
<li>二次规划</li>
</ul>
</li>
</ul>
<span id="more"></span>
<h2 id="6-1-间隔与支持向量"><a href="#6-1-间隔与支持向量" class="headerlink" title="6.1 间隔与支持向量"></a>6.1 间隔与支持向量</h2><p>对给定的训练集$D = {(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m)}, \ y_i \in {-1,+1}$，在样本空间中找到一个划分超平面，将不同类别的样本分开。</p>
<p><img src="https://i.loli.net/2019/09/03/mGFb8kqxBOV2rfv.png" alt="存在多个划分超平面将两类训练样本分开"></p>
<p>直观而言，应该是红色线的划分效果最好，因为该划分超平面对训练样本的局部扰动的“容忍”性最好。如果因训练集的局限性或噪声的因素，导致样本点更加接近划分超平面的话，对红色线划分结果影响最小，因此其鲁棒性最强，对未见过的示例泛化性能最好。</p>
<p>在样本空间中，划分超平面可以用如下线性方程描述：</p>
<script type="math/tex; mode=display">\omega^T x + b = 0</script><p>其中$\omega = (\omega_1;\omega_2;;\dots;\omega_d)$为法向量，决定超平面的方向；$b$为位移项，决定超平面与原点间的距离。将由$\omega$、$b$确定的超平面记作$(\omega,b)$，则样本中任意点到超平面的距离可写为</p>
<script type="math/tex; mode=display">r = \frac{|\omega^T x + b|}{\left \| \omega \right \|}</script><p>假设一个能对样本集正确分类的超平面$(\omega,b)$，对于$(x_i,y_i) \in D$，若$y_i=+1$，则有$\omega^T x + b &gt; 0$；若$y_i = -1$，则有$\omega^T x + b &lt; 0$。令</p>
<script type="math/tex; mode=display">\begin{cases}
\omega^T x + b \geqslant +1& ,\text{  } y_i= +1, \\
\omega^T x + b \leqslant +1 & ,\text{  } y_i= -1.
\end{cases}</script><p>距离超平面距离最近的几个样本使上式等号成立，其被称为“<em>支持向量</em>”（support vector），如下图所示。</p>
<p><img src="https://i.loli.net/2019/09/03/7DM1bECXtJpILxa.png" alt="支持向量与间隔"></p>
<p>其中，被圈中的点所对应的特征向量即为支持向量，而$\gamma$表示两个异类支持向量到超平面距离之和，被称为“<em>间隔</em>”（margin）。</p>
<script type="math/tex; mode=display">\gamma = \frac{2}{\left \| \omega \right \|}</script><p>根据分析，当超平面的间隔最大时，该超平面即为最优划分超平面。则</p>
<script type="math/tex; mode=display">\max _{\omega,b} \frac{2}{\left \| \omega \right \|}</script><script type="math/tex; mode=display">s.t.~~y_i(\omega^T x_i + b) \geqslant 1, ~~ i = 1,2,\dots,m.</script><p>显然，为了最大化间隔，需要最小化$\left | \omega \right |^{-1}$，等价于最小化$\left | \omega \right |^2$。所以，目标可以转化为</p>
<script type="math/tex; mode=display">\min _{\omega,b} \frac{1}{2} \left \| \omega \right \|^2</script><script type="math/tex; mode=display">s.t. ~~ y_i(\omega^T x_i + b) \geqslant 1, ~~ i = 1,2,\dots,m.</script><p>这就是<em>支持向量机</em>（Support Vector Machine，简称SVM）的基本型。</p>
<h2 id="6-2-对偶问题"><a href="#6-2-对偶问题" class="headerlink" title="6.2 对偶问题"></a>6.2 对偶问题</h2><p>我们希望通过求解上述SVM的基本型来得到大间隔划分超平面所对应的模型</p>
<script type="math/tex; mode=display">f(x) = \omega^T x + b</script><p>其中$\omega$和$b$是模型参数。显然，SVM的基本型是一个<em>凸二次规划</em>（convex quadratic programming）问题，将使用如下方法进行求解。</p>
<p>将支持向量机中的划分超平面看作约束条件，并将SVM的基本型转化为其拉格朗日对偶函数（$\alpha _i \geqslant 0, i = 1,2,\dots,m$）$^{[1]}$</p>
<script type="math/tex; mode=display">L(\omega,b,\alpha) = \frac{1}{2} \left \| \omega \right \|^2 + \sum^m_{i=1} \alpha_i[1-y_i(\omega^Tx_i + b)]</script><p>令$L(\omega,b,\alpha)$对$\omega$和$b$的偏导为零可得</p>
<script type="math/tex; mode=display">\omega = \sum^m_{i=1} \alpha_i y_i x_i</script><script type="math/tex; mode=display">\sum^m_{i=1} \alpha_i y_i = 0</script><p>将两式作为对偶函数的约束条件代入，可将$L(\omega,b,\alpha)$中的$\omega$和$b$消去，于是SVM基本型的对偶问题即为</p>
<script type="math/tex; mode=display">\max_\alpha \sum^m_{i=1} \alpha_i - \frac{1}{2} \sum^m_{i=1} \sum^m_{j=1}\alpha_i \alpha_j y_i y_j x_i^T x_j</script><script type="math/tex; mode=display">s.t. \ \ \sum^m_{i=1} \alpha_i y_i = 0,</script><script type="math/tex; mode=display">\alpha_i \geqslant 0, \ \ i = 1,2,\dots,m.</script><p>解出$\alpha$后，即可得到模型</p>
<script type="math/tex; mode=display">f(x) = \omega^T x + b = \sum^m_{i=1} \alpha_i y_i x_i^T x_i + b</script><p>同时，还需注意到的是，在原问题转化为拉格朗日对偶问题过程中，需要额外满足约束条件——KKT条件，即</p>
<script type="math/tex; mode=display">\begin{cases} \alpha_i \geqslant 0 \\ y_if(x_i) - 1 \geqslant 0 \\ \alpha_i(y_if(x_i)-1) = 0 \end{cases}</script><p>显然，当$y_if(x_i) = 1$时，$\alpha_i &gt; 0$，这时该点才会对问题产生影响；否则，$y_if(x_i) \neq 1$，$\alpha_i = 0$，对问题无影响。又因为$y_if(x_i)=1$时，即为样本点在最大间隔边界上的情况，即对应支持向量，所以这种训练方法被称为支持向量机。</p>
<p>总而言之，支持向量机在训练完成后，大部分的训练样本都不需要保留，最终模型仅与支持向量有关。</p>
<p>而在过程中，SVM基本型的对偶问题显然是一个二次规划问题$^{[2]}$，虽然可以直接使用二次规划算法对其求解，但考虑该问题具有特殊性，于是有人提出了针对于此类问题的高效算法，其中SMO是一个著名的代表。</p>
<p><strong>序列最小优化算法</strong>（Sequential Minimal Optimization）：基本思路为，先固定$\alpha<em>i$之外的所有参数，求$\alpha_i$上的极值，由于存在约束$\sum^m</em>{i=1} \alpha_i y_i = 0$，若固定$\alpha_i$之外的其他变量，则$\alpha_i$可由其他变量导出。于是，SMO每次选择两个变量$\alpha_i$和$\alpha_j$，并固定其他参数。之后，SMO不断执行如下两步直至收敛：</p>
<ul>
<li>随机选取两变量$\alpha_i$，$\alpha_j$；</li>
<li>固定$\alpha_i$，$\alpha_j$之外的参数，并求解二次规划，获得更新后的$\alpha_i$和$\alpha_j$值.</li>
</ul>
<p>在固定其他变量的前提下，试验证明，如果选取的$\alpha_i$、$\alpha_j$中有一个不符合KKT条件，在迭代后目标函数将会增大。直观而言，该变量对KKT条件违背的越大，更新后的目标函数值增加越大。因此，SMO先选取违背KKT条件最大的变量，和使目标函数增长最快的变量。这里SMO采用了启发式算法：选取对应样本之间间隔最大的两变量，以此代替复杂度较高的比较目标函数增幅操作。直观的解释是：两个差别很大的变量，与对应两个十分相似的变量相比，对它们进行更新会带给目标函数值更大的变化。</p>
<p>SMO算法的高效之处在于，其固定其他参数后，仅考虑$\alpha_i$、$\alpha_j$两个变量时，优化过程十分高效。</p>
<p>仅考虑$\alpha_i$、$\alpha_j$两个变量时，上述对偶问题中的约束条件可重写为</p>
<script type="math/tex; mode=display">\alpha_i y_i + \alpha_j y_j = c \ \ , \ \ \alpha_i \geqslant 0 \ \ , \ \ \alpha_j \geqslant 0</script><p>其中</p>
<script type="math/tex; mode=display">c = -\sum^m_{k \neq i,j} \alpha_k y_k</script><p>是使$\sum^m_{i=1} \alpha_i y_i$成立的常数。用</p>
<script type="math/tex; mode=display">\alpha_i y_i + \alpha_j y_j = c</script><p>消去上述对偶问题目标函数中的变量$\alpha_j$，此时仅有约束$\alpha_i \geqslant 0$，于是得到具有闭式解的二次规划问题，可以高效地计算出更新后的$\alpha_i$、$\alpha_j$。</p>
<p>之后，可以通过对任意支持向量$(x_s,y_s)$的条件$y_s f(x_s) = 1$来求解$b$的值，即</p>
<script type="math/tex; mode=display">y_s(\sum^m_{i \in S} \alpha_i y_i x^T_ix_s + b) = 1</script><p>其中，$S = {i|\alpha_i &gt; 0, i = 1,2,\dots,m}$，是所有支持向量的下标集。显然，可以通过取任意的支持向量对$b$的值求解，但有一种更鲁棒的解法，就是对所有支持向量计算得到的$b$取平均值</p>
<script type="math/tex; mode=display">b = \frac{1}{|S|}\sum_{s \in S}(\frac{1}{y_s} - \sum_{i \in S}\alpha_iy_ix_i^Tx_s)</script><h2 id="6-3-核函数"><a href="#6-3-核函数" class="headerlink" title="6.3 核函数"></a>6.3 核函数</h2><p>一般地，在原始样本空内不存在一个能正确划分两类样本的超平面（“异或”问题）。对此，可通过非线性映射将样本从原始空间映射到更高维的特征空间，使样本在这个特征空间内线性可分。</p>
<script type="math/tex; mode=display">x \mapsto \phi(x)</script><p>如果原始空间是有限维，即属性数有限，那么必然存在一个高维特征空间使样本可分。在特征空间划分超平面所对应的模型</p>
<script type="math/tex; mode=display">f(x) = \omega^T \phi(x) + b</script><p>于是，原问题可改写为</p>
<script type="math/tex; mode=display">\min_{w,b} \frac{1}{2} \left \| \omega \right \|</script><script type="math/tex; mode=display">s.t. \ \ \ y_i(\omega^T \phi(x) + b) \geqslant 1</script><p>那么，其对偶问题为</p>
<script type="math/tex; mode=display">\max_\alpha \sum^m_{i=1}\alpha_i - \frac{1}{2} \sum^m_{i=1}\sum^m_{j=1} \alpha_i \alpha_j y_i y_j \phi(x_i)^T \phi(x_j)</script><script type="math/tex; mode=display">\!\!\! s.t. \ \ \ \sum^m_{i=1} \alpha_i y_i = 0 \ ,</script><script type="math/tex; mode=display">\alpha_i \geqslant 0 \ , \ i = 1,2,\dots,m.</script><p>由于是在$\phi(x)$构造的高维空间内，计算$\phi(x_i)^T \phi(x_j)$内积通常是极为困难的。为了避开这个障碍，通过定义核函数</p>
<script type="math/tex; mode=display">\kappa(x_i,x_j) = \phi(x_i)^T \phi(x_j)</script><p>用$\kappa(x_i,x_j)$代替其复杂的高维运算，使其在原始空间内进行计算，这种方法称为“<em>核技巧</em>”（kernel trick）。</p>
<p>进而，对偶函数可重写为</p>
<script type="math/tex; mode=display">\max_\alpha \sum^m_{i=1}\alpha_i - \frac{1}{2} \sum^m_{i=1} \sum^m_{j=1} \alpha_i \alpha_j y_i y_j \kappa(x_i,x_j)</script><script type="math/tex; mode=display">\!\!\! s.t. \ \ \ \sum^m_{i=1} \alpha_i y_i = 0 \ ,</script><script type="math/tex; mode=display">\alpha_i \geqslant 0 \ , \ i = 1,2,\dots,m.</script><p>解得$\alpha_i$后，可得到</p>
<script type="math/tex; mode=display">\begin{aligned} f(x) &= \omega ^T \phi (x) + b \\ &= \sum^m_{i=1} \alpha_iy_i \phi(x_i)^T \phi(x) + b \\ &= \sum^m_{i=1} \alpha_i y_i \kappa(x_i,x) + b \end{aligned}</script><p>显然，模型最优解可通过训练样本的核函数展开，这一展式亦称为“<em>支持向量展式</em>”（support vector expansion）。下面讨论核函数的确定及其性质。</p>
<p><strong>核函数 定理1</strong>  令$\chi$为输入空间，$\kappa(\cdot,\cdot)$是定义在$\chi \times \chi$上的对称函数（即$f(x_i,x_j) = f(x_j,x_i)$的函数），则$\kappa$是核函数当且仅当对于任意数据$D = {x_1,x_2,\dots,x_m}$，“<em>核矩阵</em>”（kernel matrix）总是正定的：</p>
<script type="math/tex; mode=display">K = \begin{bmatrix}
\kappa(x_1,x_1) & \dots & \kappa(x_1,x_j) & \dots & \kappa(x_1,x_m)\\
\vdots & \ddots & \vdots & \ddots & \vdots\\
\kappa(x_i,x_1) & \dots & \kappa(x_i,x_j) & \dots & \kappa(x_i,x_m)\\
\vdots & \ddots & \vdots & \ddots & \vdots\\
\kappa(x_m,x_1) & \dots & \kappa(x_m,x_j) & \dots & \kappa(x_m,x_m)
\end{bmatrix}</script><p>上述定理表明，只要一个对称函数所对应的核矩阵是半正定的，它就能作为核函数使用。实际上，对任意的核矩阵都能找到一个映射关系$\phi$与之对应。也就是说，任何一个核函数都隐式地定义了一个称为“<em>再生核希尔伯特空间</em>”（Reproducing Kernel Hilbert Space，简称RKHS）的特征空间。</p>
<p>经过上述分析，我们希望样本能在特征空间内线性可分，因此，支持向量机的性能与特征空间的选取有很大的关系。一般地，我们不知道特征映射的形式，也就无从选择合适的核函数，而且核函数只是隐式地定义了这个特征空间。所以，“核函数选择”是支持向量机中的最大变数。</p>
<blockquote>
<p>这方面有一些基本经验，例如对文本数据通常采用线性核，情况不明时可先尝试高斯核，以下是常用核函数表</p>
</blockquote>
<p><img src="https://i.loli.net/2019/09/10/1dl4hAucpwbI6oQ.jpg" alt="常用核函数"></p>
<p>此外，核函数还可以通过组合得到，如：</p>
<ul>
<li>若$\kappa_1$和$\kappa_2$为核函数，则对于任意正数$\gamma_1$、$\gamma_2$，其线性组合<script type="math/tex">\gamma_1 \kappa_1 + \gamma_2 \kappa_2</script>也是核函数；</li>
<li>若$\kappa_1$和$\kappa_2$为核函数，则核函数的直积<script type="math/tex">\kappa_1 \bigotimes \kappa_2(x,z) = \kappa_1(x,z)\kappa_2(x,z)</script>也是核函数；</li>
<li>若$\kappa_1$为核函数，则对于任意函数$g(x)$，<script type="math/tex">\kappa(x,z) = g(x)\kappa_1(x,z)g(z)</script>也是核函数。</li>
</ul>
<h2 id="6-4-软间隔与正则化"><a href="#6-4-软间隔与正则化" class="headerlink" title="6.4 软间隔与正则化"></a>6.4 软间隔与正则化</h2><p>前面的讨论中，一直假定训练样本在训练空间或特征空间中是线性可分的，但在现实任务中，往往很难确定合适的核函数，并且即便找到了，这个结果也有过拟合的可能。</p>
<p>缓解该问题的一个办法是允许支持向量机在一些样本上出错，因此，引入“<em>软间隔</em>”（soft margin）的概念。</p>
<p>我们将之前所使用的间隔概念，即满足约束条件</p>
<script type="math/tex; mode=display">\begin{cases} \omega^T x_i + b \geqslant +1, & y_i = +1; \\ \omega^T x_i + b \leqslant -1, & y_i = -1. \end{cases}</script><p>称为“<em>硬间隔</em>”（hard margin）。相对地，软间隔则允许一部分样本不满足约束。</p>
<p>当然在能够最大化间隔的同时，要尽量减少不满足约束的样本个数，由此问题优化目标可重写为</p>
<script type="math/tex; mode=display">\min_{\omega,b} \frac{1}{2} \left \| \omega \right \|^2 + C\sum^m_{i=1} \ell _{0/1}(y_i(\omega^Tx_i + b)-1)</script><p>其中$C&gt;0$是一个常数，$\ell_{0/1}$是“0/1损失函数”。</p>
<script type="math/tex; mode=display">y = \begin{cases} 1 ,& \text{if  } \  z < 0; \\ 0,& \text{otherwise.} \end{cases}</script><p>显然，当$C \to \infin$时，所有的样本点均需满足约束条件；而$C$取有限值时，可以在一定范围内接受部分样本不满足约束。</p>
<p>按照惯例，$\ell<em>{0/1}$具有非凸、不连续等不太好的数学性质，所以需要使用其他损失函数来代替，以便于问题的求解。代替的函数被称为“<em>替代损失</em>”（surrogate loss），替代损失函数一般是连续的凸函数，且是$\ell</em>{0/1}$的上界。</p>
<p><img src="https://i.loli.net/2019/09/07/oLuOtFQmWiyA14H.png" alt="三种损失函数"></p>
<p>上图中的损失函数分别对应：</p>
<ul>
<li>红色，0/1损失函数：$\ell_{0/1}(z)$；</li>
<li>黑色，hinge损失：$\ell_{hinge}(z) = \max(0,1-z)$；</li>
<li>绿色，指数损失（exponential loss）：$\ell_{exp}(z) = \exp(-z)$；</li>
<li>绿色，对率损失（logistic loss）：$\ell_{log}(z) = log(1+\exp(-z))$.</li>
</ul>
<p>采用hinge损失代替0/1损失，同时引入松弛变量$\xi_i \geqslant 0$，得到常用的“软间隔支持向量机”</p>
<script type="math/tex; mode=display">\min_{\omega,b,\xi_i} \frac{1}{2} \left \| \omega \right \|^2 + C \sum^m_{i=1} \xi_i</script><script type="math/tex; mode=display">\text{s.t. }\ y_i(\omega^Tx_i + b) \geqslant 1-\xi_i</script><script type="math/tex; mode=display">\xi \geqslant 0, \ \ i=1,2,\dots,m.</script><p>其中$\xi_i = max(0,1-y_i(\omega^Tx_i + b))$。之后的计算与之前无损失函数时的相同，区别仅为此时$0 \leqslant \alpha \leqslant C$，其他过程详见<strong>6.3 核函数</strong>一节。</p>
<p>值得注意的是，在使用对率损失代替0/1损失时，得到的模型类似于对率回归模型。实际上，支持向量机与对率回归的优化目标相近，通常情况下性能也相仿。但相比之下，一方面，对率回归的输出具有概率意义；可直接用于多分类任务。另一方面，hinge损失的特性使支持向量机的解具有稀疏性，而对率损失是光滑单调的递减函数，不具备类似支持向量机的概念，所以其需要依赖更多的训练样本，预测开销更大。</p>
<p>使用其他的损失函数替代0/1损失函数所得到的模型，其特点与替代损失函数直接相关，但它们具有一个共性：优化目标中的第一项用来描述划分超平面的“间隔”大小，另一项用来表述训练集上的误差，可写为更一般的形式</p>
<script type="math/tex; mode=display">\min_f \Omega(f) + C\sum^m_{i=1} \ell(f(x_i),y_i)</script><p>其中$\Omega(f)$称为“<em>结构风险</em>”（strutural risk），由模型$f$的性质决定；第二项$\sum^m_{i=1} \ell(f(x_i),y_i)$称为“<em>经验风险</em>”（empirical risk），由模型和训练数据的契合程度决定；$C$用于对二者进行折中，为引入使用者的意图提供途径，同时有助于削减假设空间，降低了过拟合的风险。上式代表的问题称为“<em>正则化</em>”（regularization）问题，$\Omega(f)$称为正则化项，$C$称为正则化常数。$L_p$范数是常用的正则化项，其中$L_0$、$L_1$范数倾向于$\omega$的分量尽量稀疏，$L_2$范数倾向于$\omega$的分量尽量稠密。</p>
<h2 id="6-5-支持向量回归"><a href="#6-5-支持向量回归" class="headerlink" title="6.5 支持向量回归"></a>6.5 支持向量回归</h2><p>考虑支持向量方法的回归问题，给定训练样本$D = {(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m)}$，$y \in \mathbb{R}$，希望学得一个形如$f(x) = \omega^Tx + b$的回归模型，使得$f(x)$与$y$尽可能地接近，$\omega$和$b$是待确定的参数。</p>
<p>支持向量回归（Support Vector Regression，简称SVR）与传统回归方法的区别在于损失的计算方法不同，传统回归方法是计算每个样本的预测输出$f(x)$与真实输出$y$之间的差别作为损失，仅当$f(x) = y$时损失才为零；而支持向量回归允许在一定范围$\epsilon$内认为$f(x)$与$y$的值无差别，即仅当$f(x)$与$y$之间的差别绝对值大于$\epsilon$时，才会计算损失。</p>
<p>于是，SVR问题可形式化为</p>
<script type="math/tex; mode=display">\min_{\omega,b} \frac{1}{2} \left \| \omega \right \|^2 + C\sum^m_{i=1}\ell_{\epsilon}(f(x_i)-y_i)</script><p>其中，$C$为正则化常数，$\ell_{\epsilon}$是$\epsilon$-不敏感（$\epsilon$-insensitive loss）函数</p>
<script type="math/tex; mode=display">\ell_{\epsilon} = \begin{cases} 0, &\text{if } |z| \leqslant \epsilon, \\ |z|-\epsilon, &\text{otherwise.} \end{cases}</script><p>引入松弛变量$\xi_i$和$\hat\xi_i$将问题重写为</p>
<script type="math/tex; mode=display">\min_{\omega,b,\xi_i,\hat\xi_i}\frac{1}{2} \left \| \omega \right \|^2 + C \sum^m_{i=1}(\xi_i + \hat \xi_i)</script><script type="math/tex; mode=display">\text{s.t. } \ f(x_i) - y_i \leqslant \epsilon + \xi_i</script><script type="math/tex; mode=display">y_i - f(x_i) \leqslant \epsilon + \hat\xi_i</script><script type="math/tex; mode=display">\xi \geqslant 0, \ \hat\xi_i \geqslant 0, \ i=1,2,\dots,m.</script><p>之后的求解及优化等计算与支持向量机一致，不再赘述。值得注意的是，在支持向量机中的方法，都可以应用到支持向量回归中，例如求解更鲁棒的$b$值，使用特征空间映射选取核函数等。</p>
<h2 id="6-6-核方法"><a href="#6-6-核方法" class="headerlink" title="6.6 核方法"></a>6.6 核方法</h2><p>回顾SVM和SVR模型，给定训练样本${(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m)}$，发现二者均能表示成$\kappa(x,x_i)$的线性组合，并且总结出如下结论：</p>
<p><strong>核方法 定理2 表示定理</strong> 令$\mathbb{H}$为核函数$\kappa$对应的再生核希尔伯特空间，$\left | h \right |_{\mathbb{H}}$表示$\mathbb{H}$空间中关于$h$的范数，对于任意单调递增函数$\Omega:[0,\infin] \mapsto \mathbb{R}$和任意非负损失函数$\ell:\mathbb{R}^m \mapsto [0,\infin]$，优化问题</p>
<script type="math/tex; mode=display">\min_{h \in \mathbb{H}} F(h) = \Omega(\left \| h \right \|_{\mathbb{H}} + \ell(h(x_1),h(x_2),\dots,h(x_m))</script><p>的解总可写为</p>
<script type="math/tex; mode=display">h^*(x) = \sum^m_{i=1}\alpha_i\kappa(x,x_i)</script><p>表示定理对损失函数没有限制，对正则化项$\Omega$仅要求单调递增（不一定是凸函数），这显示出核函数的巨大威力。</p>
<p>基于核函数的一系列学习方法，统称为“<em>核方法</em>”（kernel methods）。最常见的是，通过引入核函数将线性学习器拓展为非线性学习器，例如通过核化将线性判别分析（LDA）转化为“<em>核线性判别分析</em>”（Kernelized Linear Discriminant Analysis，简称KLDA）。<em>此处不再证明</em></p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="1-拉格朗日乘数法"><a href="#1-拉格朗日乘数法" class="headerlink" title="[1] 拉格朗日乘数法"></a>[1] 拉格朗日乘数法</h3><p><strong>拉格朗日乘数法</strong>（Lagrange multipliers）：一种寻找多元函数在一组约束条件下的极值的方法。通过引入拉格朗日乘数$\lambda$，可将有$d$个变量与$k$个约束条件的最优化问题转化为具有$d+k$个变量的无约束优化问题求解。</p>
<p>对于一个等式约束的优化问题，假定$x$是$d$维向量，需寻找最优$x^*$使目标函数$f(x)$最小且满足$g(x)=0$。从几何角度解释，我们需要在约束函数确定的$d-1$维曲面上寻找使目标函数最小（大）化的点（该曲面称为约束曲面）。于是可以得到如下结论：</p>
<ul>
<li>对于约束曲面上的任意点$x$，该点的梯度$\triangledown g(x)$正交于约束曲面；</li>
<li>在最优点$x^<em>$，目标函数在该点的梯度$\triangledown f(x^ </em>)$正交于约束曲面。</li>
</ul>
<p>由此可知，在最优点$x_*$，梯度$\triangledown g(x)$和$\triangledown f(x)$的方向必相同或相反，即存在$\lambda \neq 0$使得</p>
<script type="math/tex; mode=display">\triangledown f(x^ *) + \lambda \triangledown g(x^ *) = 0,</script><p>$\lambda$称为拉格朗日乘数，定义拉格朗日函数</p>
<script type="math/tex; mode=display">L(x,\lambda) = f(x) + \lambda g(x)</script><p>而对于不等式约束$g(x) \leqslant 0$时，最优点$x^<em>$在$g(x) &lt; 0$的区域中或在边界$g(x) = 0$上。当$g(x) &lt; 0$时，约束$g(x) \leqslant 0$不起作用，可以直接使$\triangledown f(x) = 0$来获得最优点；$g(x) = 0$时，即相当于等式情况，但需注意的是这时使$\triangledown f(x^</em>) + \lambda \triangledown g(x^ *) = 0$成立的$\lambda &gt; 0$。整合两种情况，发现必然存在$\lambda g(x) = 0$。因此，在约束$g(x) \leqslant 0$下最小化$f(x)$，可转化为在如下约束下最小化目标函数的拉格朗日函数：</p>
<script type="math/tex; mode=display">
\begin{cases}
g(x) \leqslant 0; \\
\lambda \geqslant 0; \\
\lambda g(x) = 0.
\end{cases}</script><p>以上条件称为<strong>Karush-Kuhn-Tucker（简称KKT）条件</strong>。</p>
<p>之后，可以将以上做法推广到多个约束，考虑具有$m$个等式约束和$n$个不等式约束且可行域$\mathbb{D} \subset \mathbb{R}^d$非空的优化问题</p>
<script type="math/tex; mode=display">\min _x f(x)</script><script type="math/tex; mode=display">s.t. \ \ h_i(x) = 0 \ \ (i = 1,...,m),</script><script type="math/tex; mode=display">g_j(x) \leqslant 0 \ \ (j = 1,...,n).</script><p>引入拉格朗日乘数$\lambda = (\lambda_1, \lambda_2,\dots, \lambda_m)^T$和$\mu = (\mu_1, \mu_2, \dots,\mu_n)^T$，相应的拉格朗日函数为</p>
<script type="math/tex; mode=display">L(x,\mathbf{\lambda},\mathbf{\mu}) = f(x) + \sum^m_{i=1} \lambda_i h_i(x) + \sum^n_{j=1} \mu_j g_j(x)</script><p>同时，由不等式引入的KKT条件（$j=1,2,\dots,n$）为</p>
<script type="math/tex; mode=display">\begin{cases} g_j(x) \leqslant 0; \\ \mu_j \geqslant 0; \\ \mu_j g_j(x) = 0. \end{cases}</script><p>对于优化问题，可以从“<em>主问题</em>”（primal problem）和“<em>对偶问题</em>”（dual problem）两个方面考虑。对该优化问题而言，可基于以上拉格朗日函数，构造其拉格朗日“<em>对偶函数</em>”（dual function）$\Gamma:\mathbb{R}^m \times \mathbb{R}^n \mapsto \mathbb{R}$定义为</p>
<script type="math/tex; mode=display">\begin{aligned}\Gamma(\lambda,\mu) &= \inf_{x \in \mathbb{D}} L(x,\lambda,\mu) \\ &=\inf_{x \in \mathbb{D}} (f(x) + \sum^m_{i=1} \lambda_i h_i(x) + \sum^n_{j=1}\mu_j g_j(x) \end{aligned}</script><p>上式中$\inf-$表示<em>下确界</em>（infimum）。</p>
<p>若$\tilde x \in \mathbb{D}$为主问题可行域中的点，则对任意$\mu \succeq 0$和$\lambda$都有</p>
<script type="math/tex; mode=display">\sum^m_{i=1} \lambda_i h_i(x) + \sum^n_{j=1}\mu_j g_j(x) \leqslant 0</script><p>进而有</p>
<script type="math/tex; mode=display">\Gamma (\lambda,\mu) = \inf _{x \in \mathbb{D}} L(x,\lambda,\mu) \leqslant L(\tilde x,\lambda,\mu) \leqslant f(\tilde x).</script><p>若主问题$min_x f(x)$最优值为$p^*$，则对任意$\mu \succeq 0$和$\lambda$都有</p>
<script type="math/tex; mode=display">\Gamma (\lambda,\mu) \leqslant p^*</script><p>即对偶函数给初了主问题最优值的下界，且这个下界取决于$\mu$和$\lambda$的值。于是，很自然地想到对偶函数能获得的最好下界是什么？这就引出优化问题</p>
<script type="math/tex; mode=display">\max_{\lambda,\mu} \Gamma (\lambda,\mu) \ \ s.t. \mu \succeq 0.</script><p>上式即为主问题的对偶问题，其中$\lambda$和$\mu$称为“<em>对偶变量</em>”（dual variable）。无论主问题的凸性如何，对偶问题始终是凸优化问题。</p>
<p>考虑对偶问题</p>
<script type="math/tex; mode=display">\max_{\lambda,\mu} \Gamma(\lambda,\mu) = d^*</script><p>显然有$d^<em> \leqslant p^</em> $，这被称为“<em>弱对偶性</em>”（weak duality）；若$d^<em> = p^</em> $成立，则称该问题具有“<em>强对偶性</em>”（strong duality），此时该对偶问题能获得主问题的最优下界。对一般问题而言，强对偶性通常不成立，但若满足Slater条件，则强对偶性必然成立，且此时有</p>
<script type="math/tex; mode=display">\sum^m_{i=1} \lambda_i h_i(x) + \sum^n_{j=1}\mu_j g_j(x) = 0</script><p>之后，将拉格朗日函数分别对原变量和对偶变量求导，并令导数为0，即可解决。</p>
<h3 id="2-二次规划"><a href="#2-二次规划" class="headerlink" title="[2] 二次规划"></a>[2] 二次规划</h3><p><strong>二次规划</strong>（Quadratic Programming）：目标函数是变量的二次函数，约束条件是变量的线性不等式的一类优化问题。</p>
<p>假定变量个数为$d$，约束条件为$m$，则标准的二次规划问题形如：</p>
<script type="math/tex; mode=display">\min_x \frac{1}{2}x^TQx + c^Tx</script><script type="math/tex; mode=display">s.t. \ \ Ax \leqslant b</script><p>其中，$x$为$d$维向量，$Q \in R^{d \times d}$为实对称矩阵，$A \in R^{m \times d}$为实矩阵，$b \in R^m$和$c \in R^d$为实向量，$Ax \leqslant b$的每一行对应一个约束。</p>
<ul>
<li>Q—半正定矩阵时，目标函数为凸函数，若此时约束条件$Ax \leqslant b$的可行域非空，且目标函数在此可行域内有下界，则该问题有全局最小值。</li>
<li>Q—正定矩阵时，该问题有唯一最小值</li>
<li>Q—非正定矩阵时，目标函数将有多个平稳点和局部最小值，此时是NP难问题。</li>
</ul>
<p>对二次规划问题的常用解法有：椭球法（ellipsoid method）、内点法（interior point）、增广拉格朗日法（augmented Lagrangian）、梯度投影法（gradient projection）等。若Q为正定矩阵，则相应的二次规划问题可由椭球法在多项式时间内求解。</p>
]]></content>
      <tags>
        <tag>《机器学习》</tag>
      </tags>
  </entry>
  <entry>
    <title>Chapter7 贝叶斯分类器</title>
    <url>/2019/09/20/Chapter7%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</url>
    <content><![CDATA[<p>本文内容：</p>
<ul>
<li>贝叶斯决策论</li>
<li>极大似然估计</li>
<li>朴素贝叶斯分类器</li>
<li>半朴素贝叶斯分类器</li>
<li>贝叶斯网<ul>
<li>结构</li>
<li>学习</li>
<li>推断</li>
</ul>
</li>
<li>EM算法</li>
<li>附录<ul>
<li>参数估计</li>
</ul>
</li>
</ul>
<span id="more"></span>
<h2 id="7-1-贝叶斯决策论"><a href="#7-1-贝叶斯决策论" class="headerlink" title="7.1 贝叶斯决策论"></a>7.1 贝叶斯决策论</h2><p><strong>贝叶斯决策论</strong>（Bayesian decision theory）：是在概率框架下实施决策的基本方法。对于分类任务来说，在所有相关概率都已知的理想情况下，贝叶斯决策论考虑如何基于这些概率核误判损失来选择最优的类别标记。</p>
<p>以多分类任务为例：</p>
<p>假设有$N$种可能的类别标记，即$\gamma={c<em>1,c_2,\dots,c_N}$，$\lambda</em>{ij}$是将一个真实的$c_j$样本误分类为$c_j$所产生的损失，基于后验概率$P(c_i|x)$可获得将样本$x$分类为$c_i$所产生的<em>期望损失</em>（expected loss），即在样本$x$上的“<em>条件风险</em>”（conditional risk）</p>
<script type="math/tex; mode=display">R(c_i|x) = \sum^N_{j=1}\lambda_{ij}P(c_i|x)</script><p>目标：找到一个最小化总体风险的判定准则：$\chi \mapsto \gamma$</p>
<script type="math/tex; mode=display">R(h) = \mathbb{E}_x[R(h(x)|x)]</script><p>由此，即可得到<em>贝叶斯判定准则</em>（Bayes decision rule）：为最小化总体风险，只需在每个样本上选择能使条件风险$R(c|x)$最小的类别标记，即</p>
<script type="math/tex; mode=display">h^*(x) = \argmin_{c \in \gamma}R(c|x)</script><p>此时，$h^<em> $称为</em>贝叶斯最优分类器<em>（Bayes optimal classifier），对应的总体风险$R(h^</em> )$称为<em>贝叶斯风险</em>（Bayes risk）。则该模型精度的理论上限为$1-R(h^*)$。误判概率$\lambda_{ij}$为</p>
<script type="math/tex; mode=display">\lambda_{ij}=\begin{cases} 0, &\text{ if } i=j; \\ 1, & \text{ otherwise.}\end{cases}</script><p>于是，条件风险为</p>
<script type="math/tex; mode=display">R(c|x) = 1-P(c|x)</script><p>最小化分类错误的贝叶斯最优分类器为</p>
<script type="math/tex; mode=display">h^*(x) = \argmax_{c \in \gamma} P(c|x)</script><p>即对每个样本$x$，选择能使后验概率$P(c|x)$最大的类别标记。</p>
<p>由此可知，要想通过贝叶斯判定准则得到最小化风险的贝叶斯分类器，需要求得其后验概率$P(c|x)$。所以，此时机器学习的目标在于利用有限的样本集尽可能准确地估计出后验概率$P(c|x)$。对后验概率的估计主要分为两种策略：</p>
<ul>
<li><p><strong>判别式模型</strong>（discriminative models）：给定$x$，可直接建模$P(c|x)$来预测。例：决策树、BP神经网络、支持向量机等；</p>
</li>
<li><p><strong>生成式模型</strong>（generative models）：先对联合概率分布$P(x,c)$建模，然后再由此获得$P(c|x)$。</p>
</li>
</ul>
<p>基于贝叶斯定理，$P(c|x)$可写为</p>
<script type="math/tex; mode=display">P(c|x)= \frac{P(c)P(x|c)}{P(x)}</script><p>其中，$P(c)$是类“<em>先验</em>”（prior）概率；$P(x|c)$是样本$x$相对于类标记$c$的<em>类条件概率</em>（class-conditional probablity），或称为“<em>似然</em>”（likelihood）；$P(x)$是用于归一化的“<em>证据</em>”（evidence）因子。显然，$P(x)$与类标记无关，因此估计$P(c|x)$的问题即转化为基于训练数据$D$估计先验概率$P(c)$和似然$P(x|c)$。</p>
<p>类先验概率$P(c)$直接通过大数定律，用各类样本出现的频率进行估计。</p>
<p>而类条件概率$P(x|c)$，由于其涉及关于$x$全部属性的联合概率，直接根据样本出现的频率来估计将会遇到严重的困难。例如，假设样本有$d$个属性且都为二值，则样本空间将有$2^d$种可能的取值。一般地，样本数$m&lt;2^d$，这就造成很多样本的取值在训练集中未能出现，导致将其概率直接判为0，而事实上通常来说“未被观测到” $\neq$ “出现概率为零”。</p>
<h2 id="7-2-极大似然估计"><a href="#7-2-极大似然估计" class="headerlink" title="7.2 极大似然估计"></a>7.2 极大似然估计</h2><p>估计类的条件概率的一种常用策略为：先假定样本具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。记类别$c$的类条件概率为$P(x|c)$，且$P(x|c)$具有确定的形式被参数向量$\theta_c$唯一确定。因此我们的目标是利用数据集$D$估计$\theta_c$，而$P(x|c)$可记为$P(x|\theta_c)$。</p>
<p>事实上，概率模型的训练过程就是<em>参数估计</em>（parameter estimation）过程。此节中介绍的<em>极大似然估计</em>（Maximum Likelihood Estimation，简称MLE），是频率主义学派$^{[1]}$根据数据采样估计概率分布参数的经典方法。</p>
<p>令$D_c$表示训练集$D$中第$c$类样本组成的集合，假设其为独立同分布，则参数$\theta_c$对于数据集$D_c$的似然是</p>
<script type="math/tex; mode=display">P(D_c|\theta_c) = \prod_{x \in D_c}P(x|\theta_c)</script><p>对$\theta$进行极大似然估计，即寻找能最大化似然$P(D_c|\theta_c)$的参数值$\hat \theta_c$。也就是说，寻找使训练集数据出现概率最大的参数值$\hat \theta_c$。</p>
<p>同时，为避免上式中的连乘操作出现溢出，通常使用<em>对数似然</em>（log-likelihood）</p>
<script type="math/tex; mode=display">LL(\theta_c) = \sum_{x \in D_c} \log P(x|\theta_c)</script><p>此时参数$\theta_c$的极大似然估计$\hat \theta_c$为</p>
<script type="math/tex; mode=display">\hat \theta_c = \argmax_{\theta_c} LL(\theta_c)</script><p>需要注意的是，这种参数化的方法虽然能够使类条件概率估计变得相对简单，但其估计结果严重依赖于假设的总体分布形式是否符合潜在的概率分布形式。</p>
<h2 id="7-3-朴素贝叶斯分类器"><a href="#7-3-朴素贝叶斯分类器" class="headerlink" title="7.3 朴素贝叶斯分类器"></a>7.3 朴素贝叶斯分类器</h2><p>在<strong>7.1 贝叶斯决策论</strong>中发现，基于贝叶斯公式估计后验概率$P(c|x)$的主要困难在于：$P(c|x)$是所有属性上的联合概率，仅使用有限的训练样本集难以直接估计得到。而<em>朴素贝叶斯分类器</em>（naive Bayes classifier）采用“<em>属性条件独立性假设</em>”来避开计算所有属性上的联合概率。</p>
<p><strong>属性独立性假设</strong>（attribute conditional independence assumption）：每个属性独立地对分类结果发生影响。在此假设下，可以将原贝叶斯模型重写为</p>
<script type="math/tex; mode=display">P(c|x) = \frac{P(c)}{P(x)}\prod^d_{i=1} P(x_i|c)</script><p>于是根据属性独立性假设，构造朴素贝叶斯分类器</p>
<script type="math/tex; mode=display">h_{nb}(x) = \argmax_{c \in \gamma} P(c) \prod^d_{i=1} P(x_i|c)</script><p>令$D_c$表示训练集$D$中第$c$类样本组成的集合，其中</p>
<script type="math/tex; mode=display">P(c) = \frac{|D_c|}{|D|}</script><ul>
<li>对离散属性而言，令$D<em>{c,x_i}$表示$D_c$中在第$i$个属性上取值为$x_i$的样本组成的集合，则有 $$P(x_i|c) = \frac{|D</em>{c,x_i}|}{|D_c|}$$</li>
<li>对连续属性而言，可用概率密度函数代替$P(x<em>i|c)$，假定$p(x_i|c) \sim \Nu(\mu</em>{c,i}, \sigma^2_{c,i})$</li>
</ul>
<p>值得注意的是，只有在样本量足够多时，才能作出有意义的估计。</p>
<p>在试验中可以发现，可能会出现某些未出现过的属性值组合，其概率将会被直接判别为零，显然这是个不合理的现象。为了避免其他属性携带的信息直接被未出现过的属性“抹去”，需要在估计概率值时进行“<em>平滑</em>”（smoothing）处理，而“<em>拉普拉斯修正</em>”是比较常用的方法之一。</p>
<p><strong>拉普拉斯修正</strong>（Laplacian correction）：令$N$表示训练集$D$中可能的类别数，$N_i$表示第$i$个属性可能的取值数，则$P(c)$和$P(x_i|c)$分别修正为</p>
<script type="math/tex; mode=display">\hat P(c) = \frac{|D_c|+1}{|D|+N},</script><script type="math/tex; mode=display">\hat P(x_i|c) = \frac{|D_{c,x_i}|+1}{|D_c|+N_i}.</script><p>在较大的训练集中，拉普拉斯修正所造成的偏差将变得可忽略，使估计值更加接近实际值。</p>
<p>在现实使用中，朴素贝叶斯分类器有多种使用方式：</p>
<ul>
<li>任务对预测速度有要求，利用给定的训练集，事先计算并储存好要使用到的概率估计，在使用时以查表的方式调用即可；</li>
<li>任务数据更替频繁，采用“<em>懒惰学习</em>”（lazy learning）方式，即在预测时再对模型进行训练；</li>
<li>任务数据不断增加，采用增量学习的方式，即在现有估值的基础上，仅对新增样本涉及的属性进行概率值的计数修正。</li>
</ul>
<h2 id="7-4-半朴素贝叶斯分类器"><a href="#7-4-半朴素贝叶斯分类器" class="headerlink" title="7.4 半朴素贝叶斯分类器"></a>7.4 半朴素贝叶斯分类器</h2><p>在<strong>7.3 朴素贝叶斯分类器</strong>一节中提出，使用属性独立性假设避免计算复杂的联合概率密度，但由于属性独立性假设在现实中很难成立，使用时将会损失大量信息。于是，提出了折中的“<em>半朴素贝叶斯分类器</em>”学习方法。</p>
<p><strong>半朴素贝叶斯分类器</strong>（semi-naive Bayes classifier）：基本思想在于，对考虑完全的联合概率密度和属性独立之间进行折中，即考虑一部分属性间的相互依赖。“<em>独依赖估计</em>”（One-Dependent Estimator，简称ODE）是半朴素贝叶斯分类器最常用的一种策略，即假设每个属性在类别之外最多仅依赖于一个其他属性</p>
<script type="math/tex; mode=display">P(c|x) \propto P(c)\prod^d_{i=1}P(x_i|c,pa_i)</script><p>其中，$pa_i$为属性$x_i$所依赖的属性，称为$x_i$的父属性。针对某一属性，确定父属性之后即可确定该属性的估计概率值$P(x_i|c,pa_i)$。而对父属性的选取有多种方法，不同的方法将产生不同的独依赖分类器。</p>
<p><img src="https://i.loli.net/2019/09/09/Le2QEzWRKsHxYdo.png" alt="朴素贝叶斯与两种半朴素的比较"></p>
<p><strong>SPODE方法</strong>（Super-Parent ODE）：最直接的方法。直接假设所有属性都依赖于同一个属性，称作“<em>超父</em>”（super-parent），然后通过交叉验证等模型选择方法确定超父属性。</p>
<p><strong>TAN方法</strong>（Tree Argumented naive Bayes）：在<em>最大带权生成树</em>（maximum weighted spanning tree）算法的基础上，通过下列步骤将属性间的依赖关系约简为树形结构：</p>
<ol>
<li>计算任意两个属性之间的条件互信息（conditional mutual information）<script type="math/tex">I(x_i,x_j|y) = \sum_{x_i,x_j;c \in \gamma}P(x_i,x_j|c) \log \frac{P(x_i,x_j|c)}{P(x_i|c)P(x_j|c)};</script></li>
<li>以属性为结点构建完全图，任意两结点间的权重设置为$I(x_i,x_j|y)$；</li>
<li>构建此完全图的最大带权生成树，挑选根变量，将边设置为有向；</li>
<li>加入类别结点$y$，增加从$y$到每个属性的有向边。</li>
</ol>
<p>其中，条件互信息$I(x_i,x_j|y)$刻画了属性$x_i$和$x_j$在已知类别情况下的相关性，因此，通过最大生成树算法，TAN实际上仅保留了强相关属性之间的依赖性。</p>
<p><strong>AODE</strong>（Average One-Dependence Estimator）：一种基于集成学习机制、更为强大的独依赖分类器，尝试将每个属性作为超父来构建SPODE，然后将那些具有足够训练数据支撑的SPODE集成起来作为最终结果，即</p>
<script type="math/tex; mode=display">P(c|x) \propto \sum^d_{i=1,|D_{x_i}| \geqslant m'} P(c,x_i) \prod^d_{j=1} P(x_j|c,x_i)</script><p>其中，$D_{x_i}$是在第$i$个属性上取值为$x_i$的样本的集合，$m’$为阈值常数。显然，AODE需估计$P(c,x_i)$和$P(x_j|c,x_i)$，则有</p>
<script type="math/tex; mode=display">\hat P(c,x_i) = \frac{|D_{c,x_i}|+1}{|D|+ N \times N_i}</script><script type="math/tex; mode=display">\hat P(x_j|c,x_i) = \frac{|D_{c,x_i,x_j}|+1}{|D_{c,x_i}|+N_j}</script><p>其中，$N$是$D$中可能的类别数，$N<em>i$是第$i$个属性可能的取值数，$D</em>{c,x<em>i}$是类别为$c$且在第$i$个属性上取值为$x_i$的样本集合，$D</em>{c,x_i,x_j}$是类别为c且在第$i$和第$j$个属性取值分别为$x_i$和$x_j$的样本集合。</p>
<p>同理，还可将独依赖进一步放松到多个属性依赖的“高阶依赖”，即将$pa_i$替换为同时包含多个属性的$pa_i$向量，从而将ODE拓展为kDE。而随着k的增加，准确估计概率$P(x_i|y,pa_i)$所需的训练样本将以指数级增加。</p>
<h2 id="7-5-贝叶斯网"><a href="#7-5-贝叶斯网" class="headerlink" title="7.5 贝叶斯网"></a>7.5 贝叶斯网</h2><p>贝叶斯网（Bayesian network）又称<em>信念网</em>（belief network），借助<em>有向无环图</em>（Directed Acyclic Graph，简称DAG）来刻画属性之间的依赖关系，并使用<em>条件概率表</em>（Conditional Probability Table，简称CPT）描述属性的联合概率分布。</p>
<p>贝叶斯网$B$由两部分构成：结构$G$和参数$\Theta$，即有$B = \left \langle G,\Theta \right \rangle$。网格结构$G$是一个有向无环图，其每个结点对应一个属性，连线表示二者之间具有直接依赖关系；参数$\Theta$为依赖关系的量化，对应依赖的权重。假设属性$x_i$在$G$中的结点为$\pi_i$，则$\Theta$包含了每个属性的条件概率表</p>
<script type="math/tex; mode=display">\theta_{x_i|\pi_i} = P_B(x_i|\pi_i)</script><h3 id="7-5-1-结构"><a href="#7-5-1-结构" class="headerlink" title="7.5.1 结构"></a>7.5.1 结构</h3><p>贝叶斯网的结构有效地表达了属性间的独立性。给定父结点集，贝叶斯网假设每个属性与它的非后裔属性独立，则对$B = \left \langle G,\Theta \right \rangle$，属性$x_1,x_2,\dots,x_d$的联合概率密度为</p>
<script type="math/tex; mode=display">P_B(x_1,x_2,\dots,x_d) = \prod^d_{i=1}P(x_i|\pi_i) = \prod^d_{i=1}\theta_{x_i|pi_i}</script><p>其中，$x_2$和$x_3$在$x_1$的值确定时相互独立，被称为条件独立性，记作$x_2 \perp x_3 | x_1$。此外存在<em>边际独立性</em>（marginal independence）的情况，例如在V型结构中，$x_4$的值未知时，$x_1$和$x_2$相互独立；在$x_4$的值已知时，$x_1$和$x_2$必不独立，这种情况记作$x_1 \rVert x_2$。下为验证：</p>
<script type="math/tex; mode=display">\begin{aligned} P(x_1,x_2) &= \sum_{x_4}P(x_1,x_2,x_4) \\ &= \sum_{x_4}P(x_4|x_1,x_2)P(x_1)P(x_2) \\ &= P(x_1)P(x_2)\end{aligned}</script><p>在贝叶斯网中有三种典型的依赖关系：同父结构、V型结构、顺序结构。</p>
<p><img src="https://i.loli.net/2019/09/10/H7msyJlokT64UxQ.jpg" alt="贝叶斯网典型依赖关系"></p>
<p>常用“<em>有向分离</em>”（D-seperation），对有向图中的依赖关系进行分析：</p>
<ul>
<li>找出所有V型结构，在V型结构的两个父结点间加一条无向边；</li>
<li>将有向边转化为无向边.</li>
</ul>
<p>由此产生“<em>道德图</em>”（moral graph），连接父结点的过程被称为<em>道德化</em>（moralization）。</p>
<p>假定道德图中存在变量$x$，$y$和变量集合$z={z_i}$，如果$x$，$y$在$z$被去除后，分属两个连通的分支，则可说明$x \perp y|z$。需要注意的是，在划分前需要对图剪枝，仅保留有向图中$x$，$y$，$z$及它们的父结点。</p>
<h3 id="7-5-2-学习"><a href="#7-5-2-学习" class="headerlink" title="7.5.2 学习"></a>7.5.2 学习</h3><p>贝叶斯网的学习首要目标即寻找结构最恰当的贝叶斯网。常使用“<em>评分搜索</em>”实现：</p>
<p>首先，定义<em>评分函数</em>（score function），以评估贝叶斯网与训练数据的契合程度。评分函数中包含了对希望构造的贝叶斯网的偏好。</p>
<p>常用评分函数一般基于信息论准则，此类准则将学习问题看作数据压缩的过程，所以最优的贝叶斯网结构应对应最小的综合编码长度，这种方法被称为“<em>最小描述长度</em>”（Minimum Description Length，简称MDL）准则。</p>
<p>给定训练集$D={x_1,x_2,\dots,x_m}$，贝叶斯网$B = \left \langle G,\Theta \right \rangle$在$D$上的评分函数可写为</p>
<script type="math/tex; mode=display">S(B|D) = f(\theta)|B| - LL(B|D)</script><p>其中，$B$是贝叶斯网的参数个数；$f(\theta)$表示描述每个参数$\theta$所需的编码位数；而</p>
<script type="math/tex; mode=display">LL(B|D) = \sum^m_{i=1} \log P_B(x_i)</script><p>是贝叶斯网$B$的对数似然。其评分函数第一项是计算编码贝叶斯网$B$所需的编码位数，第二项是计算$B$所对应的概率分布$P_B$对$D$的描述有多好。从统计学角度理解，一项为结构风险，另一项为经验风险。</p>
<p><strong>AIC评分函数</strong>（Akaike Information Criterion score function）：使$f(\theta) = 1$，即可得到AIC评分函数</p>
<script type="math/tex; mode=display">\text{AIC}(B|D) = |B| - LL(B|D)</script><p><strong>BIC评分函数</strong>（Beyesian Information Criterion score function）：使$f(\theta) = \frac{1}{2}\log m$，即可得到BIC评分函数</p>
<script type="math/tex; mode=display">\text{BIC}(B|D) = \frac{\log m}{2}|B| - LL(B|D)</script><p>特殊地，使$f(\theta) = 0$时，评分函数退化为负对数似然，同时，学习任务退化为极大似然估计。</p>
<p>最后，只要确定网络结构$G$，评分函数第一项为常数，即将最小化$s(B|D)$转化为对参数$\Theta$的极大似然估计。而参数$\Theta$可以通过训练集数据的经验估计获得</p>
<script type="math/tex; mode=display">\theta_{x_i|\pi_i} = P(x_i|\pi_i)</script><p>因此，仅需对结构网络进行搜索，即可在训练集上计算得到最优参数。但是，在所有可能的结构中搜索最优贝叶斯网结构是NP难问题，所以需要特殊方法求近似解：</p>
<ul>
<li>贪心算法，从某个网络结构出发，每次调整一条边，直到评分函数值不再降低；</li>
<li>给网络结构增加约束来削减搜索空间，如TAN中限制为树结构。</li>
</ul>
<h3 id="7-5-3-推断"><a href="#7-5-3-推断" class="headerlink" title="7.5.3 推断"></a>7.5.3 推断</h3><p>在训练好贝叶斯网模型之后，就能用来回答“<em>查询</em>”（query），即通过一些属性变量的观测值来推测其他属性变量的取值。而通过已知变量的观测值来推测待查询变量的过程被称为“<em>推断</em>”（inference），已知变量观测值称为“<em>证据</em>”（evidence）。</p>
<p>在理想状态下，可以直接根据贝叶斯网定义的联合概率分布精确计算其后验概率，但这样的“精确推断”是NP难的。因此需要作出“近似推断”，在实际应用中，常用<em>吉布斯采样</em>（Gibbs sampling）来完成（另常用<em>变分推断</em>）。</p>
<p><strong>吉布斯采样</strong>（Gibbs sampling）：一种随机抽样方法。</p>
<p>有待查询变量$Q={Q_1,Q_2,\dots,Q_n}$，证据变量$E={E_1,E_2,\dots,E_n}$取值为$e={e_1,e_2,\dots,e_n}$。目标是计算后验概率$P(Q=q|E=e)$，其中$q={q_1,q_2,\dots,q_n}$。</p>
<p>吉布斯算法首先产生一个与证据$E=e$一致的样本$q^0$作为初始点，然后每步从当前样本出发产生下一个样本。即先设$q^t=q^{t-1}$，然后逐个改变其中的非证据变量并抽样取值，采样概率根据贝叶斯网和其他变量的当前取值（即$Z=z$）计算获得。经过$T$次采样得到的与$q$一致的样本共有$n_q$个，则可近似估算出后验概率：</p>
<script type="math/tex; mode=display">P(Q=q|E=e) \simeq \frac{n_q}{T}</script><p>实质上，可以看出，在吉布斯采样中，每一步都仅依赖于前一步的状态，即符合<em>马尔科夫链</em>（Markov chain）模型，其在贝叶斯网所有变量的联合状态空间与证据$E=e$一致的子空间中。在一定条件下，无论从什么状态开始，马尔可夫链在第$t$步的状态分布在$t \to \infin$时必收敛于一个<em>平稳分布</em>（stationary distribution）；对于吉布斯采样来说，这个分布恰好是$P(Q|E=e)$。因此，在$T$很大时，吉布斯采样相当于根据$P(Q|E=e)$采样，从而保证了上式收敛于$P(Q=q|E=e)$。以下为具体流程：</p>
<figure class="highlight vb"><table><tr><td class="code"><pre><span class="line">输入：贝叶斯网B=&lt;G,Theta&gt;；</span><br><span class="line">      采样次数T；</span><br><span class="line">      证据变量E及其取值e；</span><br><span class="line">      待查询变量Q及其取值q；</span><br><span class="line">过程：</span><br><span class="line">nq = <span class="number">0</span></span><br><span class="line">q0 = 对Q随机赋值</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span>,<span class="number">2</span>,...,T <span class="keyword">do</span></span><br><span class="line">    <span class="keyword">for</span> Qi <span class="keyword">in</span> Q <span class="keyword">do</span></span><br><span class="line">        Z = E U Q \ &#123;Qi&#125;</span><br><span class="line">        z = e U q(t-<span class="number">1</span>) \ &#123;qi(t-<span class="number">1</span>)&#125;</span><br><span class="line">        根据B计算分布PB(Qi|Z=z)</span><br><span class="line">        qi(t) = 根据PB(Qi|Z=z)采样所获Qi取值</span><br><span class="line">        q(t) = 将q(t-<span class="number">1</span>)中的qi(t-<span class="number">1</span>)用qi(t)替换</span><br><span class="line">    <span class="keyword">end</span> <span class="keyword">for</span></span><br><span class="line">    <span class="keyword">if</span> qt = q <span class="keyword">then</span></span><br><span class="line">        nq = nq + <span class="number">1</span></span><br><span class="line">    <span class="keyword">end</span> <span class="keyword">if</span></span><br><span class="line"><span class="keyword">end</span> <span class="keyword">for</span></span><br><span class="line">输出：P(Q=q|E=e) ≈ nq/T</span><br></pre></td></tr></table></figure>
<p>需要注意的是，马尔可夫链趋于平稳分布的过程很慢，导致吉布斯采样算法的收敛速度较慢。此外，如果贝叶斯网中存在极端概率，即0或1时，不能保证马尔可夫链一定能收敛，会导致吉布斯采样给出错误的估计。</p>
<h2 id="7-6-EM算法"><a href="#7-6-EM算法" class="headerlink" title="7.6 EM算法"></a>7.6 EM算法</h2><p>为解决在现实中需要使用未观测到的数据的训练样本，对模型的参数进行估计的问题，引入“<em>隐变量</em>”（latent variable），即未观察的变量的概念。</p>
<p>在已观测变量集$X$，隐变量集$Z$，模型参数$\Theta$的假设下，对$\Theta$进行极大似然估计，函数</p>
<script type="math/tex; mode=display">LL(\Theta |X,Z)=\ln P(X,Z|\Theta )</script><p>显然，由于$Z$是隐变量，上式无法求解。此时可以考虑通过计算$Z$的期望，来最大化已观察数据的对数“<em>边际似然</em>”（marginal likelihood）</p>
<script type="math/tex; mode=display">LL(\Theta,X) = \ln P(X|\Theta) = \ln \sum_Z P(X,Z|\Theta)</script><p>对参数隐变量进行估计常用的方法是EM（Expectation-Maximization）算法，它是一种迭代的方法。</p>
<p>EM算法原型：</p>
<p>以初始值$\Theta^0$为起点，对边际似然函数，可迭代执行以下步骤直至收敛：</p>
<ul>
<li>基于$\Theta^t$推断隐变量$Z$的期望，记为$Z^t$；</li>
<li>基于已观测变量$X$和$Z^t$对参数$\Theta$做极大似然估计，记为$\Theta ^{t+1}$.</li>
</ul>
<p>进一步，若我们不是取$Z$的期望，而是基于$\Theta^t$计算隐变量$Z$的概率分布$P(Z|X,\Theta^t)$，则EM算法的两个步骤是：</p>
<ul>
<li>E步（Expectation）：以当前参数$\Theta^t$推断隐变量分布$P(Z|X,\Theta^t)$，并计算对数似然$LL(\Theta|X,Z)$关于$Z$的期望；</li>
<li>M步（Maximization）：寻找最大化期望似然，即<script type="math/tex">\Theta^{t+1} = \argmax_\Theta Q(\Theta|\Theta^t).</script></li>
</ul>
<p>直观而言，最终EM算法将收敛到局部最优解，因此也可用梯度下降等优化算法对隐变量估计问题求解，但由于求和的项数将随着隐变量的数目以指数级上升，会给梯度计算带来麻烦；而EM算法则可以看作一种非梯度优化方法。</p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="1-参数估计"><a href="#1-参数估计" class="headerlink" title="[1] 参数估计"></a>[1] 参数估计</h3><p>统计学界的两个学派对参数估计提供不同的方案：</p>
<ul>
<li>频率主义学派：参数虽然未知，但却是客观存在的固定值，因此，可以通过优化似然函数等准则确定参数值；</li>
<li>贝叶斯学派：参数是未观察到的随机变量，其本身也存在分布，因此，可以通过假定参数服从一个先验分布，然后基于训练集计算参数的后验分布。</li>
</ul>
]]></content>
      <tags>
        <tag>《机器学习》</tag>
      </tags>
  </entry>
  <entry>
    <title>Chapter8 集成学习</title>
    <url>/2019/10/02/Chapter8%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>本文内容：</p>
<ul>
<li>个体与集成</li>
<li>Boosting<ul>
<li>权重更新公式</li>
<li>样本分布更新公式</li>
</ul>
</li>
<li>Bagging与随机森林<ul>
<li>Bagging</li>
<li>随机森林</li>
</ul>
</li>
<li>结合策略<ul>
<li>平均法</li>
<li>投票法</li>
<li>学习法</li>
</ul>
</li>
<li>多样性<ul>
<li>误差-分歧分解</li>
<li>多样性度量</li>
<li>多样性增强</li>
</ul>
</li>
</ul>
<span id="more"></span>
<h2 id="8-1-个体与集成"><a href="#8-1-个体与集成" class="headerlink" title="8.1 个体与集成"></a>8.1 个体与集成</h2><p><strong>集成学习</strong>（ensemble learning）：通过构建并结合多个学习器来完成学习任务的方法。又称多分类器系统（multi-classifier system）、基于委员会的学习（conmmittee-based learning）等。</p>
<p>集成学习以不同的<em>个体学习器</em>（individual learner）可分为两类：</p>
<ul>
<li>同质（homogeneous）集成：仅包含同种类型的学习器。其中，个体学习器称为“<em>基学习器</em>”（base learner），构成的算法称“<em>基学习算法</em>”（base learning algorithm）。</li>
<li>异质（heterogenous）集成：包含不同类型的学习器。其中，个体学习器称为“<em>组件学习器</em>”（component learner）。</li>
</ul>
<p>集成学习的强大在于结合多个学习器，常可获得比单一学习器显著优越的泛化性能。对“<em>弱学习器</em>”（weak learner）尤为明显（弱学习器是指泛化性能不佳，预测准确度仅略高于50%的学习器）。但需要注意的是，为能够获得更好的泛化性能，通常还是选择使用泛化性能较好的学习器。其次，显而易见的，对用于集成学习的学习器，要获得好的集成效果，个体学习器应“好而不同”，也即不仅需要准确性，更要具有一定的多样性（diversity）。</p>
<p>对简单的集成模型，常用投票法（voting）对个体学习器进行集成。对一个简单的二分类问题进行分析：$y \in {-1,+1}$和真实函数$f$，并假定基分类错误率为$\epsilon$，则对每个分类器有</p>
<script type="math/tex; mode=display">P(h_i(x) \neq f(x)) = \epsilon</script><p>通过简单投票法集成$T(odd)$个基分类器，有超过一半的分类基分类器正确，则集成分类就正确。</p>
<script type="math/tex; mode=display">H(x) = \text{sign}(\sum^T_{i=1}h_i(x))</script><p>假设基分类器的错误率相互独立，则由Hoeffding不等式可知，集成的错误率为</p>
<script type="math/tex; mode=display">\begin{aligned} P(H(x) \neq f(x)) &= \sum^{[T/2]}_{k=0} \binom{T}{k}(1-\epsilon)^k\epsilon^{T-k} \\ &\leqslant exp\left (-\frac{1}{2}T(1-2\epsilon)^2 \right ) \end{aligned}</script><p>从上式可以看出，在$T$增大时错误率呈指数级下降，最终趋向于零。但需要注意的是，此过程中存在一个关键假设，即各基学习器的错误率相互独立，在现实任务中显然难以成立。</p>
<p>根据个体的生成方式，可以分为两大类：</p>
<ul>
<li>个体学习器间存在强依赖关系，必须串行生成的序列化方法，例：Boosting；</li>
<li>个体学习器间不存在强依赖关系，可同时生成的并行化方法，例：Bagging和<em>随机森林</em>（Random Forest）。</li>
</ul>
<h2 id="8-2-Boosting"><a href="#8-2-Boosting" class="headerlink" title="8.2 Boosting"></a>8.2 Boosting</h2><p><strong>Boosting</strong>：一族将弱学习器提升为强学习器的算法。</p>
<p>Boosting的工作机制如下：</p>
<ol>
<li>从出事训练集中训练出一个基学习器；</li>
<li>根据基学习器的表现，调整训练样本集，使做错的训练样本得到更多的关注；</li>
<li>基于调整后的样本训练下一个基学习器；</li>
<li>重复2、3直至学习器数目达到目标$T$值。最后将这$T$个学习器加权结合。</li>
</ol>
<p>本节中介绍Boosting族中最著名的AdaBoost算法，并以最简单的基于“加性模型”（additive model），即基学习器的线性组合进行推导</p>
<p>$f$为真实函数，$y_i \in {-1,+1}$，$H(x)$是集成函数，有</p>
<script type="math/tex; mode=display">H(x) = \sum^{T}_{t=1}\alpha_t h_t(x)</script><p>其损失函数为<em>指数损失函数</em>（exponential loss function）</p>
<script type="math/tex; mode=display">\ell _{\exp}(H|D) = \mathbb{E} _{x \sim D} [e^{-f(x)H(x)}]</script><p>对$H(x)$求偏导，得</p>
<script type="math/tex; mode=display">\frac{\partial \ell_{\exp}(H|D)}{\partial H(x)} = -e^{-H(x)}P(f(x) = 1|x) + e^{H(x)}P(f(x)=-1|x)</script><p>令导数为零，解得</p>
<script type="math/tex; mode=display">H(x) = \frac{1}{2}\ln \frac{P(f(x) = 1|x)}{P(f(x)=-1|x)}</script><p>则有</p>
<script type="math/tex; mode=display">\text{sign}(H(x)) = \argmax_{y \in \{-1,+1\}} P(f(x)=y|x)</script><p>即有$H(x) \rightarrow H’(x) = 0 \rightarrow \argmax_y P(f(x)=y|x)$。因此，对应$H’(x)=0$即指数损失函数最小时，有分类错误率最小，此时称指数损失函数是原本$0/1$损失函数的一致的替代损失函数。</p>
<p>之后，需要找出迭代的优化方法，即权重更新公式和样本分布更新公式。</p>
<h3 id="权重更新公式"><a href="#权重更新公式" class="headerlink" title="权重更新公式"></a>权重更新公式</h3><p>在生成第一个基分类器之后，需要迭代地生成$\alpha_t$和$h_t(x)$。$\alpha_t$应使得$\alpha_t h_t(x)$最小化指数损失函数，即</p>
<script type="math/tex; mode=display">\begin{aligned} \ell_{\exp}(\alpha_th_t|D_t) &= E_{x \sim D_t}[e^{-f(x) \alpha_t h_t(x)}] \\ &= E_{x \sim D_t}[e^{-\alpha_t}\mathbb{I}(f(x)=h_t(x)) + e^{\alpha_t}\mathbb{I}(f(x) \neq h_t(x))] \\ &= e^{-\alpha_t}P_{x \sim D_t}(h_t(x) = f(x)) + e^{\alpha}P_{x \sim D_t}(h_t(x) \neq f(x)) \\ &= e^{-\alpha_t}(1-\epsilon_t) + e^{\alpha_t}\epsilon_t \end {aligned}</script><p>其中，$\epsilon<em>t=P</em>{x \sim D_t}(h_t(x) \neq f(x))$，即错误率。对上式求导可得</p>
<script type="math/tex; mode=display">\frac{\partial \ell_{\exp}(\alpha_th_t(x)|D_t)}{\partial \alpha_t} = -e^{-\alpha_t}(1-\epsilon_t) + e^{\alpha}\epsilon_t</script><p>令其导数为零，可解得</p>
<script type="math/tex; mode=display">\alpha_t = \frac{1}{2}\ln\left ( \frac{1-\epsilon_t}{\epsilon_t} \right )</script><p>即得到分类器权重的更新公式。</p>
<h3 id="样本分布更新公式"><a href="#样本分布更新公式" class="headerlink" title="样本分布更新公式"></a>样本分布更新公式</h3><p>在获得$H<em>{t-1}(x)$之后，对样本分布进行调整。目的在于使下一轮学习器$h_t$尽可能地纠正$H</em>{t-1}(x)$的错误，即最小化$H_{t-1}+h_t$的损失</p>
<script type="math/tex; mode=display">\begin{aligned} \ell_{\exp} (H_{t-1}+h_t|D) &= E_{x \sim D} [ e^{-f(x)(H_{t-1}(x) + h_t(x))} ] \\ & = E_{x \sim D} [ e^{-f(x)H_{t-1}(x)} \cdot e^{-f(x)h_t(x))} ] \end{aligned}</script><p>由于$f^2(x) = h_t^2(x) = 1$，因此利用$e^x$的二阶泰勒展开式，有</p>
<script type="math/tex; mode=display">\begin{aligned} \ell_{\exp} (H_{t-1}+h_t|D) &\simeq E_{x \sim D} \left [e^{-f(x)H_{t-1}(x)} \left (1-f(x)h_t(x)+\frac{1}{2}f^2(x)h_t^2(x) \right ) \right ] \\ &= E_{x \sim D} \left [e^{-f(x)H_{t-1}(x)} \left (1-f(x)h_t(x)+\frac{1}{2} \right ) \right ] \end{aligned}</script><p>所以理想的学习器$h_t$应满足</p>
<script type="math/tex; mode=display">\begin{aligned} h_t(x) &= \argmin_h \ell_{\exp}(H_{t-1} + h_t|D) \\ &= \argmin_h E_{x \sim D} \left [e^{-f(x)H_{t-1}(x)} \left (1-f(x)h_t(x)+\frac{1}{2} \right ) \right ] \\ &= \argmax_h E_{x \sim D}\left [ e^{-f(x)H_{t-1}(x)} f(x)h_t(x)\right ] \\ &= \argmax_h E_{x \sim D}\left [ \frac{e^{-f(x)H_{t-1}(x)}}{E_{x \sim D}[e^{-f(x)H_{t-1}(x)}]} f(x)h_t(x)\right ] \end{aligned}</script><p>其中$E<em>{x \sim D}[e^{-f(x)H</em>{t-1}(x)}]$为常数，设分布</p>
<script type="math/tex; mode=display">D_t(x) = \frac{D(x)e^{-f(x)H_{t-1}(x)}}{E_{x \sim D}[e^{-f(x)H_{t-1}(x)}]}</script><p>代入学习器$h_t$式中，可得</p>
<script type="math/tex; mode=display">h_t(x) = \argmax_h E_{x \sim D_t}[f(x)h(x)]</script><p>由$f(x) \cdot h_t(x) \in {-1,+1}$，有</p>
<script type="math/tex; mode=display">f(x)h(x) = 1-2 \mathbb{I}(f(x) \neq h(x))</script><p>则理想的学习器为</p>
<script type="math/tex; mode=display">h_t(x) = \argmax_h E_{x \sim D_t}[1-2 \mathbb{I}(f(x) \neq h(x))]</script><p>所以可得$D<em>t$与$D</em>{t+1}$的关系为</p>
<script type="math/tex; mode=display">\begin{aligned} D_{t+1}(x) &= \frac{D(x)e^{-f(x)H_{t}(x)}}{E_{x \sim D}[e^{-f(x)H_{t}(x)}]} \\ &= \frac{D(x)e^{-f(x)H_{t-1}(x)} e^{-f(x)\alpha_t h_t(x)}}{E_{x \sim D}[e^{-f(x)H_{t}(x)}]} \\ &= D_t(x)e^{-f(x)\alpha_t h_t(x)} \frac{E_{x \sim D}[e^{-f(x)H_{t-1}(x)}]}{E_{x \sim D}[e^{-f(x)H_{t}(x)}]} \end{aligned}</script><p>即得到样本分布的更新公式。</p>
<p>在需要基学习器能对特定的数据分布进行学习时，对于可处理带权样本和不能的情况分别处理：</p>
<ul>
<li>可处理带权样本，“<em>重赋权法</em>”（re-weighting）：在训练过程的每一轮中，根据样本分布为每个训练样本重新赋予权重。</li>
<li>不可赋权重型，“<em>重采样法</em>”（re-sampling）：在每一轮学习中，根据样本分布对训练集重新进行采样，再用重采样而得的样本集对基学习器进行训练。</li>
</ul>
<p>从偏差-方差的角度来看，Boosting主要关注于降低偏差。</p>
<h2 id="8-3-Bagging与随机森林"><a href="#8-3-Bagging与随机森林" class="headerlink" title="8.3 Bagging与随机森林"></a>8.3 Bagging与随机森林</h2><p>前文提到，集成学习的原则是好而不同，因此，应使不同的个体学习器之间的差异尽量地大，使用采样的方法选择不同的样本对学习器进行训练；同时，不应因为仅使用部分样本对学习器进行训练而出现欠拟合，所以应使用互有交叠的样本。</p>
<h3 id="8-3-1-Bagging"><a href="#8-3-1-Bagging" class="headerlink" title="8.3.1 Bagging"></a>8.3.1 Bagging</h3><p>Bagging是并行式集成学习方法中最著名的代表，其直接基于自助采样法（有放回的多次随机抽样）。</p>
<p>Bagging通常使用自助采样法在$m$个样本中抽取$T$个含$m$个样本的训练集，然后基于每个采样集训练出一个基学习器，再将它们结合。对预测输出进行结合时，Bagging通常对分类任务使用简单投票法，对回归任务使用简单平均法。</p>
<p>基学习器的性能假定为$O(m)$，则Bagging的复杂度大致为$T(O(m) + O(s))$，$O(s)$通常很小，$T$是不太大的常数。因此Bagging是一个很高效的集成学习算法，且可以直接应用于多分类、回归等任务。</p>
<p>此外，通过自助采样法得到与原样本集大小相同的训练集，其中仅包含原样本集中$63.2\%$的样本数量。于是，可以使用剩余的$36.8\%$的样本用作验证集对泛化性能进行“<em>包外估计</em>”（out-of-bag estimate）。令$H^{oob}(x)$表示对样本$x$的包外预测，有</p>
<script type="math/tex; mode=display">H^{oob}(x) = \argmax_{y \in \gamma} \sum^T_{t=1}\mathbb{I}(h_t(x) = y) \cdot \mathbb{I}(x \notin D_t)</script><p>则Bagging泛化误差的包外估计为</p>
<script type="math/tex; mode=display">\epsilon^{obb} = \frac{1}{|D|} \sum_{(x,y) \in D}\mathbb{I}(H^{obb} \neq y)</script><p>并且，如果基学习器是决策树，包外估计可以用于辅助剪枝；或用于估计决策树中各结点的后验概率以辅助对零训练样本结点的处理；还可帮助神经网络减小过拟合风险。</p>
<p>从偏差-方差的角度看，Bagging主要关注于降低方差。</p>
<h3 id="8-3-2-随机森林"><a href="#8-3-2-随机森林" class="headerlink" title="8.3.2 随机森林"></a>8.3.2 随机森林</h3><p><strong>随机森林</strong>（Random Forest，简称RF）：是基于决策树算法作为基学习器的一种Bagging变种，在决策树的训练过程中引入了随机属性选择。其引入随机的步骤如下：</p>
<ol>
<li>在含有$d$个属性的样本集中，随机选取一个有$k$种属性的样本子集；</li>
<li>从这个样本集中选取最优划分变量对子集进行划分。</li>
</ol>
<p>显然，其使用$k$控制引入随机性的程度，如：$k=1$时，即为随机选取一个样本进行划分；$k=d$时，即为一般的决策树算法；通常推荐使用$k=\log_2d$。</p>
<p>随机森林相对于Bagging而言，其不仅有来自样本的扰动，更存在来自属性的扰动，所以其“多样性”更好，泛化性能更强。并且，随机森林有更高的运行效率，因为其每次训练仅需考虑一个$k$属性子集，而Bagging需要考虑全部子集。</p>
<h2 id="8-4-结合策略"><a href="#8-4-结合策略" class="headerlink" title="8.4 结合策略"></a>8.4 结合策略</h2><p>学习器结合的优点：</p>
<ol>
<li>统计方面：分别使用多种假设空间，避免因误选导致泛化性能减弱；</li>
<li>计算方面：多次计算避免单次陷入局部最小；</li>
<li>表示方面：结合多个学习器的假设空间，扩大假设空间，避免真实假设不在假设空间的情况。</li>
</ol>
<p>假定包含$T$个基学习器${h_1,h_2,\dots,h_T}$的集成，其中$h_i$在示例$x$上的输出为$h_i(x)$，以此为例介绍几种常见结合策略。</p>
<h3 id="8-4-1-平均法"><a href="#8-4-1-平均法" class="headerlink" title="8.4.1 平均法"></a>8.4.1 平均法</h3><p>对数值型输出$h_i(x) \in \mathbb{R}$，最常见的结合策略是使用平均法（average）。</p>
<ul>
<li>简单平均法（simple average）：<script type="math/tex">H(x) = \frac{1}{T}\sum^T_{i=1}h_i(x)</script></li>
<li>加权平均法（weighted avergae）：<script type="math/tex">H(x) = \sum^T_{i=1} w_ih_i(x)</script>其中，$w<em>i$是个体学习器的权重，通常要求$w_i \geqslant 0$，$\sum^T</em>{i=1} w_i = 1$。</li>
</ul>
<p>集成学习中各种结合方法都可看作是加权平均法的特例或变体，其在集成学习中具有特别的意义。事实上，对给定的基学习器，不同的集成学习方法可以视为通过不同的方式来确定加权平均法中的权重。</p>
<p>需要注意的是，加权平均法中的权重一般是通过对训练数据的学习而得，但在实际应用中，训练样本中通常不充分或存在噪声，这就导致学习到的权重并不可靠，特别是在规模较大的集成来说，要学习的权重较多，容易导致过拟合。因此，在现实任务中，加权平均法的性能未必优于简单平均法。一般地，我们在个体学习器性能差距不大的情况下使用简单平均法，在差距较大的时候使用加权平均法。</p>
<h3 id="8-4-2-投票法"><a href="#8-4-2-投票法" class="headerlink" title="8.4.2 投票法"></a>8.4.2 投票法</h3><p>对分类任务来说，学习器$h_i$从类别标记集合${c_1,c_2,\dots,c_N}$中预测出一个标记，最常见的结合策略是<em>投票法</em>（voting）。假定$h^j_i(x)$是$h_i$在类别标记$c_j$上的输出，由此$h_i$在样本$x$上的预测输出即为一个$N$维向量$(h^1_i(x);h^2_i(x);…;h^N_i(x))$。</p>
<ul>
<li>绝对多数投票法（majority voting）：<script type="math/tex">H(x)=\begin{cases} c_j, &\text{  if } \sum^T_{i=1} h^j_i(x) > 0.5\sum^N_{k=1}\sum^T_{i=1}h^k_i(x), \\ reject, & \text{otherwise.} \end{cases}</script>即若所得票数超过半数，则为预测该标记，否则拒绝预测。绝对多数投票法的拒绝预测机制，为可靠性要求较高的学习任务提供了一个很好的机制；</li>
<li>相对多数投票法（plurality voting）：<script type="math/tex">H(x) = c_{\argmax_j \sum^T_{i=1} h^j_i(x)}</script>即将所得票数最多的标记作为预测，如果是多个相等，就从中随机选择一个；</li>
<li>加权投票法（weighted voting）：$$H(x) = c<em>{\argmax_j \sum^T</em>{i=1} h^j<em>i(x)}$$$w_i$是$h_i$的权重，通常$w_i \geqslant 0$，$\sum^T</em>{i=1} w_i = 1$。</li>
</ul>
<p>上述的方法中，个体学习器的输出类型不受限制，以此可以产生不同的类别，常见的有：</p>
<ul>
<li>类标记：$h^j_i \in {0,1}$，使用类标记的投票称“硬投票”；</li>
<li>类概率：$h^j_i \in [0,1]$，使用类概率的投票称为“软投票”。</li>
</ul>
<p>对于在确认类标记的同时给出置信度的学习器，可以使用规范化后的置信度作为类概率进行集成。一般而言，使用类概率的结合性能比类标记更好。但需要注意的是，如果基学习器的类型不同，其类概率不能直接进行比较，此时可以转化为标记输出，使用类标记进行结合。</p>
<h3 id="8-4-3-学习法"><a href="#8-4-3-学习法" class="headerlink" title="8.4.3 学习法"></a>8.4.3 学习法</h3><p>在训练数据很多的情况下，可以使用更为强大的结合策略——“学习法”，即通过另一个学习器进行结合。Stacking是学习法的典型代表，我们称基学习器为初级学习器，结合学习器为次级学习器或元学习器（meta-learner）。</p>
<p>Stacking在训练完初级学习器后，将初级学习器的输出值作为次级学习器的输入特征，初始样本的标记仍作为输入标记，训练次级学习器。在训练中为减小过拟合风险，一般对训练样本集使用$k$折交叉验证或留一法，用训练样本集的初级学习器未使用的样本来产生次级学习器的训练样本。主要流程如下：</p>
<figure class="highlight vb"><table><tr><td class="code"><pre><span class="line">输入：训练集D=&#123;(x1,y1),(x2,y2),...,(xm,ym)&#125;</span><br><span class="line">      初级学习算法£<span class="number">1</span>,£<span class="number">2</span>,...,£T</span><br><span class="line">      次级学习算法£</span><br><span class="line">过程：</span><br><span class="line"><span class="keyword">for</span> t = <span class="number">1</span>,<span class="number">2</span>,...,T <span class="keyword">do</span></span><br><span class="line">    ht = £t(D);</span><br><span class="line"><span class="keyword">end</span> <span class="keyword">for</span></span><br><span class="line">D` = ∅;</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span>,<span class="number">2</span>,...,m <span class="keyword">do</span></span><br><span class="line">    <span class="keyword">for</span> t = <span class="number">1</span>,<span class="number">2</span>,...,T <span class="keyword">do</span></span><br><span class="line">        zit = ht(xi);</span><br><span class="line">    <span class="keyword">end</span> <span class="keyword">for</span></span><br><span class="line">    D` = D` ∪ ((zi1,zi2,...,ziT),yi);</span><br><span class="line"><span class="keyword">end</span> <span class="keyword">for</span></span><br><span class="line">h` = £(D`);</span><br><span class="line">输出：H(x) = h`(h1(x),h2(x),...,ht(x))</span><br></pre></td></tr></table></figure>
<p>Stacking的泛化性能与次级学习器的输入属性表示和次级学习器算法有很大关系。在使用初级学习器的输出类概率为输入属性，用多相应线性回归（Multi-response Linear Regression，简称MLR）作为次级学习器算法时效果教好。MLR是基于线性回归的分类器，对每个类分别进行线性回归，属于该类的训练样例标志将置为1，不属于将置为0，测试样例将被划分为概率最大的类。</p>
<p>贝叶斯模型平均（Bayes Model Averaging，简称BMA）基于后验概率给不同模型赋予权重，可以视之为加权平均法的变种。</p>
<p>从理论上来说，数据生成模型如果在当前考虑的模型中，且数据噪声较小，则BMA性能至少不弱于Stacking。但在现实中，难以满足前者的条件。因此，Stacking通常优于BMA，因为其鲁棒性比BMA好，且BMA对模型近似误差非常敏感。</p>
<h2 id="8-5-多样性"><a href="#8-5-多样性" class="headerlink" title="8.5 多样性"></a>8.5 多样性</h2><h3 id="8-5-1-误差-分歧分解"><a href="#8-5-1-误差-分歧分解" class="headerlink" title="8.5.1 误差-分歧分解"></a>8.5.1 误差-分歧分解</h3><p>为得到具有更强泛化性能的学习器，需要对个体学习器的“好而不同”性质进行理论分析。</p>
<p>假定有回归学习任务$f: \mathbb{R^d} \rightarrow \mathbb{R}$，通过使用对个体学习器$h_1,h_2,\dots,h_T$加权平均的集成来完成。于是，有示例$x$，在学习器$h_1$上的“<em>分歧</em>”（ambiguity）</p>
<script type="math/tex; mode=display">A(h_i|x) = (h_i(x) - H(x))^2</script><p>则集成上的分歧（组内误差）为</p>
<script type="math/tex; mode=display">\bar A(h_i|x) = \sum^T_{i=1} w_i(h_i(x) - H(x))^2</script><p>显然，此“分歧”式体现出了$x$的多样性。此外，存在</p>
<p>个体学习器$h_i$的平方误差（组间误差）<script type="math/tex">E(h_i|x) = (f(x)-h_i(x))^2</script></p>
<p>集成$H$的平方误差<script type="math/tex">E(H|x) = (f(x)-H(x))^2</script></p>
<p>令$\bar E(h|x) = \sum^T_{i=1} w_i \cdot E(h_i|x)$表示个体学习器的加权均值，则有</p>
<script type="math/tex; mode=display">\begin{aligned} \overline A(h|x) &= \sum^T_{i=1} w_iE(h_i|x)-E(H|x) \\ &= \overline E(h|x) - E(H|x). \end{aligned}</script><p>令$p(x)$为样本的概率密度，将上式推广至全样本，可得</p>
<script type="math/tex; mode=display">\sum^T_{i=1} w_i \int A(h_i|x)p(x)dx = \sum^T_{i=1}w_i \int E(h_i|x)p(x)dx - \int E(H|x)p(x)dx</script><p>类似的，将$h_i$的泛化误差和分歧项分别推广到全样本体现为</p>
<script type="math/tex; mode=display">E_i = \int E(h_i|x)p(x)dx</script><script type="math/tex; mode=display">A_i = \int A(h_i|x)p(x)dx</script><p>集成的泛化误差为</p>
<script type="math/tex; mode=display">E = \int E(H|x)p(x)dx</script><p>令$\overline E = w_iE_i$表示个体学习器泛化误差的加权均值，$\overline A = w_i A_i$表示个体学习确定加权分歧值，并对应于未推广的前式，则有</p>
<script type="math/tex; mode=display">E = \overline E - \overline A</script><p>显然，可以看出个体学习器的准确度越高，多样性越大，则集成效果越好。这一过程被称为“<em>误差-分歧分解</em>”（error-ambiguity decomposition）。</p>
<p>但需要注意的是，这个结论无法作为学习器的优化目标进行求解，并且上述推导过程仅适用于回归学习（源于集合方差的分解），难以直接推广到分类学习中。</p>
<h3 id="8-5-2-多样性度量"><a href="#8-5-2-多样性度量" class="headerlink" title="8.5.2 多样性度量"></a>8.5.2 多样性度量</h3><p><strong>多样性度量</strong>（diversity measure）：用于度量集成中个体分类器的多样性，即估算个体学习器的多样化程度。典型的做法是考虑个体学习器的两两相似\不相似性。</p>
<p>对给定的数据集$D = {(x_1, y_1), (x_2, y_2), \dots, (x_m, y_m)}$，有二分类任务$y_i \in {-1, +1}$，分类器$h_i$与$h_j$的预测结果列联表（contingency table）为</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$~$</th>
<th style="text-align:center">$h_i = +1 \ \ \ \  h_i = -1$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$h_j = +1$</td>
<td style="text-align:center">$a \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ c$</td>
</tr>
<tr>
<td style="text-align:center">$h_j = -1$</td>
<td style="text-align:center">$b \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ d$</td>
</tr>
</tbody>
</table>
</div>
<p>其中，$a+b+c+d = m$，基于这个列联表，下面给出一些常用的多样性度量</p>
<ul>
<li>不合度量（disagreement measure）$$d<em>is</em>{ij} = \frac{b+c}{m}$$$d<em>is</em>{ij}$的值域为$[0,1]$，值越大则多样性越大；<br>$~$</li>
<li>相关系数（correlation coefficient）$$\rho<em>{ij} = \frac{ad-bc}{\sqrt{(a+b)(a+c)(c+d)(b+d)}}$$$\rho</em>{ij}$的值域为$[-1,1]$。若$h_i$与$h_j$正相关则值为正，否则为负；<br>$~$</li>
<li>Q-统计量（Q-statistic）$$Q<em>{ij} = \frac{ad-bc}{ad+bc}$$$Q</em>{ij}$与相关系数$\rho<em>{ij}$的符号相同，且$|Q</em>{ij}| \geqslant |\rho_{ij}|$；<br>$~$</li>
<li>$\kappa$-统计量（$\kappa$-statistic）<script type="math/tex">\kappa = \frac{p_1-p_2}{1-p_2}</script>其中，$p_1$是两个分类器取得一致的概率；$p_2$是两个分类器偶然达成一致的概率，由数据集$D$估算的计算方法为：<script type="math/tex">\begin{aligned} p_1 &= \frac{a+d}{m}, \\ p_2 &= \frac{(a+b)(a+c)+(c+d)(b+d)}{m^2}. \end{aligned}</script>若分类器$h_i$与$h_j$在$D$上完全一致，则$\kappa = 1$；若它们只是偶然达成一致，则$\kappa = 0$，在$h_i$与$h_j$达成一致的概率低于偶然性的情况下有$\kappa &lt; 0$。</li>
</ul>
<p>此外，上述的度量均为“成对型”（pairwise）多样性度量，可以通过2维图绘制出来，方便分析。</p>
<h3 id="8-5-3-多样性增强"><a href="#8-5-3-多样性增强" class="headerlink" title="8.5.3 多样性增强"></a>8.5.3 多样性增强</h3><p>为增强多样性一般思路是在学习过程中引入随机性，常见的做法主要是对数据样本、输入属性、输入表示、算法参数进行扰动。以下介绍增强多样性的方法。</p>
<ul>
<li>数据样本扰动<br>数据样本的扰动通常基于采样法，例如：Bagging使用自助采样，AdaBoost使用序列采样。这种方法简单高效、使用最广，尤其是在决策树、神经网络等，对训练样本变化比较敏感的“不稳定基学习器”中效果拔群；相对地，在那些对训练样本变化不敏感的学习器，如线性学习器、支持向量机、朴素贝叶斯、$k$临近学习器等，这种方法的效果有限，需要使用其他方法进行扰动；<br>$~$</li>
<li>输入属性扰动<br>训练样本通常由一组属性描述，不同的“<em>子空间</em>”（subspace，即属性子集）提供观察数据的不同视角。著名算法<em>随机子空间</em>（random subspace）算法即是依赖于属性扰动，该算法从原始数据空间中提取若干个属性子集，再基于每个属性子集训练一个基学习器。对包含大量冗余属性的数据，一方面增加基学习器的多样性，另一方面减少属性使时间开销大幅节约。但如果数据仅包含少量属性，或冗余属性很少，就不宜使用输入属性扰动；</li>
</ul>
<figure class="highlight vb"><table><tr><td class="code"><pre><span class="line">随机子空间算法</span><br><span class="line"></span><br><span class="line">输入：训练集D=&#123;(x1,y1),(x2,y2),...,(xm,ym)&#125;</span><br><span class="line">      基学习算法£</span><br><span class="line">      基学习器数T</span><br><span class="line">      子空间属性数d`</span><br><span class="line">过程：</span><br><span class="line"><span class="keyword">for</span> t = <span class="number">1</span>,<span class="number">2</span>,...,T <span class="keyword">do</span></span><br><span class="line">    Ft = RS(D,d`)</span><br><span class="line">    Dt = MapFt(D)</span><br><span class="line">    ht = £(Dt)</span><br><span class="line"><span class="keyword">end</span> <span class="keyword">for</span></span><br><span class="line">输出：$H(x) = \argmax_&#123;y \<span class="keyword">in</span> \gamma&#125; \sum^T_&#123;t=<span class="number">1</span>&#125; \mathbb&#123;I&#125;(h_t(Map_&#123;F_t&#125;)(x) = y)$</span><br></pre></td></tr></table></figure>
<p>$~$</p>
<ul>
<li>输出表示扰动<br>此类做法的基本思路是对输出表示进行操纵以增强多样性，可对训练样本的类标记稍作变动，如“<em>翻转法</em>”（Flipping Output）随机改变一些训练样本的标记；也可对输出表示进行转化，如“<em>输出调制法</em>”（Output Smearing）将分类输出转化为回归输出后构建个体学习器；还可将原任务拆解为多个可同时求解的子任务，如ECOC法利用纠错输出码将多分类任务拆解为一系列二分类任务来训练基学习器。<br>$~$</li>
<li>算法参数扰动<br>基学习算法一般都有参数需进行设置，例如神经网络的隐层神经元数、初始连接权重等，通过随机设置不同的参数，往往可产生差别较大的个体学习器。例如“<em>负相关法</em>”（Negative Correlation）显式地通过正则化项来强制个体神经网络使用不同的参数。对参数较少的算法，可同故宫将其学习过程中某些环节用其他类似方式代替，从而达到扰动的目的，例如可将决策树使用的属性选择机制替换成其他的属性选择机制。值得指出的是，使用单一学习器时通常需使用交叉验证等方法来确定参数值，这事实上已使用了不同参数训练出多个学习器，只不过最终仅选择一个学习器进行使用，而集成学习则相当于把这些学习都利用起来；由此可以看出，集成学习技术的实际计算开销并不一定比使用单一学习器大很多。</li>
</ul>
<p>不同的多样性增强机制可同时使用，例如<strong>8.3.2</strong>节中介绍的随机森林中同时使用了数据样本扰动和输入属性扰动，有些方法甚至同时使用了更多机制。</p>
]]></content>
      <tags>
        <tag>《机器学习》</tag>
      </tags>
  </entry>
  <entry>
    <title>Chapter9 聚类</title>
    <url>/2019/10/02/Chapter9%E8%81%9A%E7%B1%BB/</url>
    <content><![CDATA[<p>本文内容：</p>
<ul>
<li>聚类任务</li>
<li>性能度量<ul>
<li>外部指标</li>
<li>内部指标</li>
</ul>
</li>
<li>距离计算</li>
<li>原型聚类<ul>
<li>k均值算法（k-means）</li>
<li>学习向量量化</li>
<li>高斯混合聚类</li>
</ul>
</li>
<li>密度聚类</li>
<li>层次聚类</li>
</ul>
<span id="more"></span>
<h2 id="9-1-聚类任务"><a href="#9-1-聚类任务" class="headerlink" title="9.1 聚类任务"></a>9.1 聚类任务</h2><p>聚类任务是常见的无监督学习方法之一，也是无监督学习中研究最多、应用最广的方法。</p>
<p><strong>无监督学习</strong>（unsupervised learning）：训练样本的标记信息是未知的，目标是通过对无标记训练样本的学习来揭示数据的内在性质和规律，为进一步的数据分析提供基础。</p>
<p>聚类算法可将样本子集划分为几个不相交的子集，其中每个子集（<em>簇</em>cluster）可能对应于不同的潜在概念。聚类仅能自动划分出簇，其中每个簇的概念语义需要自行判断。聚类可以作为一个单独的过程，也可作为其他学习任务的前驱过程。</p>
<p>聚类算法涉及两大基本问题：性能度量和距离计算。</p>
<h2 id="9-2-性能度量"><a href="#9-2-性能度量" class="headerlink" title="9.2 性能度量"></a>9.2 性能度量</h2><p>聚类性能度量亦称聚类的“<em>有效性指标</em>”（validity index），一方面评估衡量学习器性能的好坏，另一方面如果明确了最终的目标的性能度量，则可直接作为聚类过程的优化目标。</p>
<p>聚类算法期望聚类结果表现“<em>簇内相似度</em>”（intra-cluster similarity）高，且“<em>簇间相似度</em>”（inter-cluster similarity）低。聚类性能度量大致可分为两类：</p>
<ul>
<li>外部指标（external index）：将聚类结果与“<em>参考模型</em>”（reference model）进行比较得到的指标；</li>
<li>内部指标（internal index）：直接考察聚类结果而不利用任何参考模型的指标。</li>
</ul>
<h3 id="外部指标"><a href="#外部指标" class="headerlink" title="外部指标"></a>外部指标</h3><p>对数据集$D = {x_1,x_2,…,x_m}$，通过聚类得到的簇划分为$C={C_1,C_2,…,C_k}$，参考模型的簇划分为$C^<em>={C^</em>_1,C^<em>_2,…,C^</em>_s}$，$\lambda$和$\lambda^<em>$分别表示在$C$和$C^</em>$中对应的簇标记向量，将样本两两配对考虑，定义以下指标：</p>
<script type="math/tex; mode=display">a = |SS|, SS = \{(x_i,x_j)|\lambda_i = \lambda_j, \lambda^*_i=\lambda^*_j, i < j\}</script><script type="math/tex; mode=display">b = |SD|, SD = \{(x_i,x_j)|\lambda_i = \lambda_j, \lambda^*_i \neq \lambda^*_j, i < j\}</script><script type="math/tex; mode=display">c = |DS|, DS = \{(x_i,x_j)|\lambda_i \neq \lambda_j, \lambda^*_i=\lambda^*_j, i < j\}</script><script type="math/tex; mode=display">d = |DD|, DD = \{(x_i,x_j)|\lambda_i \neq \lambda_j, \lambda^*_i \neq \lambda^*_j, i < j\}</script><p>显然，有$a+b+c+d=C^2_m=\frac{m(m-1)}{2}$。根据以上定义，可导出以下聚类性能度量外部指标：</p>
<ul>
<li>Jaccard系数（Jaccard Coeffient，简称JC）<script type="math/tex">JC=\frac{a}{a+b+c}</script></li>
<li>FM指数（Fowlkes and Mallows Index，简称FMI）<script type="math/tex">FMI=\sqrt{\frac{a}{a+b}\cdot\frac{a}{a+c}}</script></li>
<li>Rand指数（Rand Index，简称RI）<script type="math/tex">RI=\frac{2(a+d)}{m(m-1)}</script></li>
</ul>
<p>显然，上述性能度量的结果值均在$[0,1]$区间，值越大越好。</p>
<h3 id="内部指标"><a href="#内部指标" class="headerlink" title="内部指标"></a>内部指标</h3><p>对聚类结果的簇划分$C={C<em>1,C_2,…,C_k}$，$\mu_i$代表簇$C_i$的中心点，$\mu_i=\frac{1}{|C_i|}\sum</em>{1 \leqslant j \leqslant |C_i|}x_j$，定义</p>
<script type="math/tex; mode=display">\begin{aligned} \text{avg}(C) &= \frac{2}{|C|(|C|-1)}\sum_{1\leqslant i < j \leqslant |C|}\text{dist}(x_i,x_j) \\ \text{diam}(C) &= \max_{1 \leqslant i < j \leqslant |C|}\text{dist}(x_i,x_j) \\ d_{\min}(C_i,C_j) &= \min_{x_i \in C_i, x_j \in C_j} \text{dist}(x_i,x_j) \\ d_{\text{cen}}(C_i,C_j) &= \text{dist}(\mu_i, \mu_j)\end{aligned}</script><p>基于以上定义可导出下列聚类性能度量的内部指标：</p>
<ul>
<li>DB指数（Davies-Bouldin Index，简称DBI）<script type="math/tex">DBI=\frac{1}{k}\sum^k_{i=1}\max_{j \neq i}\left ( \frac{\text{avg}(C_i) + \text{avg}(C_j)}{d_{\text{cen}(C_i,C_j)}} \right )</script></li>
<li>Dunn指数（Dunn Index，简称DI）<script type="math/tex">DI=\min_{1\leqslant i \leqslant k}\left \{ \min_{j\neq i}\left ( \frac{d_{\min}(C_i,C_j)}{\max_{1\leqslant l \leqslant k}\text{diam}(C_l)} \right ) \right \}</script></li>
</ul>
<p>显然，DBI的值越小越好；DI相反，值越大越好。</p>
<h2 id="9-3-距离计算"><a href="#9-3-距离计算" class="headerlink" title="9.3 距离计算"></a>9.3 距离计算</h2><p>对距离函数$\text{dist}(\cdot, \cdot)$的要求：</p>
<ul>
<li>非负性：$\text{dist}(x_i, x_j) \geqslant 0$；</li>
<li>对称性：$\text{dist}(x_i, x_j) = \text{dist}(x_j, x_i)$；</li>
<li>同一性：$\text{dist}(x_i, x_j) = 0$，当且仅当$x_i=x_j$时成立；</li>
<li>直递性：$\text{dist}(x_i, x_j) \leqslant \text{dist}(x_i, x_k) + \text{dist}(x_k, x_j)$.</li>
</ul>
<p>于是，可根据上述要求构造相应的距离函数$\text{dist}(x_i, x_j)$。</p>
<p>对给定样本$x<em>i=(x</em>{i1}, x<em>{i2}, \dots,x</em>{in})$和$x<em>j=(x</em>{j1},x<em>{j2},\dots,x</em>{jn})$，当其为连续属性或有序离散属性时，最常用“<em>闵可夫斯基距离</em>”（Minkowski distance）</p>
<script type="math/tex; mode=display">\text{dist}_{\text{mk}}(x_i, x_j)=\left \| x_i-x_j\right \|_p = \left ( \sum^n_{u=1} |x_{iu} - x_{ju}|^p \right )^{\frac{1}{p}}</script><p>对$p \geqslant 1$，上式即满足距离度量的基本性质。<br>对$p=2$，闵可夫斯基距离退化为<em>欧式距离</em>（Euclidean distance）<script type="math/tex">\text{dist}_{\text{ed}}(x_i, x_j)= \left \| x_i - x_j \right \|_2 = \sqrt{\sum^n_{u=1}|x_{iu}-x_{ju}|^2}</script><br>对$p=1$，闵可夫斯基距离退化为<em>曼哈顿距离</em>（Manhattan distance）<script type="math/tex">\text{dist}_{\text{man}}(x_i, x_j)= \left \| x_i - x_j \right \| = \sum^n_{u=1}|x_{iu}-x_{ju}|</script><br>对$p \rightarrow \infty$，得到切比雪夫距离。</p>
<p>而对于无序离散属性，可采用VDM（Value Difference Metric）法。令$m_{u,a}$表示在属性$u$上取值为$a$的样本数，$k$为样本簇数，则VDM距离为</p>
<script type="math/tex; mode=display">VDM_{p}(a,b) = \sum^k_{i=1}\left |\frac{m_{u,a,i}}{m_{u,a}}-\frac{m_{u,b,i}}{m_{u,b}} \right |^p</script><p>再通过结合VDM和闵可夫斯基距离，即可得到解决混合问题的距离度量</p>
<script type="math/tex; mode=display">MinkovDM_p(x_i,x_j)=\left ( \sum_{u=1}^{n_c} |x_{iu}-x_{ju}|^p + \sum^n_{u=n_c+1} VDM_p(x_{iu},x_{ju})\right )^{\frac{1}{p}}</script><p>其中，$n$为属性数，$n_c$为有序属性数。在样本空间中不同属性的重要性不同时，可使用“<em>加权距离</em>”（weighted distance），以闵可夫斯基距离为例：</p>
<script type="math/tex; mode=display">\text{dist}_{\text{wmk}}(x_i, x_j) = \left ( \sum^n_{u=1} w_u \cdot |x_{iu} - x_{ju}|^p \right )^{\frac{1}{p}}</script><p>其中权重$w<em>u \geqslant 0(u=1,2,\dots,n)$表征不同属性的重要性，通常$\sum^n</em>{u=1}w_u = 1$。</p>
<p>需要注意的是，“<em>相似度度量</em>”（similarity measure）是基于某种形式的距离定义的。但用于相似度度量的距离未必要满足距离度量的所有基本性质，尤其是直递性，可根据是否满足直递性将距离度量分为度量距离和非度量距离（non-metric distance）。</p>
<p>在现实任务中，基于数据样本确定合适的距离计算式，这一过程被称为“<em>距离度量学习</em>”（distance metric learning）。</p>
<h2 id="9-4-原型聚类"><a href="#9-4-原型聚类" class="headerlink" title="9.4 原型聚类"></a>9.4 原型聚类</h2><p>通过对样本中一组具有代表性的点（原型）的刻画，表示出聚类的整体结构。通常是先对原型进行初始化，然后对原型进行迭代更新求解。</p>
<h3 id="9-4-1-k均值算法（k-means）"><a href="#9-4-1-k均值算法（k-means）" class="headerlink" title="9.4.1 k均值算法（k-means）"></a>9.4.1 k均值算法（k-means）</h3><p>对给定样本集$D={x_1,x_2,\dots,x_m}$，聚类所得簇划分$C={C_1,C_2,\dots,C_k}$，最小化平方误差</p>
<script type="math/tex; mode=display">E = \sum^k_{i=1}\sum_{x \in C_i}\left \| x-\mu_i \right \|^2_2</script><p>其中，$\mu<em>i=\frac{1}{|C_i|}\sum</em>{x\in C_i}x$，即簇$C_i$的均值向量。</p>
<p>对上式求解是个NP难的问题，需考察样本集上所有簇划分。因此，k-means算法采用贪心策略通过迭代优化来求近似最优解。</p>
<p><img src="https://i.loli.net/2019/10/01/6YZWih32L4yuPgS.png" alt="k-means流程"></p>
<p>其中，计算新均值变量的方法为$\mu<em>i’ = \frac{1}{|C_i|}\sum</em>{x \in C_i} x$。</p>
<h3 id="9-4-2-学习向量量化"><a href="#9-4-2-学习向量量化" class="headerlink" title="9.4.2 学习向量量化"></a>9.4.2 学习向量量化</h3><p><strong>学习向量量化</strong>（Learning Vector Quantization，简称LVQ）：是有监督的聚类方法，利用监督信息来辅助聚类，也同样是寻找一组原型向量来刻画聚类结构，可以看作竞争学习SMO算法的有监督拓展。</p>
<p>给定样本集$D={(x<em>1,y_1), (x_2,y_2), \dots, (x_m,y_m)}$对应每个$x_j$是由$n$个属性描述的特征向量$(x</em>{j1};x<em>{j2};\dots;x</em>{jn})$，$y_j \in \gamma$是样本$x_j$的类别标记。</p>
<p>LVQ的目标是学得一组确定簇标记的$n$维原型向量${p_1,p_2,\dots,p_q}$，其对应的簇标记是预先设定的，为$t_i \in \gamma(i=1,2,\dots,q)$，即从$|\gamma|$种簇中聚类出$q$个新的簇。</p>
<p>LVQ简要流程：</p>
<ol>
<li>$n$维原型向量进行初始化，根据预设的簇标记向量${t_1,t_2,\dots,t_q}$从样本中随机选取确定原型向量${p_1,p_2,\dots,p_q}$；</li>
<li>计算距离矩阵$d<em>{m\times q}=\left {d_ji|d_ji=\left | x_j-p_i \right |</em> 2 \right }$。找出行中最小的元素，确定$p <em> {i^<em> }$，$i^ </em> = \argmin </em> {i \in (1,2,\dots,q)}d <em> {ji}$。判断$x </em> j$的簇标记是否与$p<em>{i^*}$的簇标记$t</em>{i^ *}$相同<ul>
<li>相同，则使$x<em>j \Rightarrow p</em>{i^<em>}$距离缩短$$p’ = p_{i^ </em>}+\eta(x<em>j-p</em>{i^ *})$$</li>
<li>不相同，则使$x<em>j \Rightarrow p</em>{i^<em>}$距离增加$$p’ = p_{i^ </em>}-\eta(x<em>j-p</em>{i^ <em>})$$<br>将$p_{i^</em>}$更新为$p’$；</li>
</ul>
</li>
<li>持续迭代(2)直至满足条件（达到最大迭代轮数或原型向量更新程度小于阈值）。</li>
</ol>
<p>获得最终的原型向量${p_1, p_2, \dots, p_q}$后，可利用其对样本空间进行划分，将任意样本$x$，划入与其距离最近的原型向量所代表的簇中，即对样本空间$\chi$的簇划分${R_1,R_2, \dots, R_q}$有</p>
<script type="math/tex; mode=display">R_i=\{x \in \chi| \left \| x-p_i \right \| \leqslant \left \| x-p_{i'} \right \|, i \neq i'\}</script><p>这种划分通常称为“<em>Voronoi剖分</em>”。</p>
<h3 id="9-4-3-高斯混合聚类"><a href="#9-4-3-高斯混合聚类" class="headerlink" title="9.4.3 高斯混合聚类"></a>9.4.3 高斯混合聚类</h3><p>与其他方法不同的是，高斯混合聚类采用概率模型来表达聚类原型。$n$维多元高斯分布的概率密度函数为</p>
<script type="math/tex; mode=display">p(x) = \frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}</script><p>为明确表达出高斯分布与相应参数的依赖关系，将其概率密度函数记为$p(x|\mu,\Sigma)$。若使每个混合成分（聚类簇）对应一个高斯分布，定义一个由$k$个混合成分构成的高斯混合分布</p>
<script type="math/tex; mode=display">p_\mathcal{M}(x)=\sum^k_{i=1}\alpha_i \cdot p(x|\mu_i,\Sigma_i)</script><p>其中，$\mu<em>i$与$\Sigma_i$是第i个混合成分的参数，$\alpha_i&gt;0$是对应的“混合系数”（mixture-coeffcient），有$\sum^k</em>{i=1} \alpha_i = 1$。</p>
<p>假设样本是由该高斯混合分布给出，以$\alpha_1,\alpha_2, \dots,\alpha_k$为选择第$i$个高斯混合成分的先验概率。根据选定的混合成分的概率密度函数进行采样，从而得到当前样本。</p>
<p>训练集$D={x_1,x_2,\dots,x_n}$由上述过程生成，随机变量$z_j \in {1,2,\dots,k}$为生成样本$x_j$的高斯混合成分。显然，$p(z_j=i)=\alpha_i$，计算$z_j$的后验分布</p>
<script type="math/tex; mode=display">\begin{aligned} p_{\mathcal{M}}(z_j=i|x_j)&=\frac{p(z_j=i)\cdot p_{\mathcal{M}}(x_j|z_j=i)}{p_{\mathcal{M}}(x_j)} \\ &= \frac{\alpha_i\cdot p(x_j|\mu_i,\Sigma_i)}{\sum^k_{u=1}\alpha_u p(x_j|\mu_u,\Sigma_u)}\end{aligned}</script><p>$p<em>{\mathcal{M}(z_j=i|x_j)}$即为$x_j$由第$i$个高斯混合分布生成的后验概率，记作$\gamma</em>{ji}$。</p>
<p>显然，可以根据每个混合成分，定义一个分类簇，即在高斯混合分布已知时，有$k$个簇分类$C={C_1,C_2,\dots,C_k}$，记每个样本$x_j$分类簇标记为$\lambda_j$，则有</p>
<script type="math/tex; mode=display">\lambda_j= \argmax_{i \in \{1,2,\dots,k\}} \gamma_{ji}</script><p>综上，可看出高斯混合聚类是以其概率模型对原型进行刻画的，而其簇划分则以后验概率决定。</p>
<p>之后，问题就在于对模型参数${(\alpha_i,\mu_i,\Sigma_i)| 1 \leqslant i \leqslant k}$的求解上，对给定样本集$D$，可以采用极大似然估计，即最大化对数似然</p>
<script type="math/tex; mode=display">\begin{aligned} LL(D) &= \ln(\prod^m_{j=1}p_{\mathcal{M}}(x_j))\\ &= \sum^m_{j=1}\ln \left ( \sum^k_{i=1} \alpha_i\cdot p(x_j|\mu_i,\Sigma_i) \right ) \end{aligned}</script><p>常用EM算法对上式进行迭代化求解。</p>
<p>若参数${(\alpha_i,\mu_i,\Sigma_i)| 1 \leqslant i \leqslant k}$能使上式最大化，则对参数$\mu_i$和$\Sigma_i$有</p>
<script type="math/tex; mode=display">\begin{cases} \frac{\partial LL(D)}{\partial \mu_i} = 0, \\ \frac{\partial LL(D)}{\partial \Sigma_i} = 0, \\ \gamma_{ji} = p_{\mathcal{M}}(z_j=i|x_j). \end{cases}</script><p>即</p>
<script type="math/tex; mode=display">\begin{cases} \mu_i = \frac{\sum^m_{j=1}\gamma_{ji}x_j}{\sum^m_{j=1}\gamma_{ji}}, \\ \Sigma_i = \frac{\sum^m_{j=1}\gamma_{ji}(x_j-\mu_i)(x_j-\mu_i)^T}{\sum^m_{j=1}\gamma_{ji}}. \end{cases}</script><p>显然，各混合成分的均值可通过样本加权平均来估计，样本权重是每个样本属于该成分的后验概率。</p>
<p>对于混合系数$\alpha<em>i$，由于其存在约束条件$\alpha_i \geqslant 0$和$\sum^k</em>{i=1}\alpha_i = 1$，所以在计算最大化$LL(D)$时，需要考虑$LL(D)$的拉格朗日形式</p>
<script type="math/tex; mode=display">LL(D) + \lambda \left ( \sum^k_{i=1} \alpha_i -1 \right )</script><p>其中$\lambda$是拉格朗日乘子，由于上式为达到极值点的情况，所以有对$\alpha_i$的导数为0，则有</p>
<script type="math/tex; mode=display">\sum^m_{j=1} \frac{p(x_j|\mu_i,\Sigma_i)}{\sum^k_{l=1}\alpha_l\cdot p(x_j|\mu_j,\Sigma_l)} + \lambda = 0</script><p>对等式两边同乘$\alpha_i$，并对所有混合成分进行求和，即可得</p>
<script type="math/tex; mode=display">\begin{aligned} -\sum^k_{i=1} \alpha_i \lambda &= \sum^k_{i=1}\sum^m_{j=1}\frac{\alpha_i \cdot p(x_j|\mu_i,\Sigma_i)}{\sum^k_{l=1}\alpha_j \cdot p(x_j|\mu_l,\Sigma_l)} \\ -\lambda &= \sum^k_{i=1}\sum^m_{j=1}\gamma_{ji} \\ -\lambda &= m \end{aligned}</script><p>由上述关系对原式继续变形，可得</p>
<script type="math/tex; mode=display">\sum^m_{j=1} \frac{p(x_j|\mu_i,\Sigma_i)}{\sum^k_{l=1}\alpha_l\cdot p(x_j|\mu_j,\Sigma_l)} + \lambda = \sum^m_{j=1} \frac{\gamma_i}{\alpha_i} = m</script><p>即</p>
<script type="math/tex; mode=display">\alpha_i = \frac{1}{m} \sum^m_{j=1}\gamma_{ji}</script><p>即每个高斯成分的混合系数由样本属于该成分的平均后验概率确定。</p>
<p>通过上述推导，获得了更新参数${(\alpha<em>i,\mu_i,\Sigma_i)| 1 \leqslant i \leqslant k}$的公式，因此高斯混合模型的EM算法为：在每步迭代中，先根据当前参数来计算每个样本属于每个高斯成分的后验概率$\gamma</em>{ji}$（E步）；再根据参数的更新公式，更新参数模型（M步）。</p>
<h2 id="9-5-密度聚类"><a href="#9-5-密度聚类" class="headerlink" title="9.5 密度聚类"></a>9.5 密度聚类</h2><p><strong>密度聚类</strong>：亦称“<em>基于密度的聚类</em>”（density-based clustering），密度聚类假设聚类结构能通过样本分布的紧密程度确定。一般地，密度聚类算法从样本密度角度考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇以获得最终的聚类结果。</p>
<p><strong>DBSCAN</strong>（Density-Based Spatial Clustering of Applications with Noise）：一种著名的密度聚类算法，基于一组“<em>邻域</em>”（neighborhood）参数$(\epsilon,MinPts)$来刻画样本分布的紧密程度。对给定数据集$D={x_1,x_2,\dots,x_m}$，定义以下几个概念：</p>
<ul>
<li>$\epsilon$-邻域：对$x<em>j \in D$，其$\epsilon$-邻域包含样本集$D$中与$x_j$的距离不大于$\epsilon$的样本，即$N</em>{\epsilon}(x_j) = {x_i \in D|\text{dist}(x_i,x_j) \leqslant \epsilon}$；</li>
<li>核心对象（core object）：若$x<em>j$的邻域至少包含$MinPts$个样本，即$|N</em>{\epsilon}(x_j)| \geqslant MinPts$，则$x_j$是一个核心对象；</li>
<li>密度直达（directly density-reachable）：若$x_j$位于$x_i$的$\epsilon$-邻域中，且$x_i$是核心对象，则称$x_j$由$x_i$密度直达（通常不满足对称性）；</li>
<li>密度可达（density-reachable）：对$x<em>i$与$x_j$，若存在样本序列$p_1,p2,\dots,p_n$，其中$p_1=x_i,p_n=x_j$且$p</em>{i+1}$由$p_i$密度直达，则称$x_j$由$x_i$密度可达（满足直递性，但不满足对称性）；</li>
<li>密度相连（density-connected）：对$x_i$与$x_j$，若存在$x_k$使得$x_i$与$x_j$均由$x_k$密度可达，则称$x_i$与$x_j$密度相连（满足对称性）。</li>
</ul>
<p>上述概念的直观显示为</p>
<p><img src="https://i.loli.net/2019/10/02/JUtQi6ae3u1Wkh4.png" alt="DBSCAN示意图"></p>
<p>图为$MinPts=3$时的情况，其中，虚线圈出的是$\epsilon$-邻域，$x_1$是核心对象，$x_2$由$x_1$密度直达，$x_3$由$x_1$密度可达，$x_3$与$x_4$密度相连。</p>
<p>基于上述概念，DBSCAN的簇定义为：由密度可达关系导出的最大的密度相连样本集合。形式化地说，给定邻域参数$(\epsilon,MinPts)$，簇$C \subseteq D$是满足以下性质的非空样本子集：</p>
<ul>
<li>连接性（connectivity）：$x_i \in C$，$x_j \in C \Rightarrow x_j$与$x_i$密度相连；</li>
<li>最大性（maximality）：$x_i \in C$，$x_j$由$x_i$密度可达$\Rightarrow x_j \in C$。</li>
</ul>
<p>因此，满足连接性与最大性的簇，即是以$x$为核心对象，由$x$密度可达的所有样本组成的集合$X = {x’ \in D|x’$由$x$密度可达$}$。</p>
<p>于是，DBSCAN的主要流程为：</p>
<p><img src="https://i.loli.net/2019/10/02/xJcUs1anvBRoqNY.png" alt="DBSCAN流程图"></p>
<h2 id="9-6-层次聚类"><a href="#9-6-层次聚类" class="headerlink" title="9.6 层次聚类"></a>9.6 层次聚类</h2><p><strong>层次聚类</strong>（hierarchical clustering）：是一类在不同层次上对数据集进行划分，最终形成树形结构的聚类算法。数据集的划分有“自底向上”的聚合策略和“自顶向下”的分拆策略。</p>
<p><strong>AGNES</strong>（AGglomerative NESting）：是一种采用自底向上聚合策略的层次聚类算法。它先将所有样本均看作一个分类簇，在算法运行的每一步中找出距离最近的两个聚类簇进行合并，重复该过程直至达到目标聚类簇的个数。</p>
<p>在上述过程中的关键在于，计算聚类簇之间的距离。由于每个簇都是一个样本集合，因此，只需要采用关于集合的某种距离即可。对给定的聚类簇$C_i$和$C_j$，可通过下式计算距离（集合间的距离计算常用<em>豪斯多夫距离</em>Hausdorff distance）：</p>
<ul>
<li>最小距离：<script type="math/tex">d_{\min}(C_i,C_j) = \min_{x \in C_i, z \in C_j}\text{dist}(x,z)</script></li>
<li>最大距离：<script type="math/tex">d_{\max}(C_i,C_j) = \max_{x \in C_i, z \in C_j}\text{dist}(x,z)</script></li>
<li>平均距离：<script type="math/tex">d_{\text{avg}}(C_i,C_j) = \frac{1}{|C_i||C_j|} \sum_{x \in C_i} \sum_{z \in C_j}\text{dist}(x,z)</script></li>
</ul>
<p>显然，最小距离由两个簇的最近样本决定，最大距离由两个簇的最近样本决定，平均距离由两个簇的全部样本决定。对应于计算聚类簇间的距离$d<em>{\min}$、$d</em>{\max}$、$d_{\text{avg}}$，AGNES算法相应的称为“<em>单链接</em>”（single-linkage）、“<em>全链接</em>”（complete-linkage）或“<em>均链接</em>”（average-linkage）算法。</p>
<p>令AGNES算法一直执行到将所有样本划分到同一个簇中时，可以得到一个“<em>树状图</em>”（dendrogram）。将分割层逐步提升，即可得到聚类簇逐渐减少的聚类结果。</p>
]]></content>
      <tags>
        <tag>《机器学习》</tag>
      </tags>
  </entry>
  <entry>
    <title>FibonacciSequence</title>
    <url>/2017/09/17/FibonacciSequence/</url>
    <content><![CDATA[<h2 id="FibonacciSequence简介"><a href="#FibonacciSequence简介" class="headerlink" title="FibonacciSequence简介"></a>FibonacciSequence简介</h2><span id="more"></span>
<p>FibonacciSequence即斐波那契数列，指的是由 A1 = 1,A2 = 1,且 An+1 = An + An-1,组成的数列。<br>例：1，1，2，3，5，8···</p>
<h2 id="相关算法"><a href="#相关算法" class="headerlink" title="相关算法"></a>相关算法</h2><p><em>以下代码仅写出关键模块</em></p>
<h3 id="1-for循环实现-初级版"><a href="#1-for循环实现-初级版" class="headerlink" title="1.for循环实现 - 初级版"></a>1.for循环实现 - 初级版</h3><hr>
<pre><code>int fi1 = 1;
int fi2 = 2;
if (input &lt;= 2) &#123;
    switch (input) &#123;
        case 1:
            System.out.print(&quot;1&quot;);
            case 2:
            System.out.print(&quot;1 ,&quot;);
            System.out.print(&quot;1&quot;);
        &#125;
    &#125;
else &#123;
    System.out.print(&quot;1 ,&quot; );
    System.out.print(&quot;1 ,&quot;);
    for (int i = 0;i &lt; input - 2 ;i++ ) &#123;
        if (i % 2 == 0) &#123;
            fi1 = fi1 + fi2;
            System.out.print(fi1 + &quot; ,&quot;);
        &#125;
        else &#123;
            fi2 = fi1 + fi2;
            System.out.print(fi2 + &quot; ,&quot;);
        &#125;
    &#125;
// 用两个变量交替相加实现
</code></pre><h3 id="2-for循环-进阶版"><a href="#2-for循环-进阶版" class="headerlink" title="2.for循环 - 进阶版"></a>2.for循环 - 进阶版</h3><hr>
<pre><code>int f = 0;
int g = 1;
for (int i = 0;i &lt;= input ;i++ ) &#123;
    System.out.println(f);
    f = f + g;
    g = f - g;
&#125;
// 原理同上，写法更加简洁
</code></pre><h3 id="3-递归实现"><a href="#3-递归实现" class="headerlink" title="3.递归实现"></a>3.递归实现</h3><hr>
<pre><code>public static int F(int i)&#123;
    if (i == 1) &#123;
        return 1;
    &#125;
    else if (i == 2) &#123;
        return 1;
    &#125;
    return F(i - 2) + F(i - 1);
&#125;
// 简便代码，更易读
</code></pre>]]></content>
      <tags>
        <tag>Algorithms</tag>
        <tag>《Algorithms》</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>Java中continue和break</title>
    <url>/2017/08/26/Java%E4%B8%ADcontinue%E5%92%8Cbreak/</url>
    <content><![CDATA[<ul>
<li>本文中概念，代码均出自《Thinking in Java》</li>
</ul>
<span id="more"></span>
<blockquote>
<p>break，continue用法可总结为:<br>  1) 一般的continue会退回最内层循环的开头（顶部），并继续执行；<br>  2) 带标签的continue会到达标签的位置，并重新进入紧接在那个标签后面的循环；<br>  3) 一般的break会中断并跳出当前循环；<br>  4) 带标签的break会中断标签所指的循环。</p>
<p>  <strong>重点：在Java里需要使用标签的唯一理由就是因为有循环嵌套的存在，而且想从多层嵌套中break或continue。</strong></p>
</blockquote>
<p>代码示范如下：<br><strong>· for循环中应用：</strong></p>
<pre><code>class Test&#123;
    public static void print(String s)&#123;
        System.out.println(s);
    &#125;
    public static void main(String args[])&#123;
        int i = 0;
        outer:
        for (;true ; ) &#123;
            inner:
            for (;i &lt; 10 ; i++) &#123;
                print(&quot;i = &quot; + i);
                if (i == 2) &#123;
                    print(&quot;continue&quot;);
                    continue;
                &#125;
                if (i == 3) &#123;
                    print(&quot;break&quot;);
                    i++;
                    break;
                &#125;
                if(i == 7)&#123;
                    print(&quot;continue     outer&quot;);
                    i++;
                    continue outer;
                &#125;
                if (i == 8) &#123;
                    print(&quot;break outer&quot;);
                    break outer;
                &#125;
                for (int k = 0; k &lt; 5; k++) &#123;
                    if (k == 3) &#123;
                        print(&quot;continue inner&quot;);
                        continue inner;
                    &#125;
                &#125;
            &#125;
        &#125;
    &#125;
&#125;
/* Output:
i = 0
continue inner
i = 1
continue inner
i = 2
continue
i = 3
break
i = 4
continue inner
i = 5
continue inner
i = 6
continue inner
i = 7
continue outer
i = 8
break outer
\*/
</code></pre><p><strong>· while循环中应用</strong></p>
<pre><code>class Test&#123;
    public static void main(String args[])&#123;
        int i = 0;
        outer:
        while (true) &#123;
            System.out.println(&quot;Outer while loop&quot;);
            while (true) &#123;
                i++;
                System.out.println(&quot;i = &quot; + i);
                if (i == 1) &#123;
                    System.out.println(&quot;continue&quot;);
                    continue;
                &#125;
                if (i == 3) &#123;
                    System.out.println(&quot;continue outer&quot;);
                    continue outer;
                &#125;
                if (i == 5) &#123;
                    System.out.println(&quot;break&quot;);
                    break;
                &#125;
                if (i == 7) &#123;
                    System.out.println(&quot;break outer&quot;);
                    break outer;
                &#125;
            &#125;
        &#125;
    &#125;
&#125;
/* Output:
Outer while loop
i = 1
continue
i = 2
i = 3
continue outer
Outer while loop
i = 4
i = 5
break
Outer while loop
i = 6
i = 7
break outer
\*/
</code></pre>]]></content>
      <tags>
        <tag>java</tag>
        <tag>《Thinking in Java》</tag>
      </tags>
  </entry>
  <entry>
    <title>Java中foreach(初识)</title>
    <url>/2017/08/26/Java%E4%B8%ADforeach(%E5%88%9D%E8%AF%86)/</url>
    <content><![CDATA[<ul>
<li>本文中概念，代码均出自《Thinking in Java》*</li>
</ul>
<span id="more"></span>
<p>foreach语法</p>
<hr>
<p>  JavaSE5引入foreach语法，用于数组和容器。</p>
<p>  例：</p>
<pre><code>class Test&#123;
    public static void main(String args[])&#123;
        for (char c : &quot;An African Swallow&quot;.toCharArray()) &#123;
            System.out.print(c + &quot; &quot;);
        &#125;
    &#125;
&#125;/* Output:
A n A f r i c a n S w a l l o w
*/
</code></pre><p><strong>由上可知：</strong><br>    <strong>foreach语法可使用于任意数组及返回数组的方法</strong></p>
]]></content>
      <tags>
        <tag>java</tag>
        <tag>《Thinking in Java》</tag>
      </tags>
  </entry>
  <entry>
    <title>Java引用传递</title>
    <url>/2017/07/30/Java%E5%BC%95%E7%94%A8%E4%BC%A0%E9%80%92/</url>
    <content><![CDATA[<h2 id="概念引入"><a href="#概念引入" class="headerlink" title="概念引入"></a>概念引入</h2><span id="more"></span>
<p>通过以下程序理解：</p>
<pre><code>class Person&#123;
    String name;
    int age;
&#125;
class Test&#123;
    public static void main(String args[])&#123;
        Person per1 = new Person();
        per1.name = &quot;Mary&quot;;
        per1.age = 18;
        Person per2 = per1;
        per2.age = 20;
        System.out.println(per1.age);
    &#125;
&#125;
程序输出：20
</code></pre><p>此时结果与预想出现差异，此现象即为 <strong>引用传递</strong> 。</p>
<h2 id="内存分析"><a href="#内存分析" class="headerlink" title="内存分析"></a>内存分析</h2><p><img src="http://oro9jb0cl.bkt.clouddn.com/Step1.jpg" alt></p>
<pre><code>Person per1 = new Person();
//将per1存入栈内存（栈内存只存指向堆内存的地址），此时指向null
</code></pre><p><img src="http://oro9jb0cl.bkt.clouddn.com/Step2.jpg" alt></p>
<pre><code>per1.name = &quot;Mary&quot;;
per1.age = 18;

//栈内存中per1指向堆内存中 name = &quot;Mary&quot;,age = 18
</code></pre><p><img src="http://oro9jb0cl.bkt.clouddn.com/Step3.jpg" alt></p>
<pre><code>Person per2 = per1;
//在栈内存中创建per2，并使其指向地址与per1相同
</code></pre><p><img src="http://oro9jb0cl.bkt.clouddn.com/Step4.jpg" alt></p>
<pre><code>per2.age = 20;
System.out.println(per1.age);
//因为per1、per2所存地址相同，因此，程序输出20。
</code></pre><h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><p>在Java中，数组同样为引用数据类型<br>    数据类型 数组名称 [] = new 数据类型[长度]；<br>仅用 <strong>数组名称</strong> 即可代表该数组（为引用数据类型）</p>
<p><strong>因此，引用传递对数组同样适用。</strong></p>
]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>Java递归</title>
    <url>/2017/07/29/Java%E9%80%92%E5%BD%92/</url>
    <content><![CDATA[<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><span id="more"></span>
<p><strong>递归：</strong>即一个方法，自己调用自己的编程技巧。</p>
<h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p>① 计算 1 + 2 + 3 + ··· + 100 = ？</p>
<pre><code>class Test&#123;
    public static void main(String args[])&#123;
        System.out.println(sum(100));
    &#125;
    public static int sum(int num)&#123;
        if(num == 1)&#123;
            return 1;
        &#125;
        return num + sum(--num);
    &#125;
&#125;
</code></pre><p>② 计算 60！= ？</p>
<pre><code>class Test&#123;
    public static void main(String args[])&#123;
        System.out.println(factorial(60));
    &#125;
    public static double factorial(double num)&#123;
        if(num == 1)&#123;
            return 1;
        &#125;
        return num * factorial(--num);
    &#125;
&#125;
</code></pre>]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>Java类的private封装</title>
    <url>/2017/07/30/Java%E7%B1%BB%E7%9A%84private%E5%B0%81%E8%A3%85/</url>
    <content><![CDATA[<h2 id="概念引入"><a href="#概念引入" class="headerlink" title="概念引入"></a>概念引入</h2><span id="more"></span>
<p>观察如下程序：</p>
<pre><code>class Person&#123;
    int age;
&#125;
class Test&#123;
    public static void main(String args[])&#123;
        Person per = new Person();
        per.age = 200;
        System.out.println(&quot;age: &quot; + per.age);
    &#125;
&#125;
</code></pre><p>此时进行编译，运行，程序输出正常。但是，age = 200，明显不符合程序要求，这种错误即为 <strong>业务逻辑</strong> 错误。</p>
<p>因而，为解决业务逻辑问题，引入关键字 <strong>private</strong> 对类中属性进行 <strong>封装处理</strong> 。（一般不对类中方法进行封装，但支持此操作）</p>
<h2 id="语法格式"><a href="#语法格式" class="headerlink" title="语法格式"></a>语法格式</h2><p>在使用private对类中属性封装后，类中属性将无法被外部程序操作。<strong>而需要通过实现该属性的setter和getter方法来对属性进行处理。</strong><br>以此方法将上程序改写后，如下：</p>
<pre><code>class Person&#123;
    private int age;
    public void setAge(int a)&#123;
        if(0 &lt; a &lt; 150)&#123;
            age = a;
        &#125;
        else&#123;
            age = 0;
        &#125;
    &#125;
    public int getAge()&#123;
        return age;
    &#125;
&#125;
class Test&#123;
    public static void main(String args[])&#123;
        Person per = new Person();
        per.setAge = (15);
        System.out.prinln(&quot;age: &quot; + per.getAge());
    &#125;
&#125;
</code></pre><p>以上，可看出对属性实现setter、getter方法可有效解决业务逻辑问题。</p>
]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>Java错题</title>
    <url>/2017/07/29/Java%E9%94%99%E9%A2%98/</url>
    <content><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><span id="more"></span>
<blockquote>
<p>运行下面程序，结果为：</p>
<pre><code>public class Demo &#123;
    public static void main(String args[]) &#123;
        int num = 50 ;
        num = num ++ * 2 ;
        System.out.println(num) ;
    &#125;
&#125;
</code></pre></blockquote>
<h2 id="易错点"><a href="#易错点" class="headerlink" title="易错点"></a>易错点</h2><p>误将 num * 2 = 100 后再进行 num = num + 1<br>输出 101</p>
<h2 id="改正"><a href="#改正" class="headerlink" title="改正"></a>改正</h2><pre><code>public class Demo &#123;
    public static void main(String args[]) &#123;
        int num = 50 ;
        num = num ++ * 2 ;
        /**
        上式分解为
        1. num * 2 = 100
        2. num = num + 1 → num = 51
        3. num = 100**/
        System.out.println(num) ;
    &#125;
&#125;
</code></pre>]]></content>
      <tags>
        <tag>Java</tag>
        <tag>错题</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL学习</title>
    <url>/2017/09/29/MySQL%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h1 id="MySQL安装"><a href="#MySQL安装" class="headerlink" title="MySQL安装"></a>MySQL安装</h1><span id="more"></span>
<ol>
<li>直接在官网下载并安装。</li>
<li>在cmd中进入安装路径（例：C:\Program Files\MySQL\MySQL Server 5.7\bin）。</li>
<li>执行</li>
</ol>
<pre><code>mysql -u root -p
password:****************
</code></pre><p>完成连接即可。</p>
<h1 id="Database操作"><a href="#Database操作" class="headerlink" title="Database操作"></a>Database操作</h1><p>1.Database创建</p>
<pre><code>create database 数据库名;
</code></pre><p>2.Database删除</p>
<pre><code>drop database 数据库名;
</code></pre><p>3.Database查询</p>
<pre><code>show databases;
</code></pre><p>4.Database使用</p>
<pre><code>use 数据库名;
</code></pre><h1 id="Table操作"><a href="#Table操作" class="headerlink" title="Table操作"></a>Table操作</h1><p><strong>在进行Table操作之前，需先使用数据库</strong></p>
<h2 id="表单格式操作"><a href="#表单格式操作" class="headerlink" title="表单格式操作"></a>表单格式操作</h2><p>1.Table创建</p>
<pre><code>create table 表名;
</code></pre><p>2.Table删除</p>
<pre><code>drop table 表名;
</code></pre><p>3.Table查询</p>
<pre><code>show tables;
</code></pre><p>4.Table改名</p>
<pre><code>alter table [table_name] rename [new_table_name];
</code></pre><p>5.增加列</p>
<pre><code>alter table [table_name] add [colume_name] [data_type] (not null) (defult);
</code></pre><p>6.删除列</p>
<pre><code>alter table [table_name] drop [colume_name];
</code></pre><p>7.改变列</p>
<pre><code>alter table [table_name] change [old_colume_name] [new_colume_name] [data_type];
</code></pre><h2 id="表单内容操作"><a href="#表单内容操作" class="headerlink" title="表单内容操作"></a>表单内容操作</h2><p>1.insert</p>
<p>Ⅰ. 对应列数插入数据</p>
<pre><code>insert into [table_name] values (value1，value2···);
</code></pre><p>Ⅱ. 选择列数插入数据</p>
<pre><code>insert into [table_name] (列1，列2···) values (value1，value2···);
</code></pre><p><strong>进阶：与select组合</strong></p>
<p>Ⅰ. 对应列数插入数据</p>
<pre><code>insert into [table_name1] select 列1，列2 from [table_name2];
</code></pre><p>Ⅱ. 选择列数插入数据</p>
<pre><code>insert into [table_name1] (列1，列2···) select (列3，列4···) from [table_name2];
</code></pre><p>2.select</p>
<p>Ⅰ. 基础</p>
<p>①. 全部</p>
<pre><code>select * from table_name;
</code></pre><p>②. 列名</p>
<pre><code>select colume_name1,colume_name2 from table_name;
</code></pre><p>Ⅱ. where</p>
<pre><code>select * from table_name where colume_name 运算符 value;
//支持使用 or、and 增加条件
</code></pre><p><strong>进阶</strong></p>
<p>①. in</p>
<pre><code>select * from table_name where colume_name in (value1,value2···);

select * from table_name1 where colume_name in (select colume_name from table_name2);
</code></pre><p>②. between</p>
<pre><code>select * from table_name where colume_name (not) between value1 and value2;
</code></pre><p>③. like</p>
<pre><code>select * form table_name where colume_name (not) like pattern;
</code></pre><p><em>pattern:匹配。例：’abc’,’abc%’</em></p>
<p>Ⅲ. order by</p>
<p>①. 单列排序</p>
<pre><code>select * from table_name (where子句) order by colume_name (asc/desc);
</code></pre><p>②. 多列排序</p>
<pre><code>select * from table_name (where子句) order by col1(asc/desc),col2(acs/desc);
</code></pre><p><em>默认为asc</em></p>
<p>Ⅳ. limit</p>
<pre><code>select * from table_name (where子句) (order by子句) limit (offset,)rowCount;
</code></pre><p><em>从0计数。</em><br><em>offset不填时默认从0开始。</em></p>
<p>Ⅴ. update</p>
<p>①. 修改单列</p>
<pre><code>update table_name set colume_name = xxx (where子句);
</code></pre><p>②. 修改多列</p>
<pre><code>update table_name set colume_name1 = xxx,colume_name2 = xxx···(where子句)
</code></pre>]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux学习(一)——系统安装</title>
    <url>/2017/07/02/Linux1/</url>
    <content><![CDATA[<h1 id="Linux系统选择与下载"><a href="#Linux系统选择与下载" class="headerlink" title="Linux系统选择与下载"></a>Linux系统选择与下载</h1><span id="more"></span>
<p>初学Linux，选择安装Debian系统(名字好评)。(<a href="https://www.debian.org/">Debian官网</a>)</p>
<p>点选<strong>取得Debian</strong>中的<strong>USB/CD ISO 映像</strong></p>
<p><img src="http://oro9jb0cl.bkt.clouddn.com/Debian.jpg" alt></p>
<p>选择<br><strong>通过BitTorrent下载光盘映像文件</strong>(主要原因是使用Jigdo下载时出错<strong>:-(</strong>) →→→<br><strong>CD</strong>→→→<br><strong>amd64</strong>(适用于AMD64位和Intel64位)</p>
<p>下载全部ISO镜像文件。(大概11G)</p>
<p>下载完成后，创建新虚拟机，挂载映像文件，照提示完成安装即可。</p>
<p><img src="http://oro9jb0cl.bkt.clouddn.com/Debiantable.JPG" alt></p>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>二分查找</title>
    <url>/2018/03/04/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/</url>
    <content><![CDATA[<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><span id="more"></span>
<p>在数组中找到对应数据的下标的一种方法(优于遍历)。<br>要求：数组有序。</p>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>利用数组中间值与查找数据比较，确定查找数据位置。<br>因而其时间复杂度为O(log2n)</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><pre><code>public int binarySearch(int value, int[] array)&#123;
    int lo = 0;
    int up = array.length - 1;
    while(lo &lt;= up) &#123; // 循环比较至查找结束
        int key = (lo + up) / 2; // 取中间值比较
        if(value &gt; array[key]) &#123;
            lo = key; // 改变下界 缩小范围
        &#125;
        else if(value &lt; array[key]) &#123;
            up = key; // 改变上界 缩小范围
        &#125;
        else &#123;
            return key; // 查找到key
        &#125;
    &#125;
    return -1; // 未查找到
&#125;
</code></pre>]]></content>
      <tags>
        <tag>《Algorithms》</tag>
        <tag>java</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>函数试题</title>
    <url>/2017/09/24/%E5%87%BD%E6%95%B0%E8%AF%95%E9%A2%98/</url>
    <content><![CDATA[<p><em>试题来源知乎</em></p>
<span id="more"></span>
<h2 id="试题"><a href="#试题" class="headerlink" title="试题"></a>试题</h2><hr>
<blockquote>
<p>编程/数学挑战题<br>函数f(n)满足以下条件：<br>1）定义域和值域都是正整数；<br>2）f(n) &lt; f(n + 1);<br>3）f(f(n)) = 3n;<br>求 f(2017).</p>
</blockquote>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><hr>
<p>∵ 当 f(1) = 1 时，明显不成立；<br>  当 f(1) = 2 时，成立；<br>  当 f(1) &gt;= 3 时，均不成立；</p>
<p>∴ f(1) = 2;<br>  f(2) = 3;<br>  ······</p>
<p>  分析可知：<br>  两个被3整除的函数值间，函数值均匀分布。</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><hr>
<pre><code>class Test&#123;
    public static void main(String args[])&#123;
        System.out.println(f(2017));
    &#125;
    public static int f(int n)&#123;
        if (n == 1) &#123;
            return 2;
        &#125;
        else if (n == 2) &#123;
            return 3;
        &#125;
        else if (n % 3 == 0) &#123;
            return 3 * f(n / 3);
        &#125;
        else &#123;
            int nm = n;
            int na = n;
            // 取得小于n且能被3整除的数
            while (nm % 3 != 0) &#123;
                nm--;
            &#125;
            // 取得大于n且能被3整除的数
            while (na % 3 != 0) &#123;
                na++;
            &#125;
            return f(nm) + (f(na) - f(nm)) / (na - nm) * (n - nm);
        &#125;
    &#125;
&#125;
//3864
//：~
</code></pre><h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><hr>
<p><strong>1. 采用递归函数形式求解，省去繁琐循环；</strong><br><strong>2. 代码简介易读。</strong></p>
]]></content>
      <tags>
        <tag>Algorithms</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>多项式回归</title>
    <url>/2019/04/22/%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<p>本文内容：</p>
<ul>
<li>多项式回归问题<ul>
<li><em>一次多项式回归</em></li>
<li>多次多项式回归</li>
</ul>
</li>
<li>代码实现</li>
<li><strong>求解矩阵（方程组）方法归纳</strong><ul>
<li>Cramer法则</li>
<li>高斯消去法</li>
<li>LU分解法</li>
</ul>
</li>
</ul>
<span id="more"></span>
<h2 id="多项式回归问题"><a href="#多项式回归问题" class="headerlink" title="多项式回归问题"></a>多项式回归问题</h2><p>问题描述：对大多数数据集的驱动关系来说，都是非线性的。对于这类问题，我们可以采用一些数学模型进行替代求解。</p>
<h3 id="一次多项式回归"><a href="#一次多项式回归" class="headerlink" title="一次多项式回归"></a><strong>一次多项式回归</strong></h3><p>一次多项式即为线性函数：$y = b<em>{0} + b</em>{1}x$，对其回归求解可以使用==最小二乘法导出系数的公式==：</p>
<script type="math/tex; mode=display">b_{1}=\frac{n\sum x_{i}y_{i} - \sum x_{i}y_{i}}{n \sum x_{i}^2 -(\sum x_{i})^2}</script><script type="math/tex; mode=display">b_{0} = \bar{y} - b_{1}\bar{x}</script><p>这些公式根据正规方程组可以推知：</p>
<script type="math/tex; mode=display">(\sum_{i=1}^{n}x_{i}^2)b_{1} + (\sum x_{i})b_{0} = \sum x_{i}y_{i}</script><script type="math/tex; mode=display">(\sum x_{i})b_{1} + nb_{0} = \sum y_{i}</script><p>方程组通过平方和最小化得到：</p>
<script type="math/tex; mode=display">\sum_{n}^{i=1}(y_{i} - \tilde{y_{i}}) = \sum_{n}^{i=1}(y_{i} - b_{1}x_{i} - b_{0})^2</script><h3 id="多次多项式回归"><a href="#多次多项式回归" class="headerlink" title="多次多项式回归"></a>多次多项式回归</h3><p>以最简单的非线性一元多项式 $y = f(x) = b<em>{0} + b</em>{1}x + b<em>{2}x^2 + \cdot \cdot \cdot + b</em>{m}x^m$ 为例，其中m为多项式的次数，$b<em>{0},b</em>{1},b<em>{2},\cdot\cdot\cdot,b</em>{m}$为待定的系数。</p>
<p>可以运用相同的最小二乘法方法，对给定的数据集找到任意次数为m的最佳拟合的多项式，假定小于数据集中独立数据点的个数。</p>
<p>例：要找到拟合最佳的次数为 m=2 的多项式 $f(x) = b<em>{0} + b</em>{1} + b_{2}x^2$ （也叫作给定数据集的最小二乘抛物线），需要对求和式最小化。</p>
<script type="math/tex; mode=display">\sum_{n}^{i=1}(y_{i} - b_{2}x_{i}^2 - b_{1}x_{i} - b_{0})^2</script><p>这个方程用来决定系数$b<em>{0}、b</em>{1}和b_{2}$。从微积分角度来讲，这个表达式具有如下性质：</p>
<script type="math/tex; mode=display">z = \sum (A_{i} + B_{i}u + C_{i}v + D_{i}w)^2</script><p>通过求它的偏导数为0，对$z$最小化：</p>
<script type="math/tex; mode=display">\frac{\partial z}{\partial u} = \frac{\partial z}{\partial v} = \frac{\partial z}{\partial w}</script><p>由此产生二次回归的正规方程：</p>
<script type="math/tex; mode=display">nb_{0} + (\sum x_{i})b_{1} + (\sum x_{i}^2)b_{2} = \sum y_{i}</script><script type="math/tex; mode=display">(\sum x_{i})b_{0} + (\sum x_{i}^2)b_{1} + (\sum x_{i}^3)b_{2} = \sum x_{i}y_{i}</script><script type="math/tex; mode=display">(\sum x_{i}^2)b_{0} + (\sum x_{i}^3)b_{1} + (\sum x_{i}^4)b_{2} = \sum x_{i}^2y_{i}</script><p>即可将$b<em>{0},b</em>{1},b_{2}$同时解出。</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>本例中使用了Apache Commons math类协助计算。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.commons.math3.linear.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Example4</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span> n = <span class="number">6</span>; <span class="comment">// 数据集点的个数</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">double</span>[] x = &#123;<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>,<span class="number">50</span>,<span class="number">60</span>,<span class="number">70</span>&#125;; <span class="comment">// 数据集 x</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">double</span>[] y = &#123;<span class="number">52</span>,<span class="number">87</span>,<span class="number">136</span>,<span class="number">203</span>,<span class="number">290</span>,<span class="number">394</span>&#125;; <span class="comment">// 数据集 y</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">double</span>[][] a; <span class="comment">// 构造3个正规方程的参数</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">double</span>[] b; <span class="comment">// 解集</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 正规方程 ab = w</span></span><br><span class="line">    <span class="comment">// 解为数组 b</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">double</span>[][] a = <span class="keyword">new</span> <span class="keyword">double</span>[<span class="number">3</span>][<span class="number">3</span>];</span><br><span class="line">        <span class="keyword">double</span>[] w = <span class="keyword">new</span> <span class="keyword">double</span>[<span class="number">3</span>];</span><br><span class="line">        deriveNormalEquations(a, w);</span><br><span class="line">        printNormalEquations(a, w);</span><br><span class="line">        solveNormalEquations(a, w);</span><br><span class="line">        printResults();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    ** 构造正规方程</span></span><br><span class="line"><span class="comment">    **     &#123;n           , sigma x   , sigma x^2&#125;</span></span><br><span class="line"><span class="comment">    ** a = &#123;sigma x     , sigma x^2 , sigma x^3&#125;</span></span><br><span class="line"><span class="comment">    **     &#123;sigma x^2   , sigma x^3 , sigma x^4&#125;</span></span><br><span class="line"><span class="comment">    **</span></span><br><span class="line"><span class="comment">    **     &#123;sigma y     &#125;</span></span><br><span class="line"><span class="comment">    ** w = &#123;sigma xy    &#125;</span></span><br><span class="line"><span class="comment">    **     &#123;sigma x^2y  &#125;</span></span><br><span class="line"><span class="comment">    **/</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">deriveNormalEquations</span><span class="params">(<span class="keyword">double</span>[][] a, <span class="keyword">double</span>[] w)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = y.length;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">            <span class="keyword">double</span> xi = x[i];</span><br><span class="line">            <span class="keyword">double</span> yi = y[i];</span><br><span class="line">            a[<span class="number">0</span>][<span class="number">0</span>] = n;</span><br><span class="line">            a[<span class="number">0</span>][<span class="number">1</span>] = a[<span class="number">1</span>][<span class="number">0</span>] += xi;</span><br><span class="line">            a[<span class="number">0</span>][<span class="number">2</span>] = a[<span class="number">1</span>][<span class="number">1</span>] = a[<span class="number">2</span>][<span class="number">0</span>] += xi * xi;</span><br><span class="line">            a[<span class="number">1</span>][<span class="number">2</span>] = a[<span class="number">2</span>][<span class="number">1</span>] += xi * xi * xi;</span><br><span class="line">            a[<span class="number">2</span>][<span class="number">2</span>] = xi * xi * xi * xi;</span><br><span class="line">            w[<span class="number">0</span>] += yi;</span><br><span class="line">            w[<span class="number">1</span>] += xi * yi;</span><br><span class="line">            w[<span class="number">2</span>] += xi * xi * yi;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    ** 打印正规方程</span></span><br><span class="line"><span class="comment">    **/</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">printNormalEquations</span><span class="params">(<span class="keyword">double</span>[][] a, <span class="keyword">double</span>[] w)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; a.length; i++) &#123;</span><br><span class="line">            System.out.printf(<span class="string">&quot;%8.0fb0 + %6.0b1 + %8.0fb2 = %7.0f%n&quot;</span>, a[i][<span class="number">0</span>], a[i][<span class="number">1</span>], a[i][<span class="number">2</span>], w[i]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    ** 解正规方程</span></span><br><span class="line"><span class="comment">    **/</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">solveNormalEquations</span><span class="params">(<span class="keyword">double</span>[][] a, <span class="keyword">double</span>[] ww)</span> </span>&#123;</span><br><span class="line">        RealMatrix m = <span class="keyword">new</span> Array2DRowRealMatrix(a, <span class="keyword">false</span>); <span class="comment">// 通过已有数组构造正规方程</span></span><br><span class="line">        LUDecomposition lud = <span class="keyword">new</span> LUDecomposition(m); <span class="comment">// 创建 LU分解 类</span></span><br><span class="line">        DecompositionSolver solver = lud.getSolver(); <span class="comment">// 获取解类</span></span><br><span class="line">        RealVector v = <span class="keyword">new</span> ArrayRealVector(ww, <span class="keyword">false</span>); <span class="comment">// 创建v向量</span></span><br><span class="line">        b =  solver.solve(v).toArray(); <span class="comment">// 获取解</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">printResults</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.printf(<span class="string">&quot;f(t) = %.2f + %.3ft + %.5ft^2%n&quot;</span>, b[<span class="number">0</span>], b[<span class="number">1</span>], b[<span class="number">2</span>]);</span><br><span class="line">        System.out.printf(<span class="string">&quot;f(55) = %.1f%n&quot;</span>, f(<span class="number">55</span>,b));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    ** 构造 f(x) 方程</span></span><br><span class="line"><span class="comment">    **/</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">double</span> <span class="title">f</span><span class="params">(<span class="keyword">double</span> t, <span class="keyword">double</span>[] b)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> b[<span class="number">0</span>] + b[<span class="number">1</span>] * t + b[<span class="number">2</span>] * t * t;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="求解矩阵（方程组）方法反思"><a href="#求解矩阵（方程组）方法反思" class="headerlink" title="求解矩阵（方程组）方法反思"></a><strong>求解矩阵（方程组）方法反思</strong></h2><p>对矩阵方程求解一般有以下方法：</p>
<ul>
<li>Cramer法则</li>
<li>高斯消去法</li>
<li>LU分解法</li>
</ul>
<h3 id="Cramer法则"><a href="#Cramer法则" class="headerlink" title="Cramer法则"></a>Cramer法则</h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><h5 id="对n个n元线性方程组，当其常数项-b-1-b-2-cdots-b-n-不全为0时，线性方程组被称为非齐次方程组"><a href="#对n个n元线性方程组，当其常数项-b-1-b-2-cdots-b-n-不全为0时，线性方程组被称为非齐次方程组" class="headerlink" title="对n个n元线性方程组，当其常数项$b{1},b{2},\cdots,b_{n}$不全为0时，线性方程组被称为非齐次方程组"></a>对n个n元线性方程组，当其常数项$b<em>{1},b</em>{2},\cdots,b_{n}$不全为0时，线性方程组被称为非齐次方程组</h5><script type="math/tex; mode=display">\left\{\begin{matrix}a_{11}x_{1} + a_{12}x_{2} + \cdots + a_{1n}x_{n} = b_{1}
\\ a_{11}x_{1} + a_{12}x_{2} + \cdots + a_{1n}x_{n} = b_{2}
\\ \cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdot
\\a_{n1}x_{1} + a_{n2}x_{2} + \cdots + a_{nn}x_{n} = b_{n}
\end{matrix}\right.</script><p>令 $A = [a<em>{ij}]</em>{n \times n}, X = (x<em>{1},x</em>{2},\cdots,x<em>{n})^T, \beta = (b</em>{1}, b<em>{2}, \cdots, b</em>{n})^T$ ，其中$A$为线性方程的系数矩阵，$\beta$是由常数组成的列向量。线性方程组的矩阵形式为 $AX=\beta$。</p>
<h5 id="对n个n元线性方程组，当其常数项-b-1-b-2-cdots-b-n-全为0时，线性方程组被称为齐次方程组"><a href="#对n个n元线性方程组，当其常数项-b-1-b-2-cdots-b-n-全为0时，线性方程组被称为齐次方程组" class="headerlink" title="对n个n元线性方程组，当其常数项$b{1},b{2},\cdots,b_{n}$全为0时，线性方程组被称为齐次方程组"></a>对n个n元线性方程组，当其常数项$b<em>{1},b</em>{2},\cdots,b_{n}$全为0时，线性方程组被称为齐次方程组</h5><script type="math/tex; mode=display">\left\{\begin{matrix}a_{11}x_{1} + a_{12}x_{2} + \cdots + a_{1n}x_{n} = 0
\\ a_{11}x_{1} + a_{12}x_{2} + \cdot\cdot\cdot + a_{1n}x_{n} = 0
\\ \cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdot
\\a_{n1}x_{1} + a_{n2}x_{2} + \cdots + a_{nn}x_{n} = 0
\end{matrix}\right.</script><p>此时，线性方程组的矩阵形式为$AX=O$</p>
<p>系数构成的行列式称为该方程组的系数行列式D，即</p>
<script type="math/tex; mode=display">D = \begin{vmatrix}
a_{11} & a_{12} & \cdots & a_{1n}\\
a_{21} & a_{22} & \cdots & a_{2n}\\
\vdots & \vdots & \ddots  & \vdots\\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{vmatrix}</script><h4 id="定理"><a href="#定理" class="headerlink" title="定理"></a>定理</h4><h5 id="解法1"><a href="#解法1" class="headerlink" title="解法1"></a>解法1</h5><p>对于系数矩阵$A$可逆的线性方程组，即系数行列式$D \neq 0$。有唯一解，解为</p>
<script type="math/tex; mode=display">X_{0} = A^{-1} \beta</script><h5 id="解法2"><a href="#解法2" class="headerlink" title="解法2"></a>解法2</h5><p>对于系数矩阵$A$可逆的线性方程组，即系数行列式$D \neq 0$。有唯一解，解为</p>
<script type="math/tex; mode=display">X_{0} = \frac {D_{j}}{D} (j = 1,2,\cdots,n)</script><p>其中$D_{j}$是将D中第$j$列对应元素换为常数项$\beta$，而其他保持不变得到的行列式。</p>
<h4 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h4><p>使用Cramer法则时，对于多于两个或三个方程的系统，会十分低效，其渐进的复杂度为$O(n \times n!)$。因此，通常不考虑使用Cramer法则进行求解。</p>
<h3 id="高斯消去法"><a href="#高斯消去法" class="headerlink" title="高斯消去法"></a>高斯消去法</h3><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><h5 id="核心"><a href="#核心" class="headerlink" title="核心"></a>核心</h5><ol>
<li>两方程互换位置，解不变。</li>
<li>一方程乘以非零常数k，解不变。</li>
<li>一方程加上另一方程，解不变。</li>
</ol>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><ol>
<li>对一线性方程组的矩阵形式$AX = \beta$，可构造如下形式：</li>
</ol>
<script type="math/tex; mode=display">\begin{Bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} & | & b_{1}\\
a_{21} & a_{22} & \cdots & a_{2n} & | & b_{2}\\
\vdots & \vdots & \ddots & \vdots & | & \vdots\\
a_{n1} & a_{n2} & \cdots & a_{nn} & | & b_{n}
\end{Bmatrix}</script><ol>
<li>通过初等行变换（即根据核心原理），将左侧$n \times n$矩阵化为单位矩阵$I$，右侧即为所求解$X_{0}$。</li>
</ol>
<script type="math/tex; mode=display">\begin{Bmatrix}
1 & 0 & \cdots & 0 & | & x_{1}\\
0 & 1 & \cdots & 0 & | & x_{2}\\
\vdots & \vdots & \ddots & \vdots & | & \vdots\\
0 & 0 & \cdots & 1 & | & x_{n}
\end{Bmatrix}</script><h4 id="后记-1"><a href="#后记-1" class="headerlink" title="后记"></a>后记</h4><ol>
<li>高斯消元法的时间复杂度为$O(n^3)$。</li>
<li>高斯消去法可用于任何域。</li>
<li>高斯消去法应用时对于普通矩阵来说是稳定的。</li>
</ol>
<h3 id="LU分解法"><a href="#LU分解法" class="headerlink" title="LU分解法"></a>LU分解法</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>LU分解是指，将一个 $n \times n$ 的方阵$A$（所有顺序主子式不为0），转化为以下形式：<br>$A = LU$（所有顺序主子式为0时，矩阵不一定不可以进行LU分解）<br>其中，L、U分别为上三角矩阵和下三角矩阵。</p>
<p><em>顺序主子式</em><br>设$A$为$n \times n$的矩阵，子式</p>
<script type="math/tex; mode=display">D_{i} = \begin{vmatrix}
a_{11} & a_{12} & \cdots & a_{1i}\\
a_{21} & a_{22} & \cdots & a_{2i}\\
\vdots & \vdots & \ddots  & \vdots\\
a_{i1} & a_{i2} & \cdots & a_{ii}
\end{vmatrix}
(i = 1,2,\cdots,n)</script><p>被称为$A$的$i$阶顺序主子式。</p>
<h4 id="应用-1"><a href="#应用-1" class="headerlink" title="应用"></a>应用</h4><ol>
<li>将满足条件的$A$分解为上三角矩阵$L$和下三角矩阵$U$</li>
</ol>
<script type="math/tex; mode=display">A = LDU</script><p>其中$D$为对角矩阵，或分解为</p>
<script type="math/tex; mode=display">A = PLUQ</script><p>其中P为置换矩阵，Q为排列矩阵。</p>
<ol>
<li>将原方程等价代换，求解。</li>
</ol>
<p>$AX = \beta$ —&gt; $LUX = \beta$</p>
<p>$\therefore Y = L^{-1}\beta$ —&gt; $X = U^{-1}Y$</p>
<h4 id="后记-2"><a href="#后记-2" class="headerlink" title="后记"></a>后记</h4><ol>
<li>LU分解法在==本质上是高斯消去法==的一种表达形式。</li>
<li>LU分解法的==时间复杂度一般在$O(\frac{2n^3}{3})$左右==。</li>
<li>利用LU分解法求逆时，时间复杂度在$O(n^2)$左右，较高斯消去法快。</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li>求解非线性多项式回归问题，思路与线性方程回归类似，将多次问题看作多元。</li>
<li>利用Apache Commons math库中的RealMatrix、LUDecomposition、RealVector等类可对多项式回归问题快速求解。</li>
<li>求解矩阵时在LU分解法适用的情况下，一般快于高斯消去法，但高斯消去法适用任何域。</li>
</ol>
]]></content>
      <tags>
        <tag>Java</tag>
        <tag>《Java Data Analysis》</tag>
        <tag>回归</tag>
        <tag>矩阵求解</tag>
      </tags>
  </entry>
  <entry>
    <title>模拟登录自动健康填报脚本</title>
    <url>/2021/10/26/%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95%E8%87%AA%E5%8A%A8%E5%81%A5%E5%BA%B7%E5%A1%AB%E6%8A%A5%E8%84%9A%E6%9C%AC/</url>
    <content><![CDATA[<p>使用Selenium包结合<del>PhantomJS虚拟浏览器</del>无头浏览器模拟登录校园VPN进行健康填报。（仅供学习使用，勿用于破坏网络环境）</p>
<span id="more"></span>
<p>参考博文：<a href="https://www.cnblogs.com/chenxiaohan/p/7654667.html">Python模拟登录的几种方法</a></p>
<h2 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h2><p><strong>简单观察问题</strong> 通过观察健康填报登录POST响应及其填报时POST方法参数，可以发现构造极为复杂，且登录健康填报页面，通常需要校园网VPN协助，因此直接使用<code>requests</code>包POST健康信息实现有一定难度（笔者认为不能实现）。</p>
<p>由于需要登录校园VPN，所以对模拟登录问题进行检索，发现有模拟用户点击从而实现模拟登录的方法。该方法需要用到<del>PhantomJS虚拟浏览器</del>无头浏览器，优点是可以实现复杂的操作，缺点在于运行缓慢（无头浏览器加载性质决定无法优化）且占用资源比较多。</p>
<h2 id="代码部分"><a href="#代码部分" class="headerlink" title="代码部分"></a>代码部分</h2><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>安装<a href="https://www.selenium.dev/">Selenium</a>包，通过<code>pip</code>安装</p>
<pre><code>pip install selenium
</code></pre><p>或通过<code>conda</code>安装</p>
<pre><code>conda install selenium
</code></pre><p><del>安装<a href="https://phantomjs.org/">PhantomJS</a>，下载安装即可。</del></p>
<p>安装对应浏览器的webdriver驱动，建议使用Edge的无头浏览器，可于<a href="https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/">官网</a>下载对应版本的driver驱动。</p>
<h3 id="代码解析"><a href="#代码解析" class="headerlink" title="代码解析"></a>代码解析</h3><p>导入所需包</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br></pre></td></tr></table></figure>
<p>定义所需变量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Mxxxxxxxxx</span></span><br><span class="line">username_str = <span class="string">&#x27;学号&#x27;</span></span><br><span class="line"><span class="comment"># include A-Za-z0-9</span></span><br><span class="line">password_str = <span class="string">&#x27;密码&#x27;</span></span><br><span class="line"><span class="comment"># range: 35.0-45.0, precision: 0.1</span></span><br><span class="line">tw_num = <span class="string">&#x27;体温&#x27;</span></span><br><span class="line"><span class="comment"># 替换成你的edgedriver存放路径</span></span><br><span class="line">edgedriver_dir = <span class="string">r&#x27;C:\Users\GeniusGrass\Downloads\edgedriver_win64\msedgedriver.exe&#x27;</span></span><br><span class="line"><span class="comment"># 健康填报URL</span></span><br><span class="line">url = <span class="string">r&#x27;https://workflow.sues.edu.cn/default/work/shgcd/jkxxcj/jkxxcj.jsp&#x27;</span></span><br></pre></td></tr></table></figure>
<p>读入<del>PhantomJS</del>Edge无头浏览器，创建对象</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># phantom_dir= r&#x27;C:\Users\GeniusGrass\AppData\Local\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs.exe&#x27;</span></span><br><span class="line"><span class="comment"># browser = webdriver.PhantomJS(phantom_dir)</span></span><br><span class="line"></span><br><span class="line">browser = webdriver.Edge(edgedriver_dir) </span><br></pre></td></tr></table></figure>
<p>打开健康填报URL，等待其跳转到登录界面，通过标签找到相应组件，输入账号密码并模拟点击登录。<em>此处登录是提交表单的过程，使用`</em>.submit()<code>方法，如果是一般的</code>button<code>组件，使用</code><em>.click()`方法。</em>登录后等待浏览器加载，即会跳转到体温填报界面。<br>模拟登录后等待浏览器加载完成，进行截图<code>browser.save_screenshot(&#39;picture1.png&#39;)</code>，图片保存至脚本所在目录下，图片名为<code>picture1.png</code>（如下图）。<br><img src="/2021/10/26/picture1.jpg" alt><br>需要注意的是，这一步可能会产生登录错误（账号密码错误、网络异常等），所以下一步操作前需要考虑此因素。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">browser.get(url)</span><br><span class="line">browser.implicitly_wait(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">username = browser.find_element_by_name(<span class="string">&#x27;username&#x27;</span>)</span><br><span class="line">username.send_keys(username_str)</span><br><span class="line"></span><br><span class="line">password = browser.find_element_by_name(<span class="string">&#x27;password&#x27;</span>)</span><br><span class="line">password.send_keys(password_str)</span><br><span class="line"></span><br><span class="line">login_button = browser.find_element_by_name(<span class="string">&#x27;submit&#x27;</span>)</span><br><span class="line">login_button.click()</span><br><span class="line">browser.implicitly_wait(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 截图观察或留作log</span></span><br><span class="line">browser.save_screenshot(<span class="string">&#x27;picture1.png&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>通过观察登录后的体温填报界面，可以发现，其填报信息只有体温数据需要自行输入。所以直接对体温输入框进行检索，对其填入信息即完成信息输入。注意，体温输入要求精度为0.1，且范围在35.0-45.0之间。<br>考虑登录失败的情况，在对体温输入框进行检索时，需要进行异常处理，直接抛出异常终止进程。（可进一步优化，如重新输入账号密码等）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    tw = browser.find_element_by_name(<span class="string">&#x27;tw&#x27;</span>)</span><br><span class="line">    tw2 = tw.find_element_by_name(<span class="string">&#x27;tw&#x27;</span>)</span><br><span class="line">    tw2.clear()</span><br><span class="line">    tw2.send_keys(tw_num)</span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;username or password is invalid!&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Please check your info and retry.&quot;</span>)</span><br><span class="line">    browser.close()</span><br><span class="line">    <span class="keyword">raise</span></span><br></pre></td></tr></table></figure>
<p>将填好的信息提交，并等待浏览器加载后截图。之后关闭<code>PhantomJS</code>对象，打印成功信息。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">submit_button = browser.find_element_by_id(<span class="string">&#x27;post&#x27;</span>)</span><br><span class="line">submit_button.click()</span><br><span class="line">browser.implicitly_wait(<span class="number">3</span>)</span><br><span class="line">browser.save_screenshot(<span class="string">&#x27;picture2.png&#x27;</span>)</span><br><span class="line">browser.close()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Successfully excuted!&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2021/10/26/picture2.jpg" alt></p>
<p>最终可通过脚本目录下保存的截图“picture1.png”和“picture2.png”进行验证。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文通过简单使用Selenium实现了对校园VPN的模拟登录，及健康填报操作，对其他类似的爬虫工作具有一定参考价值。仅从技术层面上讲，具体可进一步使用服务器周期运行脚本，从而实现每日自动填报。</p>
<p><em>本文仅做技术讨论，如使用该技术造成危害概不负责。</em></p>
]]></content>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title>现代算法</title>
    <url>/2019/07/23/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h1 id="现代算法"><a href="#现代算法" class="headerlink" title="现代算法"></a>现代算法</h1><span id="more"></span>
<h2 id="模拟退火"><a href="#模拟退火" class="headerlink" title="模拟退火"></a>模拟退火</h2><p><strong>特别针对TSP问题</strong>的启发式算法。</p>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><ol>
<li><strong>初始化</strong>：初始温度T（足够大）、初始解状态s（算法迭代起点）、每个T值的迭代次数L;</li>
<li><strong>寻找解</strong>：对k=1:L：<ul>
<li>产生新的解</li>
<li>计算新增能量$\triangle f=f(t)-f(s)$（f为目标评价函数）</li>
<li>==若$\triangle f&lt;0,s=t$否则以概率$e^{-\frac{\triangle f}{T}}$接受s=t==</li>
<li>达到终止条件，输出最优解（一般终止条件为没有新解被接受）</li>
</ul>
</li>
<li><strong>降温</strong>：根据T，返回2迭代</li>
</ol>
<h3 id="收敛到最优解的一般条件"><a href="#收敛到最优解的一般条件" class="headerlink" title="收敛到最优解的一般条件"></a>收敛到最优解的一般条件</h3><ol>
<li>T足够高</li>
<li>热平衡时间足够长</li>
<li>终止时间足够长</li>
<li>下降足够缓慢</li>
</ol>
<h3 id="一般产生新解方法"><a href="#一般产生新解方法" class="headerlink" title="一般产生新解方法"></a>一般产生新解方法</h3><h4 id="倒序法"><a href="#倒序法" class="headerlink" title="倒序法"></a>倒序法</h4><p>在2-101之间产生两个随机数u，v之间的数全部倒序排列（包括u，v），产生新的解。</p>
<h4 id="变异法"><a href="#变异法" class="headerlink" title="变异法"></a>变异法</h4><p>在2-101之间产生三个随机数u，v，w将u，v之间的数全部移动到w之后，产生新的解。</p>
<h3 id="实例及算法实现"><a href="#实例及算法实现" class="headerlink" title="实例及算法实现"></a>实例及算法实现</h3><h2 id="遗传算法"><a href="#遗传算法" class="headerlink" title="遗传算法"></a>遗传算法</h2><p>基于自然选择和自然遗传机制的搜索寻优算法。</p>
<h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><ol>
<li>确定可行域和量化方式</li>
<li>对每个解构造衡量解的优良性的适应度函数</li>
<li>确定进化参数群体规模$M$、交叉概率$p_c$、变异概率$p_m$、进化终止条件</li>
</ol>
<h3 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a>步骤</h3><ol>
<li>设置初始参数：种群大小$M$，最大代数$G$，交叉概率$p_c$、变异概率$p_m$；</li>
<li>可行解编码；</li>
<li>确定初始种群；</li>
<li>交叉操作；</li>
<li>变异操作；</li>
<li>选择适应度高的父代和子代构成下一代；</li>
<li>迭代。</li>
</ol>
<h2 id="基于高响应比值优先动态调度算法（HRRN）和贪婪算法（Greedy）的动态加工安排"><a href="#基于高响应比值优先动态调度算法（HRRN）和贪婪算法（Greedy）的动态加工安排" class="headerlink" title="基于高响应比值优先动态调度算法（HRRN）和贪婪算法（Greedy）的动态加工安排"></a>基于高响应比值优先动态调度算法（HRRN）和贪婪算法（Greedy）的动态加工安排</h2><h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h3><p>利用HHRN算法对$n$道工序进行优先级排序：</p>
<script type="math/tex; mode=display">第j个工序的优先级f=\frac{等待时间tw+加工时间t}{加工时间}</script><pre><code>优先原则：

1. HHRN: 按f值排序
2. Greey: f值相等，以t排序
3. 随机: f,t相等，随机排序
</code></pre><p><em>本算法更接近于应用，详见ppt</em></p>
]]></content>
      <tags>
        <tag>数学建模</tag>
      </tags>
  </entry>
  <entry>
    <title>重定向与管道</title>
    <url>/2017/09/17/%E9%87%8D%E5%AE%9A%E5%90%91%E4%B8%8E%E7%AE%A1%E9%81%93/</url>
    <content><![CDATA[<p><strong>重定向与管道，均为在cmd中编译时的技巧。</strong></p>
<h2 id="重定向"><a href="#重定向" class="headerlink" title="重定向"></a>重定向</h2><span id="more"></span>
<p><strong>符号：&gt; </strong></p>
<h4 id="用例"><a href="#用例" class="headerlink" title="用例"></a>用例</h4><hr>
<pre><code>java Test args[0] args[1]··· &gt; data.txt
</code></pre><h4 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h4><hr>
<p>将执行 Test 的输出结果储存至 data.txt 中<br>且命令行中不再输出</p>
<h2 id="管道"><a href="#管道" class="headerlink" title="管道"></a>管道</h2><p><strong>符号：|</strong></p>
<h4 id="用例-1"><a href="#用例-1" class="headerlink" title="用例"></a>用例</h4><hr>
<pre><code>java Test1 args[0] args[1]··· | java Test2
</code></pre><h4 id="效果-1"><a href="#效果-1" class="headerlink" title="效果"></a>效果</h4><hr>
<p>将执行 Test1 的输出结果作为运行 Test2 的参数输入<br>命令行中只显示 Test2 的输出结果</p>
]]></content>
      <tags>
        <tag>《Algorithms》</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>简单Requests爬虫——股票分时数据爬取</title>
    <url>/2021/10/28/%E7%AE%80%E5%8D%95Requests%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E8%82%A1%E7%A5%A8%E5%88%86%E6%97%B6%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96/</url>
    <content><![CDATA[<p>使用Requesets包结合浏览器开发者工具分析网页结构，实现针对股票网站任意股票的单线程无代理爬虫。（仅供学习使用，勿用于破坏网络环境）</p>
<span id="more"></span>
<p>参考推文：<a href="https://mp.weixin.qq.com/s/0f3ELHJzBehxW2LRwCqs1g">手把手教你爬取任意日期全部股票分时数据</a></p>
<h2 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h2><p><strong>简单观察问题</strong> 首先观察网页结构（如下图），可以看到其分时数据是以文本形式直接体现。同时，对于单一股票的分时数据，可能会存在多页的情况，其排列顺序为时间顺序。并且，该网站不要求登录才能获取数据，这就可以确定，只需要简单的爬虫就可以实现对该网站数据进行爬取。爬取后再以csv格式本地储存。</p>
<p><img src="/2021/10/28/简单Requests爬虫——股票分时数据爬取\fullscreen.png" alt></p>
<p>细节方面，第一步先对网站进行分析，在数据位置右键打开选项卡选择检查（如下图），其数据存放方式简单，并且所有内容都保存在各自的相同标签下，所以可以通过对Html解析进行爬取。</p>
<p><img src="/2021/10/28/简单Requests爬虫——股票分时数据爬取\pageanalysis.png" alt></p>
<p>本文从另一角度进行处理，由于分时数据的快速更新特性，需要持续获取，如果大量读入Html文件可能会占用大量内存，从而无法实现大规模并发爬虫，因此使用了参考博文中从GET/POST的响应头获取数据的方法。</p>
<p>首先，打开开发者工具中的网络选项卡，选中保留日志，刷新网页重新进入查看数据相关的GET/POST方法，通过观察各响应头对数据获取的GET/POST请求进行定位（如下图）。</p>
<p><img src="/2021/10/28/简单Requests爬虫——股票分时数据爬取\postanalysis.png" alt></p>
<p>观察发现，其GET/POST响应头中数据结构为：整个页面的数据放在键值对<code>&quot;data&quot;:value</code>中，其不同时间点数据表现为<code>list</code>中的元素，其每个时间点数据表现为<code>dict</code>，分别有</p>
<p><code>&quot;t&quot;:91503, &quot;p&quot;:8990, &quot;v&quot;:21, &quot;bs&quot;:4</code></p>
<p>其中，<code>t</code>表示时间，<code>p</code>表示成交价，<code>v</code>表示手数，<code>bs</code>表示交易类型。以上述时间点为例，表示时间为09:15:03，交易价为8.99，手数为21，交易类型为4（具体代表含义本文不做讨论）。</p>
<h2 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h2><p>综上所述，已经得到了足够的信息，可以着手编写代码。首先，构造爬虫方面，导入所需包并读取响应头信息。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://push2ex.eastmoney.com/getStockFenShi&#x27;</span></span><br><span class="line">params = (</span><br><span class="line">        (<span class="string">&#x27;pagesize&#x27;</span>,<span class="string">&#x27;144&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;ut&#x27;</span>,<span class="string">&#x27;7eea3edcaed734bea9cbfc24409ed989&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;dpt&#x27;</span>,<span class="string">&#x27;wzfscj&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;cb&#x27;</span>,<span class="string">&#x27;jQuery112406727726525607527_1633998766392&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;pageindex&#x27;</span>, <span class="string">&#x27;0&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;id&#x27;</span>,<span class="string">&#x27;6000001&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;sort&#x27;</span>,<span class="string">&#x27;1&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;ft&#x27;</span>,<span class="string">&#x27;1&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;code&#x27;</span>,<span class="string">&#x27;600000&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;market&#x27;</span>,<span class="string">&#x27;1&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;_&#x27;</span>,<span class="string">&#x27;1633998766403&#x27;</span>)</span><br><span class="line">    )</span><br><span class="line">headers = &#123;</span><br><span class="line"><span class="string">&#x27;Host&#x27;</span>:<span class="string">&#x27;push2ex.eastmoney.com&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;Connection&#x27;</span>:<span class="string">&#x27;keep-alive&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) \</span></span><br><span class="line"><span class="string">AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 \</span></span><br><span class="line"><span class="string">    Safari/537.36 Edg/94.0.992.38&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;DNT&#x27;</span>:<span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Accept&#x27;</span>:<span class="string">&#x27;*/*&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Referer&#x27;</span>:<span class="string">&#x27;http://quote.eastmoney.com/&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Accept-Encoding&#x27;</span>:<span class="string">&#x27;gzip, deflate&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Accept-Language&#x27;</span>:<span class="string">&#x27;zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">cookies = &#123;<span class="string">&quot;one_cookie&quot;</span>:<span class="string">&quot;qgqp_b_id=c40; st_si=20784472685104; st_asi=delete; em_hq_fls=js; HAList=a-; st_pvi=47008391842977; st_sp=2021-10-12%2008%3A29%3A28; st_inirUrl=https%3A%2F%2Fcn.bing.com%2F; st_sn=6; st_psi=20211012083246623-0-4344614687&quot;</span>&#125;</span><br><span class="line">response = requests.get(url, headers=headers, params=params, cookies=cookies, verify=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>其中，各参数可由GET/POST请求的标头中得到（<em>代码中cookie有改动</em>），如下图所示。</p>
<p><img src="/2021/10/28/简单Requests爬虫——股票分时数据爬取\param1.png" alt><br><img src="/2021/10/28/简单Requests爬虫——股票分时数据爬取\param2.png" alt><br><img src="/2021/10/28/简单Requests爬虫——股票分时数据爬取\param3.png" alt="对应上图中的params"></p>
<p>对运行得到的<code>response</code>内容进行初步选取，结果如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response.text[<span class="number">43</span>:-<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#x27;&quot;rc&quot;:0,&quot;rt&quot;:108,&quot;svr&quot;:181735295,&quot;lt&quot;:2,&quot;full&quot;:0,&quot;data&quot;:</span><br><span class="line">&#123;&quot;c&quot;:&quot;600000&quot;,&quot;m&quot;:1,&quot;n&quot;:&quot;浦发银行&quot;,&quot;ct&quot;:0,&quot;cp&quot;:9150,&quot;tc&quot;:1345,</span><br><span class="line">&quot;data&quot;:</span><br><span class="line">[&#123;&quot;t&quot;:91503,&quot;p&quot;:9160,&quot;v&quot;:23,&quot;bs&quot;:4&#125;,</span><br><span class="line">&#123;&quot;t&quot;:92021,&quot;p&quot;:9150,&quot;v&quot;:143,&quot;bs&quot;:4&#125;,</span><br><span class="line">&#123;&quot;t&quot;:92403,&quot;p&quot;:9140,&quot;v&quot;:453,&quot;bs&quot;:4&#125;,</span><br><span class="line">&#123;&quot;t&quot;:92424,&quot;p&quot;:9130,&quot;v&quot;:506,&quot;bs&quot;:4&#125;,</span><br><span class="line">&#123;&quot;t&quot;:92451,&quot;p&quot;:9120,&quot;v&quot;:637,&quot;bs&quot;:4&#125;,</span><br><span class="line">&#123;&quot;t&quot;:92505,&quot;p&quot;:9120,&quot;v&quot;:793,&quot;bs&quot;:1&#125;,</span><br><span class="line">&#123;&quot;t&quot;:93000,&quot;p&quot;:9120,&quot;v&quot;:60,&quot;bs&quot;:1&#125;,</span><br><span class="line">&#123;&quot;t&quot;:93003,&quot;p&quot;:9120,&quot;v&quot;:961,&quot;bs&quot;:1&#125;,</span><br><span class="line">&#123;&quot;t&quot;:93006,&quot;p&quot;:9140,&quot;v&quot;:531,&quot;bs&quot;:2&#125;,</span><br><span class="line">......</span><br><span class="line">&#123;&quot;t&quot;:93651,&quot;p&quot;:9140,&quot;v&quot;:20,&quot;bs&quot;:1&#125;,</span><br><span class="line">&#123;&quot;t&quot;:93654,&quot;p&quot;:9140,&quot;v&quot;:24,&quot;bs&quot;:1&#125;,</span><br><span class="line">&#123;&quot;t&quot;:93657,&quot;p&quot;:9140,&quot;v&quot;:14,&quot;bs&quot;:1&#125;,</span><br><span class="line">&#123;&quot;t&quot;:93700,&quot;p&quot;:9140,&quot;v&quot;:326,&quot;bs&quot;:1&#125;,</span><br><span class="line">&#123;&quot;t&quot;:93703,&quot;p&quot;:9140,&quot;v&quot;:224,&quot;bs&quot;:2&#125;,</span><br><span class="line">&#123;&quot;t&quot;:93706,&quot;p&quot;:9150,&quot;v&quot;:22,&quot;bs&quot;:2&#125;,</span><br><span class="line">&#123;&quot;t&quot;:93709,&quot;p&quot;:9130,&quot;v&quot;:37,&quot;bs&quot;:1&#125;,</span><br><span class="line">&#123;&quot;t&quot;:93712,&quot;p&quot;:9130,&quot;v&quot;:1361,&quot;bs&quot;:1&#125;]&#125;&#125;&#x27;</span><br></pre></td></tr></table></figure>
<p>其上内容包含了所有所需数据，所以可以作为初步的数据划分。由观察可以发现，可以利用<code>&#123;</code>进一步划分，划分后的<code>list</code>中从第三个元素开始直至最后一个是我们需要的数据，即<code>response.text[43:-2].split(&quot;&#123;&quot;)[2:]</code>。此时，针对其中第一个元素，其形式应该为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;&quot;t&quot;:91503,&quot;p&quot;:9160,&quot;v&quot;:23,&quot;bs&quot;:4&#125;,&quot;</span><br></pre></td></tr></table></figure>
<p>显然，可以使用<code>:</code>再次划分，得到</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[&quot;\&quot;\&quot;t\&quot;:&quot; , &quot;91503,\&quot;p\&quot;:&quot; , &quot;9160,\&quot;v\&quot;:&quot; , &quot;23,\&quot;bs\&quot;:&quot; , &quot;4&#125;,\&quot;&quot;]</span><br></pre></td></tr></table></figure>
<p>显然，我们关心的数据已经被完全划分出来了，只需要将数据后无用的内容割去即可，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># i 为上文分割出的一行数据</span></span><br><span class="line">sp = i.split(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">t = sp[<span class="number">1</span>].split(<span class="string">&quot;,&quot;</span>)[<span class="number">0</span>] <span class="comment"># 时间</span></span><br><span class="line">p = sp[<span class="number">2</span>].split(<span class="string">&quot;,&quot;</span>)[<span class="number">0</span>] <span class="comment"># 价格</span></span><br><span class="line">v = sp[<span class="number">3</span>].split(<span class="string">&quot;,&quot;</span>)[<span class="number">0</span>] <span class="comment"># 交易手数</span></span><br><span class="line">bs = sp[<span class="number">4</span>].split(<span class="string">&quot;&#125;&quot;</span>)[<span class="number">0</span>] <span class="comment"># 交易类型</span></span><br><span class="line"><span class="comment"># 将时间化为 09:15:03 形式</span></span><br><span class="line">t = <span class="built_in">str</span>(t)</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(t) &lt; <span class="number">6</span>:</span><br><span class="line">    t = <span class="string">&quot;0&quot;</span> + t</span><br><span class="line">time = t[:<span class="number">2</span>] + <span class="string">&quot;:&quot;</span> + t[<span class="number">2</span>:<span class="number">4</span>] + <span class="string">&quot;:&quot;</span> + t[-<span class="number">2</span>:]</span><br></pre></td></tr></table></figure>
<p>由此，即将一行数据中的数据依次提取出来了，结合循环即可达到取出所有行的数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> response.text[<span class="number">43</span>:-<span class="number">2</span>].split(<span class="string">&quot;&#123;&quot;</span>)[<span class="number">2</span>:]:</span><br><span class="line">    sp = i.split(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">    t = sp[<span class="number">1</span>].split(<span class="string">&quot;,&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">    p = sp[<span class="number">2</span>].split(<span class="string">&quot;,&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">    v = sp[<span class="number">3</span>].split(<span class="string">&quot;,&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">    bs = sp[<span class="number">4</span>].split(<span class="string">&quot;&#125;&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">    t = <span class="built_in">str</span>(t)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(t) &lt; <span class="number">6</span>:</span><br><span class="line">        t = <span class="string">&quot;0&quot;</span> + t</span><br><span class="line">    time = t[:<span class="number">2</span>] + <span class="string">&quot;:&quot;</span> + t[<span class="number">2</span>:<span class="number">4</span>] + <span class="string">&quot;:&quot;</span> + t[-<span class="number">2</span>:]</span><br><span class="line">    <span class="comment"># 保存 time,p,v,bs</span></span><br></pre></td></tr></table></figure>
<p>这样一来即获得了一页中的所有数据，而注意到在<code>params</code>中有参数<code>pageindex: 0</code>，通过改变该参数即可得到不同页面中的数据，以此就可以利用循环实现一次性自动爬取全部页面。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> page_num <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line"></span><br><span class="line">    params = (</span><br><span class="line">        (<span class="string">&#x27;pagesize&#x27;</span>,<span class="string">&#x27;144&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;ut&#x27;</span>,<span class="string">&#x27;7eea3edcaed734bea9cbfc24409ed989&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;dpt&#x27;</span>,<span class="string">&#x27;wzfscj&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;cb&#x27;</span>,<span class="string">&#x27;jQuery112406727726525607527_1633998766392&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;pageindex&#x27;</span>, page_num),</span><br><span class="line">        (<span class="string">&#x27;id&#x27;</span>,<span class="string">&#x27;6000001&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;sort&#x27;</span>,<span class="string">&#x27;1&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;ft&#x27;</span>,<span class="string">&#x27;1&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;code&#x27;</span>,<span class="string">&#x27;600000&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;market&#x27;</span>,<span class="string">&#x27;1&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;_&#x27;</span>,<span class="string">&#x27;1633998766403&#x27;</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">#############################################</span></span><br><span class="line">    <span class="comment">#### 处理过程略</span></span><br><span class="line">    <span class="comment">#############################################</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>最后，添加文件读写操作，即实现了对某一只股票的全部分时数据爬取。</p>
<h2 id="最终代码"><a href="#最终代码" class="headerlink" title="最终代码"></a>最终代码</h2><p>综上，将所有代码结合后，数据保存到当前目录下的”600000.csv”文件中，可以得到最终代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;600000.csv&quot;</span>, <span class="string">&#x27;a&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    writer = csv.writer(f)</span><br><span class="line">    writer.writerow([<span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;price&#x27;</span>, <span class="string">&#x27;num&#x27;</span>, <span class="string">&#x27;bs&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    url = <span class="string">&#x27;http://push2ex.eastmoney.com/getStockFenShi&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> page_num <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line"></span><br><span class="line">        params = (</span><br><span class="line">            (<span class="string">&#x27;pagesize&#x27;</span>,<span class="string">&#x27;144&#x27;</span>),</span><br><span class="line">            (<span class="string">&#x27;ut&#x27;</span>,<span class="string">&#x27;7eea3edcaed734bea9cbfc24409ed989&#x27;</span>),</span><br><span class="line">            (<span class="string">&#x27;dpt&#x27;</span>,<span class="string">&#x27;wzfscj&#x27;</span>),</span><br><span class="line">            (<span class="string">&#x27;cb&#x27;</span>,<span class="string">&#x27;jQuery112406727726525607527_1633998766392&#x27;</span>),</span><br><span class="line">            (<span class="string">&#x27;pageindex&#x27;</span>, page_num),</span><br><span class="line">            (<span class="string">&#x27;id&#x27;</span>,<span class="string">&#x27;6000001&#x27;</span>),</span><br><span class="line">            (<span class="string">&#x27;sort&#x27;</span>,<span class="string">&#x27;1&#x27;</span>),</span><br><span class="line">            (<span class="string">&#x27;ft&#x27;</span>,<span class="string">&#x27;1&#x27;</span>),</span><br><span class="line">            (<span class="string">&#x27;code&#x27;</span>,<span class="string">&#x27;600000&#x27;</span>),</span><br><span class="line">            (<span class="string">&#x27;market&#x27;</span>,<span class="string">&#x27;1&#x27;</span>),</span><br><span class="line">            (<span class="string">&#x27;_&#x27;</span>,<span class="string">&#x27;1633998766403&#x27;</span>)</span><br><span class="line">        )</span><br><span class="line">        headers = &#123;</span><br><span class="line">        <span class="string">&#x27;Host&#x27;</span>:<span class="string">&#x27;push2ex.eastmoney.com&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;Connection&#x27;</span>:<span class="string">&#x27;keep-alive&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) \</span></span><br><span class="line"><span class="string">        AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 \</span></span><br><span class="line"><span class="string">            Safari/537.36 Edg/94.0.992.38&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;DNT&#x27;</span>:<span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Accept&#x27;</span>:<span class="string">&#x27;*/*&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Referer&#x27;</span>:<span class="string">&#x27;http://quote.eastmoney.com/&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Accept-Encoding&#x27;</span>:<span class="string">&#x27;gzip, deflate&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Accept-Language&#x27;</span>:<span class="string">&#x27;zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line">        cookies = &#123;<span class="string">&quot;one_cookie&quot;</span>:<span class="string">&quot;qgqp_b_id=e88f78d4eecdc0672be30cada8c59c40; st_si=20784472685104; st_asi=delete; em_hq_fls=js; HAList=a-sh-600000-%u6D66%u53D1%u94F6%u884C%2Cf-0-000001-%u4E0A%u8BC1%u6307%u6570; st_pvi=47008391842977; st_sp=2021-10-12%2008%3A29%3A28; st_inirUrl=https%3A%2F%2Fcn.bing.com%2F; st_sn=6; st_psi=20211012083246623-0-4344614687&quot;</span>&#125;</span><br><span class="line">        response = requests.get(url, headers=headers, params=params, cookies=cookies, verify=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> response.text[<span class="number">43</span>:-<span class="number">2</span>].split(<span class="string">&quot;&#123;&quot;</span>)[<span class="number">2</span>:]:</span><br><span class="line">            sp = i.split(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">            t = sp[<span class="number">1</span>].split(<span class="string">&quot;,&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">            p = sp[<span class="number">2</span>].split(<span class="string">&quot;,&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">            v = sp[<span class="number">3</span>].split(<span class="string">&quot;,&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">            bs = sp[<span class="number">4</span>].split(<span class="string">&quot;&#125;&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">            t = <span class="built_in">str</span>(t)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(t) &lt; <span class="number">6</span>:</span><br><span class="line">                t = <span class="string">&quot;0&quot;</span> + t</span><br><span class="line">            time = t[:<span class="number">2</span>] + <span class="string">&quot;:&quot;</span> + t[<span class="number">2</span>:<span class="number">4</span>] + <span class="string">&quot;:&quot;</span> + t[-<span class="number">2</span>:]</span><br><span class="line"></span><br><span class="line">            writer.writerow([time, p, v, bs])</span><br></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文对参考博文中的代码进行了补充，并附带了更加详细的思路，希望能对读者进行简单爬虫时有所启发或帮助。其中部分代码可用更优雅的方式解决，请读者自行研究。</p>
<p>另外，在对网站进行爬取时，通常需要人为添加时间间隔，防止网站封锁ip；在对会封锁ip的网站进行爬取时，通常需要添加代理池；在爬取量较大时，通常会使用多线程方法；在对网页Html解析爬取数据时，通常需要使用<code>BeautifulSoup</code>······因此，本文仅作为抛砖引玉。</p>
<p><em>本文仅做技术讨论，如使用该技术造成危害概不负责。</em></p>
]]></content>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title>链表实现(一)</title>
    <url>/2017/12/02/%E9%93%BE%E8%A1%A8%E5%AE%9E%E7%8E%B0(%E4%B8%80)/</url>
    <content><![CDATA[<h2 id="链表基本功能"><a href="#链表基本功能" class="headerlink" title="链表基本功能"></a>链表基本功能</h2><span id="more"></span>
<ol>
<li><p>数据串动态存储</p>
</li>
<li><p>数据查找</p>
</li>
<li><p>数据删改</p>
</li>
</ol>
<p><em>数组无法满足需要，因而引入新的数据类型</em></p>
<h2 id="链表核心思想"><a href="#链表核心思想" class="headerlink" title="链表核心思想"></a>链表核心思想</h2><p><strong>引用 + 递归</strong><br><em>还将使用泛型，私有内部类等技巧</em></p>
<h2 id="链表结构"><a href="#链表结构" class="headerlink" title="链表结构"></a>链表结构</h2><p><em>Link类中方法通过 引用 + 递归 的模式调用Node类中方法，以实现需求</em></p>
<h4 id="Link类"><a href="#Link类" class="headerlink" title="Link类"></a>Link类</h4><pre><code>class Link&#123;
    private Node root; // 链表头
    private Object[] rtArray; // 返回数据数组
    private int foot = 0; // 作用于Node类中，起到迭代作用
    private int count; // 实现计数
    private class Node&#123;...&#125; // 构建Node类，private只从Link类访问

    public void add(Object data)&#123;...&#125; // 调用Node类中addNode()方法
    public boolean isEmpty()&#123;...&#125;
    public int size()&#123;...&#125; // 返回count的值
    public Object [] toArray()&#123;...&#125; // 递归调用toArrayNode()方法
    public boolean contains(Object reserch)&#123;...&#125; // 递归调用containsNode()方法
    public Object get(int index)&#123;...&#125; // 递归调用getNode()方法
    public void set(int index)&#123;...&#125; // 递归调用setNode()方法
    public void remove(Object data)&#123;...&#125; // 递归调用removeNode()方法
&#125;
</code></pre><h4 id="Node私有内部类"><a href="#Node私有内部类" class="headerlink" title="Node私有内部类"></a>Node私有内部类</h4><pre><code>private class Node&#123;
    private Object data; // 储存的数据内容
    private Node next; // 下一个Node(节点)

    public Node(Object data)&#123;...&#125; // 构造函数，方便节点创建
    public void addNode()&#123;...&#125; // 递归方法；增加节点方法
    public void toArrayNode()&#123;...&#125; // 递归方法；利用Link类的foot属性
    public boolean containsNode(Object reserch)&#123;...&#125; // 递归方法；判断是否存在 reserch 数据
    public void setNode(int index, Object data)&#123;...&#125; // 递归方法；设置index方便迭代
    public Object getNode(int index)&#123;...&#125; // 递归方法
    public void removeNode(Node previous, Object data)&#123;...&#125; // 递归方法；通过修改previous.next实现数据删除
    public void setNext(Node next)&#123;...&#125;
    public void getNext()&#123;...&#125;
    public void getData()&#123;...&#125;
&#125;
</code></pre><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>  以上即是链表的相关内容。<br>  <strong>链表是最基础的数据结构之一，要求熟练掌握</strong>。</p>
]]></content>
      <tags>
        <tag>java</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>实验室学习时长脚本</title>
    <url>/2021/10/28/%E5%AE%9E%E9%AA%8C%E5%AE%A4%E5%AD%A6%E4%B9%A0%E6%97%B6%E9%95%BF%E8%84%9A%E6%9C%AC/</url>
    <content><![CDATA[<p>实验室安全教育时长需要一小时才能参加考试，并且5分钟内必须更换学习网页，所以通过Selenium控制Edge浏览器进行定时刷新，从而达到自动刷时长的方法。（仅供学习使用，勿用于破坏网络环境）</p>
<span id="more"></span>
<h2 id="观察问题"><a href="#观察问题" class="headerlink" title="观察问题"></a>观察问题</h2><p>校外需要通过校园VPN登入，登入后需要进入实验室安全网站进行学习等等一系列操作，因此难度极大。于是，本文采用Selenium控制Edge浏览器实现半自动脚本，通过Selenium创建Edge浏览器进程，由用户自己登录进入学习网站，之后通过简单代码进行定时刷新，即实现了自动刷时长的过程。</p>
<h2 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h2><p>使用notebook环境（如jupyter notebook、jupyterlab等），先运行Selenium创建Edge进程（Selenium和Edge driver的安装参考前文：<a href="https://www.zzforgood.top/2021/10/26/%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95%E8%87%AA%E5%8A%A8%E5%81%A5%E5%BA%B7%E5%A1%AB%E6%8A%A5%E8%84%9A%E6%9C%AC/">模拟登录自动健康填报脚本</a>）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">browser = webdriver.Edge(<span class="string">r&#x27;C:\Users\GeniusGrass\Downloads\edgedriver_win64\msedgedriver.exe&#x27;</span>) </span><br></pre></td></tr></table></figure>
<p>在打开的Edge进程的空白网页中登录校园VPN（学校官网最下方）。</p>
<p><img src="/2021/10/28/实验室学习时长脚本\login.png" alt></p>
<p>第二步，进入实验室学习网站（没有通过VPN登录过该网站的需要在上方搜索栏输入相应网址）。</p>
<p><img src="/2021/10/28/实验室学习时长脚本\site.png" alt></p>
<p>第三步，打开学习界面。</p>
<p><img src="/2021/10/28/实验室学习时长脚本\steps.png" alt></p>
<p>第四步，运行下述代码块：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    time.sleep(<span class="number">121</span>)</span><br><span class="line">    browser.refresh()</span><br></pre></td></tr></table></figure>
<p>脚本将2分钟刷新一次页面，保持notebook进程打开，直到1小时任务完成强制关闭即可。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>使用Selenium控制Edge会先弹出浏览器界面，对于自动化爬虫来说相对不便且耗费内存，但对于复杂任务而言，可视化界面和可交互性无可替代。因此，可以进一步发掘此类用法以优化生活。</p>
<p><em>本文仅做技术讨论，如使用该技术造成危害概不负责。</em></p>
]]></content>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title>下载超星课件脚本</title>
    <url>/2021/10/31/%E4%B8%8B%E8%BD%BD%E8%B6%85%E6%98%9F%E8%AF%BE%E4%BB%B6%E8%84%9A%E6%9C%AC/</url>
    <content><![CDATA[<p>由于日常学习需要参考超星课件，而超星学习界面中课件加载速度慢，刷新后需要重新加载等聪明操作使得学习过程异常艰难，因此本文通过对网页代码解析，找到文件原链接对课件进行下载。（仅供学习使用，勿用于破坏网络环境）</p>
<span id="more"></span>
<p>参考内容：<a href="https://www.zhihu.com/question/448827791">知乎</a></p>
<h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><p>首先，观察超星学习页面，最直观的信息表示，课件所在框架应该指向课件资源。但暂时无法确定其指向路径和文件类型。此时，通过搜索引擎对问题进行简单搜索，发现存在<a href="https://www.zhihu.com/question/448827791">相关信息</a>。</p>
<p><img src="/2021/10/31/下载超星课件脚本\reference.png" alt></p>
<p>根据其解读，可以发现课件的真实存储地址的获取方式，并且其存放类型为pdf。于是，可以通过爬虫的方法对文件进行下载。</p>
<h2 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h2><p>首先导入相关包</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br></pre></td></tr></table></figure>
<p>观察发现，其课件单页是以png格式存储，直接打开即可得到<code>objectid</code>，故可以接受一页课件的url地址，找出其<code>objectid</code>。其单页课件地址可以通过鼠标将其图片拖至浏览器地址栏得到，如下图所示。</p>
<p><img src="/2021/10/31/下载超星课件脚本\get_url1.png" alt></p>
<p><img src="/2021/10/31/下载超星课件脚本\get_url2.png" alt></p>
<p>以某计量经济学第四章为例，定义从png图片获取<code>objectid</code>的方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_url = <span class="string">r&quot;https://s3.ananas.chaoxing.com/sv-w7/doc/60/11/0c/3b6736ce7b901056ae84a949b24d6e81/thumb/1.png&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getid</span>(<span class="params">input_url</span>):</span></span><br><span class="line">    part_url = input_url.split(<span class="string">&quot;/&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i, part <span class="keyword">in</span> <span class="built_in">enumerate</span>(part_url):</span><br><span class="line">        <span class="keyword">if</span> part == <span class="string">&quot;thumb&quot;</span>:</span><br><span class="line">            <span class="keyword">return</span> part_url[i-<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提前定义异常</span></span><br><span class="line">fileid = getid(input_url)</span><br><span class="line"><span class="keyword">if</span> fileid == <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">raise</span> FileNotFoundError</span><br><span class="line"></span><br><span class="line">getid(input_url)</span><br></pre></td></tr></table></figure>
<p>out: 3b6736ce7b901056ae84a949b24d6e81</p>
<p>得到其<code>objectid</code>再转向中介页面，定义<code>axe_url</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">axe_url = <span class="string">r&quot;https://mooc1-1.chaoxing.com/ananas/status/&quot;</span> + fileid + <span class="string">r&quot;?flag=normal&quot;</span></span><br></pre></td></tr></table></figure>
<p>观察中介页面，如下图。</p>
<p><img src="/2021/10/31/下载超星课件脚本\axe_page.png" alt></p>
<p>显然，其明文内容为json格式。于是，可以通过读取该页面，将其内容以json格式读取为<code>dict</code>，再从中获取<code>pdf</code>的值，即得到课件的真实存储路径。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">headers = requests.utils.default_headers()</span><br><span class="line">headers[<span class="string">&#x27;User-Agent&#x27;</span>] = <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36 Edg/95.0.1020.40&quot;</span></span><br><span class="line"></span><br><span class="line">req = requests.get(axe_url, headers=headers)</span><br><span class="line">dic = json.loads(req.text)</span><br><span class="line"></span><br><span class="line">dic[<span class="string">&#x27;pdf&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>out:<a href="https://s3.ananas.chaoxing.com/doc/60/11/0c/3b6736ce7b901056ae84a949b24d6e81/pdf/3b6736ce7b901056ae84a949b24d6e81.pdf">https://s3.ananas.chaoxing.com/doc/60/11/0c/3b6736ce7b901056ae84a949b24d6e81/pdf/3b6736ce7b901056ae84a949b24d6e81.pdf</a></p>
<p>最后，通过爬虫方式下载并写入本地文件（例为当前目录下）即可</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">filename = <span class="string">&quot;计量经济学第四章&quot;</span></span><br><span class="line"></span><br><span class="line">df = requests.get(dic[<span class="string">&#x27;pdf&#x27;</span>])</span><br><span class="line"><span class="built_in">open</span>(filename + <span class="string">&quot;.pdf&quot;</span>,<span class="string">&#x27;wb&#x27;</span>).write(pdf.content)</span><br></pre></td></tr></table></figure>
<h2 id="最终代码"><a href="#最终代码" class="headerlink" title="最终代码"></a>最终代码</h2><p>整合上述代码，调整结构，可得到最终代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 课件某页url</span></span><br><span class="line">input_url = <span class="string">r&quot;https://s3.ananas.chaoxing.com/doc/cb/49/2e/a626aad4c9dff450c728709ed983b461/thumb/2.png&quot;</span></span><br><span class="line"><span class="comment"># 存储文件名称（无需后缀）</span></span><br><span class="line">filename = <span class="string">r&quot;高等统计学正交表&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getid</span>(<span class="params">input_url</span>):</span></span><br><span class="line">    part_url = input_url.split(<span class="string">&quot;/&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i, part <span class="keyword">in</span> <span class="built_in">enumerate</span>(part_url):</span><br><span class="line">        <span class="keyword">if</span> part == <span class="string">&quot;thumb&quot;</span>:</span><br><span class="line">            <span class="keyword">return</span> part_url[i-<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">fileid = getid(input_url)</span><br><span class="line"><span class="keyword">if</span> fileid == <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">raise</span> FileNotFoundError</span><br><span class="line"></span><br><span class="line">axe_url = <span class="string">r&quot;https://mooc1-1.chaoxing.com/ananas/status/&quot;</span> + fileid + <span class="string">r&quot;?flag=normal&quot;</span></span><br><span class="line"></span><br><span class="line">headers = requests.utils.default_headers()</span><br><span class="line">headers[<span class="string">&#x27;User-Agent&#x27;</span>] = <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36 Edg/95.0.1020.40&quot;</span></span><br><span class="line"></span><br><span class="line">req = requests.get(axe_url, headers=headers)</span><br><span class="line">dic = json.loads(req.text)</span><br><span class="line"></span><br><span class="line">pdf = requests.get(dic[<span class="string">&#x27;pdf&#x27;</span>])</span><br><span class="line"><span class="built_in">open</span>(filename + <span class="string">&quot;.pdf&quot;</span>,<span class="string">&#x27;wb&#x27;</span>).write(pdf.content)</span><br></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过爬虫读取网页、下载文件都是比较常见的工作，将其结合更能实现复杂的操作，另外还可以增加对GET/POST响应头分析，就可以胜任绝大部分工作。当然，此类操作如果使用js编写，可以作为浏览器脚本使用，能够集成实现打开网页一键下载，故本文仅作为分享学习抛砖引玉，希望可以引发对学习或生活的思考。</p>
<p><em>本文仅做技术讨论，如使用该技术造成危害概不负责。</em></p>
]]></content>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title>市调大赛视频下载脚本</title>
    <url>/2021/11/01/%E5%B8%82%E8%B0%83%E5%A4%A7%E8%B5%9B%E8%A7%86%E9%A2%91%E4%B8%8B%E8%BD%BD%E8%84%9A%E6%9C%AC/</url>
    <content><![CDATA[<p>在久违的休息日起床发现已经错过了半个小时的直播内容，还被要求需要录制大赛视频，后期录制又诸多不顺，好在有视频回放。本着所见即可得的想法，动手写了如此脚本。（仅供学习使用，勿用于破坏网络环境）</p>
<span id="more"></span>
<p>参考博文：<a href="https://blog.csdn.net/qq_46018418/article/details/108102206">Python爬取blob:http//的视频</a></p>
<h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><p>对视频进行下载应该是最容易的爬虫（广义），通常仅需要右键复制视频链接就基本完工，先观察网页内容，发现右键无法得到视频链接也没有下载按钮（经查阅是通过某tag:nodownload和return false进行实现，如下图）。</p>
<p><img src="/2021/11/01/市调大赛视频下载脚本\view1.png" alt></p>
<p>同时，可以从上图中看到其<code>src</code>地址，以<code>blob:https</code>开头，显然不是真实的视频地址，自然无法直接得到视频。此时问题开始复杂，但无论如何这都是可以无限重播的视频，自然不存在无法爬取的情况。因此，只能采用最暴力的方式，解析视频页面发送的每段视频缓存，保存全部缓存后再合并取得全视频。而视频缓存地址通常可以在GET/POST请求头中发现，可惜此页面使用了其他方式将缓存方法隐藏了（下图），这就进一步增大了爬取的难度。</p>
<p><img src="/2021/11/01/市调大赛视频下载脚本\view2.png" alt></p>
<p>通过查阅得知，部分浏览器插件可以对网页视频嗅探并分段下载。本文中使用的是Edge浏览器的插件（随便找的），开启插件并播放网页视频，可以从视频开头嗅探到以下文件（下图），可以观察到文件命名存在规律，即都是以相同文件名为基础更改最后的编号命名。</p>
<p><img src="/2021/11/01/市调大赛视频下载脚本\downloader1.png" alt></p>
<p>进一步对插件中的可下载分段视频使用浏览器开发者工具检查，可以找到该分段文件的下载地址（下图）。</p>
<p><img src="/2021/11/01/市调大赛视频下载脚本\downloader2.png" alt></p>
<p>同时，根据对文件命名规则的猜测，需要将视频进度拉至最后数十秒，观察插件嗅探到的视频分段命名，由此得到最后一个文件的编号（下图），此时即可开始着手编写代码解决问题。</p>
<p><img src="/2021/11/01/市调大赛视频下载脚本\downloader3.png" alt></p>
<h2 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h2><p>先以第一个片段为例（v.f100230_0.ts），导入相关包</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br></pre></td></tr></table></figure>
<p>根据视频片段命名规则，构建地址头和地址尾。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num = <span class="number">0</span></span><br><span class="line">url_head = <span class="string">r&quot;https://1252524126.vod2.myqcloud.com/2919df88vodtranscq1252524126/3d68a8598602268011224008468/&quot;</span></span><br><span class="line">url_tail = <span class="string">r&quot;v.f100230_&quot;</span> + <span class="built_in">str</span>(num) + <span class="string">r&quot;.ts&quot;</span></span><br><span class="line">url = url_head + url_tail</span><br></pre></td></tr></table></figure>
<p>读取并保存（保存在当前目录下的cache文件夹中）该片段。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">path = <span class="string">&quot;cache\\&quot;</span> + url_tail</span><br><span class="line">part = requests.get(url)</span><br><span class="line"><span class="built_in">open</span>(path, <span class="string">&#x27;wb&#x27;</span>).write(part.content)</span><br></pre></td></tr></table></figure>
<p>由此即得到文件v.f100230_0.ts。</p>
<h2 id="最终代码"><a href="#最终代码" class="headerlink" title="最终代码"></a>最终代码</h2><p>组合上述代码，加入循环方法即可得到爬取全部片段的代码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">final_num = <span class="number">4607</span></span><br><span class="line"></span><br><span class="line">url_head = <span class="string">r&quot;https://1252524126.vod2.myqcloud.com/2919df88vodtranscq1252524126/3d68a8598602268011224008468/&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(final_num):</span><br><span class="line">    url_tail = <span class="string">r&quot;v.f100230_&quot;</span> + <span class="built_in">str</span>(i) + <span class="string">r&quot;.ts&quot;</span></span><br><span class="line">    url = url_head + url_tail</span><br><span class="line">    path = <span class="string">&quot;cache\\&quot;</span> + url_tail</span><br><span class="line">    part = requests.get(url)</span><br><span class="line">    <span class="built_in">open</span>(path, <span class="string">&#x27;wb&#x27;</span>).write(part.content)</span><br></pre></td></tr></table></figure>
<p>得到全部ts片段后，还要进行视频片段的合并和格式转换。视频片段合并可以通过cmd的copy命令简单实现，其后可以使用ffmpeg对合并后的ts文件重新打包为其他格式，即可得到最终方便播放的视频（视频仅供学习使用）。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>此类代码可以用于其他视频的下载，由此，除VIP类限制观看视频以外，不可下载不过是面上的君子协议。但是，一切行为都应建立在不破坏法律法规的基础上，使用技术进行丰富生活时更应注意不要触犯国家法律。</p>
<p><em>本文仅做技术讨论，如使用该技术造成危害概不负责。</em></p>
]]></content>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
</search>
