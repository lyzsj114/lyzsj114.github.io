<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  <title>
    Chapter5 神经网络 |
    
    GeniusGrass&#39;s Blog
  </title>
  
    <link rel="shortcut icon" href="/favicon.ico">
    
  
<link rel="stylesheet" href="/css/style.css">

  
  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<script src="/js/pace.min.js"></script>

<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <main class="content">
    <section class="outer">
  <article id="post-Chapter5神经网络" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
      

<h1 class="article-title" itemprop="name">
  Chapter5 神经网络
</h1>



    </header>
    

    
    <div class="article-meta">
      <a href="/2019/09/20/Chapter5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="article-date">
  <time datetime="2019-09-20T04:39:17.000Z" itemprop="datePublished">2019-09-20</time>
</a>
      
    </div>
    

    
    
<div class="tocbot"></div>

    

    <div class="article-entry" itemprop="articleBody">
      
      
      
      <p>本文内容：</p>
<ul>
<li>神经元模型</li>
<li>感知机与多层网络</li>
<li>误差逆传播算法</li>
<li>全局最小与局部最小</li>
</ul>
<span id="more"></span>
<h2 id="5-1-神经元模型"><a href="#5-1-神经元模型" class="headerlink" title="5.1 神经元模型"></a>5.1 神经元模型</h2><p><strong>神经网络</strong>（neural networks）：是由具有适应性的简单单元组成的广泛并行互联的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。在机器学习中的神经网络是指，神经网络学习，是机器学习与神经网络学科领域的交叉部分。</p>
<p><strong>神经元模型</strong>（neuron model）：即上述定义中的简单单元。在一个神经元兴奋时，会传递神经递质至其他神经元，神经递质改变其他神经元内的电位，在某神经元电位差达到“<em>阈值</em>”（threshold）时，该神经元就会兴奋，继续传递神经递质。</p>
<p><strong>M-P神经元模型</strong>：神经元接受n个来自其他神经元的输入信号，这些输入信号通过带权重的<em>连接</em>（connection）进行传递，神经元收到的总输入值将与神经元的阈值进行比较，然后通过“<em>激活函数</em>”（activetion function）处理产生神经元的输出。</p>
<p>理想的激活函数是阶跃函数，但其具有不连续、不光滑等不太好的性质，因而常用Sigmoid函数作为激活函数，其又被称作“<em>挤压函数</em>”（squashing function）。其中，对数几率函数，即logistic函数是Sigmoid函数的代表。</p>
<p>连接多个这样的神经元即形成了神经网络。神经网络可以看作一个包含许多参数的数学模型，由若干个函数构成，例如$y_j = f(\sum_i \omega_i x_i - \theta_j)$相互嵌套而得。</p>
<h2 id="5-2-感知机与多层网络"><a href="#5-2-感知机与多层网络" class="headerlink" title="5.2 感知机与多层网络"></a>5.2 感知机与多层网络</h2><p><strong>感知机</strong>（Perceptron）：由两层神经元构成，输入层接收外界信号后传递给输出层，输出层是M-P神经元，亦称“<em>阈值逻辑单元</em>”（threshold logic unit）</p>
<p>感知机可容易地实现与、或、非运算。对于$y = f(\sum_i \omega_i  x_i - \theta _i)$，假定$f$为阶跃函数</p>
<ul>
<li><p><strong>与</strong> $(x_1 \wedge x_2)$：令$\omega_1 = \omega_2 = 1$，$\theta = 2$，则$y = f(1 \cdot x_1 + 1 \cdot x_2 - 2)$，仅在$x_1 = x_2 =1$时，$y = 1$；</p>
</li>
<li><p><strong>或</strong> $(x_1 \vee x_2)$：令$\omega_1 = \omega_2 = 1$，$\theta = 1$，则$y = f(1 \cdot x_1 + 1 \cdot x_2 - 0.5)$，当$x_1 = 1$或$x_2 = 1$时，$y = 1$；</p>
</li>
<li><p><strong>非</strong> $(\urcorner x_1)$：令$\omega_1 = 0.6$，$\omega_2 = 0$，$\theta = -0.5$，则$y = f(-0.6 \cdot x_1 + 0 \cdot x_2 + 0.5)$，当$x_1 = 1$时，$y = 0$；当$x_1 = 0$时，$y = 1$.</p>
</li>
</ul>
<p>更一般地，给定训练集，权重$\omega<em>i(i=1,2,\dots,n)$以及阈值$\theta$可以通过学习得到。阈值$\theta$可看作一个固定输入为-1.0的“<em>哑结点</em>”（dummy node）所对应的连接权重$\omega </em>{n+1}$，以此将权重和阈值的问题统一为学习权重的问题。感知机的学习过程如下：</p>
<p>对于训练样例$(x,y)$，若当前感知机输出为$\hat y$，则感知机权重调整方法为</p>
<script type="math/tex; mode=display">\omega_i \leftarrow \omega_i + \Delta \omega_i,</script><script type="math/tex; mode=display">\Delta \omega_i = \eta(y - \hat y) x_i.</script><p>其中，$\eta \in (0,1)$称为学习率（learning rate），可大致看作向错误修正方向的步长。（通常设置为小整数，如0.1）</p>
<p>需要注意的是，感知机仅有输出层神经元进行激活函数处理，即只有一层<em>功能神经元</em>（functional neuron），学习能力十分有限。事实上，感知机仅能处理类似“与”、“或”、“非”的<em>线性可分</em>（linearly separable）问题。即若问题是线性可分的，感知机的学习过程一定会<em>收敛</em>（converge），求得适当的权向量$\omega = (\omega<em>1;\omega_2;…;\omega</em>{n+1})$，形成线性超平面将其分开；而如果是非线性可分问题（即便是最简单的异或问题），感知机的学习过程将会发生<em>震荡</em>（fluctuation），$\omega$难以稳定下来，不能求得合适解。</p>
<p>要解决非线性可分问题，需考虑使用多层功能神经元。如利用两层感知机就能解决异或问题，在输入层与输出层之间的一层神经元，被称为隐层或<em>隐含层</em>（hidden layer），隐含层和输出层都是拥有激活函数的功能神经元。</p>
<p>更一般地，常见的神经网络是每层神经元与下一层神经元互连，神经元之间不存在同层连接，也不存在跨层连接，这样的神经网络结构被称为“<em>多层前馈神经网络</em>”（multi-layer feedforward neural networks）。其中，输入层神经元接收外界输入，隐层与输出层神经元对信号进行处理，并由输出层神经元输出最终结果。</p>
<h2 id="5-3-误差逆传播算法"><a href="#5-3-误差逆传播算法" class="headerlink" title="5.3 误差逆传播算法"></a>5.3 误差逆传播算法</h2><p>如果对多层网络进行训练，显然无法继续沿用单层感知机中的学习规则（权重调整法则），需要更加强大的算法。<em>误差逆传播</em>算法就是其中最杰出的代表，它也是迄今最成功的神经网络学习算法。它不仅可以用于多层前馈神经网络，还可用于其他类型的神经网络，例如训练递归神经网络，但通常的“BP网络”是指用BP算法训练的多层前馈神经网络。</p>
<p><strong>误差逆传播算法</strong>（error BackPropagation algorithm，简称BP）：对训练集$D={(x<em>1,y_1),(x_2,y_2),…,(x_m,y_m)}$，其中$x_i \in R^d$，$y_i \in R^l$分别表示d个输入属性和l维输出变量，假设该BP网络中有q层隐层神经元，输出层第j个神经元的阈值为$\theta _j$，隐层第h个神经元阈值为$\gamma_h$，输入层第i个神经元与隐层第h个神经元的连接权为$v</em>{ih}$，隐层第h个神经元接收到的输入为$\alpha<em>h = \sum^d</em>{i=1} v<em>{ih}x_i$，输出层第j个神经元接收到的输入为$\beta_j = \sum^q</em>{h=1}w_{hj}b_h$，其中$b_h$为隐层第h个神经元的输出。并且，所有激活函数都使用Sigmoid函数。则需要确定的参数有$(d+l+1)q + l$个，即</p>
<p><img src="https://i.loli.net/2019/09/03/SAMJx54b6tFmOcv.png" alt="参数数量"></p>
<p>对训练例$(x_k,y_k)$，假定神经网络的输出为$\hat y_k = (y^k_1,y^k_2,..,y^k_l)$，即</p>
<script type="math/tex; mode=display">\hat y^k_j = f(\beta_j - \theta_j)</script><p>对任意参数$a$的更新估计式为</p>
<script type="math/tex; mode=display">a \leftarrow a + \Delta a</script><p>以隐层到输出层的连接权$w_{hj}$为例进行推导。BP算法基于<em>梯度下降</em>（gradient descent）的方法以目标的负梯度方向对参数进行调整。对均方误差$E_k$和给定的学习率$\eta$得到</p>
<script type="math/tex; mode=display">\Delta w_{hj} = -\eta \frac {\partial E_k}{\partial w_{hj}}</script><p>针对上式进行分解变换</p>
<script type="math/tex; mode=display">\frac {\partial E_k}{\partial w_{hj}} = \frac{\partial E_k}{\partial \hat y^k_j} \cdot \frac{\partial \hat y^k_j}{\partial \beta_j} \cdot \frac{\partial \beta_j}{\partial w_{hj}}</script><p>注意到$\beta<em>j = \sum^q</em>{h=1} w_{hj}b_h$，则有</p>
<script type="math/tex; mode=display">\frac{\partial \beta_j}{\partial w_{hj}} = b_h</script><p>剩余部分则设$g_j$</p>
<script type="math/tex; mode=display">\begin{aligned} g_j &= -\frac{\partial E_k}{\partial \hat y^k_j} \cdot \frac{\partial \hat y^k_j}{\partial \beta_j} \\ &= -(\hat y^k_j - y^k_j)f'(\beta_j - \theta_j) \\ &= \hat y^k_j(1 - \hat y^k_j)(y^k_j - \hat y^k_j) \end{aligned}</script><p>（其中用到了Sigmoid函数的性质$f’(x) = f(x)(1-f(x))$）</p>
<p>将结果反代得到</p>
<script type="math/tex; mode=display">\Delta w_{hj} = \eta g_j b_h</script><p>同理可得</p>
<script type="math/tex; mode=display">\Delta \theta_j = -\eta g_j,</script><script type="math/tex; mode=display">\Delta v_{ih} = \eta e_h x_i,</script><script type="math/tex; mode=display">\Delta \gamma _h = - \eta e_h.</script><p>其中</p>
<script type="math/tex; mode=display">e_h = -\frac{\partial E_k}{\partial b_h} \cdot \frac{\partial b_h}{\partial \alpha _h} = b_h(1 - b_h) \sum ^l_{j=1} w_{hj}g_j</script><p>确定变量的更新方式之后，即可按照流程构造神经网络算法。（训练集$D = {(x<em>k,y_k)}^m</em>{k=1}$，学习率$\eta$）</p>
<figure class="highlight vb"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">输入：训练集D；学习率eta</span><br><span class="line">过程：</span><br><span class="line">在(<span class="number">0</span>,<span class="number">1</span>)范围内随机初始化所有连接权和阈值</span><br><span class="line">repeat</span><br><span class="line">    <span class="keyword">for</span> all (xk,yk) <span class="keyword">in</span> D <span class="keyword">do</span></span><br><span class="line">        根据当前参数，计算当前样本的输出\hat y_k；</span><br><span class="line">        计算输出层的输出神经元的梯度项g_j；</span><br><span class="line">        计算隐层的输出神经元的梯度项e_h；</span><br><span class="line">        更新连接权和阈值</span><br><span class="line"><span class="keyword">end</span> <span class="keyword">for</span></span><br><span class="line"><span class="keyword">until</span> 达到停止条件</span><br><span class="line">输出：连接权与阈值确定的多层前馈神经网络。</span><br></pre></td></tr></table></figure>
<p>值得注意的是，以上考虑的使用基于单个的$E_k$的更新法则，将目标替换为使每个$E_k$达到最优。因此，如果针对于累积误差优化</p>
<script type="math/tex; mode=display">E = \frac{1}{m} \sum^m_{k=1}E_k</script><p>就得到了<em>累积误差逆传播</em>（accumulated error backpropagation）算法。以下对比<em>标准BP算法</em>与<em>累积BP算法</em>的特点：</p>
<p><strong>标准BP算法</strong>：一般来说，每次更新都只针对单个样例，参数更新的比较频繁，并且可能会对不同的样本产生“抵消”现象，导致如果要达到同样的累计误差极小点，需要进行更多次数的迭代；在累计误差下降到一定程度之后，相较之下，进一步下降依旧可以较快地得到更好的解，在样本集$D$非常大时更明显。</p>
<p><strong>累积BP算法</strong>（accumulated error backpropagation）：累积BP算法直接针对累积五擦汗最小化，在读取整个数据集$D$一遍后才对参数进行更新，参数更新频率低得多，但在累积误差下降到一定程度时，进一步下降过程将会极为缓慢。</p>
<hr>
<p><strong>缓解BP算法的过拟合</strong>：</p>
<ul>
<li><em>早停</em>（early stopping） 在训练过程中，不断测试训练集和验证集的误差，如果训练集误差减小，但验证集误差增高，则停止训练；</li>
<li><em>正则化</em> （regularization） 基本思想是在误差目标函数中增加一个用于描述网络复杂度的部分。例如：<script type="math/tex">E = \lambda \frac{1}{m} \sum^m_{k=1} E_k + (1 - \lambda ) \sum_i \omega_i^2</script>其中$\lambda \in (0,1)$用于对经验误差和网络复杂度的折中，常通过交叉验证法来估计。（增加连接权与阈值的平方这一项之后，算法将对较小的连接权和阈值产生偏好，从而使输出网络更加平滑，达到缓解过拟合的作用）</li>
</ul>
<h2 id="5-4-全局最小与局部最小"><a href="#5-4-全局最小与局部最小" class="headerlink" title="5.4 全局最小与局部最小"></a>5.4 全局最小与局部最小</h2><p>通常地，对于一个目标函数，在它的梯度上不断优化时，可以找到它的一个极小值点。如果该目标函数仅有一个极小值点，则该点即为全局最小值点，否则是局部最小点。因此，在梯度下降算法使用时，容易掉入局部最小点。有以下几种避免方法：</p>
<ul>
<li>使用多组不同的参数值初始化多个神经网络，按标准方法训练后，取其中误差最小的解作为最终参数。相当于从不同的起点出发，大概率可以收敛于不同的局部极小点，通过选择即可得出全局最小。</li>
<li>使用“<em>模拟退火</em>”（simulated annealing）技术。模拟退火技术允许在一定程度上接受较差的结果，从而有利于跳出局部最小。在爹地啊过程中，接受“次优解”的概率随着时间推移而降低，以保证算法稳定。</li>
<li>使用“<em>随机梯度下降</em>”（stochastic gradient descent），在标准梯度下降算法的基础上增加随机因素，从而使其在陷入局部最小点时，仍可能继续迭代跳出局部最小。</li>
<li>使用“<em>遗传算法</em>”（genetic algorithms）。</li>
</ul>
<p>需要注意的是，以上的方法虽然可以在一定程度上跳出局部最小，但由于都是启发式算法，没有理论保障。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.zzforgood.top/2019/09/20/Chapter5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" data-id="ckvaxveuk000848ojfmz6citb" class="article-share-link">
        Share
      </a>
      
<ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B/" rel="tag">《机器学习》</a></li></ul>

    </footer>

  </div>

  
  
<nav class="article-nav">
  
  <a href="/2019/09/20/Chapter6%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" class="article-nav-link">
    <strong class="article-nav-caption">Newer</strong>
    <div class="article-nav-title">
      
      Chapter6 支持向量机
      
    </div>
  </a>
  
  
  <a href="/2019/09/20/Chapter4%E5%86%B3%E7%AD%96%E6%A0%91/" class="article-nav-link">
    <strong class="article-nav-caption">Older</strong>
    <div class="article-nav-title">Chapter4 决策树</div>
  </a>
  
</nav>

  

  
  
  
<div class="gitalk" id="gitalk-container"></div>

<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">


<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>


<script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>

<script type="text/javascript">
  var gitalk = new Gitalk({
    clientID: 'b9cfa880780f6dc246f0',
    clientSecret: 'eece6422d853f25460f68edcaa0506ce9f0b30a8',
    repo: 'lyzsj114',
    owner: 'lyzsj114',
    admin: ['lyzsj114'],
    // id: location.pathname,      // Ensure uniqueness and length less than 50
    id: md5(location.pathname),
    distractionFreeMode: false,  // Facebook-like distraction free mode
    pagerDirection: 'last'
  })

  gitalk.render('gitalk-container')
</script>

  

</article>
</section>
    <footer class="footer">
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
  <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span></li>
  
  <li><i class="fe fe-bookmark"></i> <span id="busuanzi_value_page_pv"></span></li>
  
</ul>
    </div>
    <ul class="list-inline">
      <li>GeniusGrass&#39;s Blog &copy; 2021</li>
      
        <li>ZHWANGART</li>
      
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
      <li>theme  <a target="_blank" rel="noopener" href="https://github.com/zhwangart/hexo-theme-ocean">Ocean</a></li>
    </ul>
  </div>
</footer>
  </main>
  <aside class="sidebar">
    <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/hexo.svg" alt="GeniusGrass&#39;s Blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">Home</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">Archives</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/gallery">Gallery</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">About</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="Search">
        <i class="fe fe-search"></i>
        Search
      </a>
    </li>
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
  <i class="fe fe-rocket"></i>
</div>
    </li>
    <li class="nav-item">
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="fe fe-feed"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
  </aside>
  
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>



<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/copybtn.js"></script>





<script src="/js/tocbot.min.js"></script>

<script>
  // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto',
  });
</script>



<script src="/js/ocean.js"></script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

</html>