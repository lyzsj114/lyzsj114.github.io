<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  <title>
    Chapter7 贝叶斯分类器 |
    
    GeniusGrass&#39;s Blog
  </title>
  
    <link rel="shortcut icon" href="/favicon.ico">
    
  
<link rel="stylesheet" href="/css/style.css">

  
  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<script src="/js/pace.min.js"></script>

<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <main class="content">
    <section class="outer">
  <article id="post-Chapter7贝叶斯分类器" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
      

<h1 class="article-title" itemprop="name">
  Chapter7 贝叶斯分类器
</h1>



    </header>
    

    
    <div class="article-meta">
      <a href="/2019/09/20/Chapter7%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" class="article-date">
  <time datetime="2019-09-20T04:39:19.000Z" itemprop="datePublished">2019-09-20</time>
</a>
      
    </div>
    

    
    
<div class="tocbot"></div>

    

    <div class="article-entry" itemprop="articleBody">
      
      
      
      <p>本文内容：</p>
<ul>
<li>贝叶斯决策论</li>
<li>极大似然估计</li>
<li>朴素贝叶斯分类器</li>
<li>半朴素贝叶斯分类器</li>
<li>贝叶斯网<ul>
<li>结构</li>
<li>学习</li>
<li>推断</li>
</ul>
</li>
<li>EM算法</li>
<li>附录<ul>
<li>参数估计</li>
</ul>
</li>
</ul>
<span id="more"></span>
<h2 id="7-1-贝叶斯决策论"><a href="#7-1-贝叶斯决策论" class="headerlink" title="7.1 贝叶斯决策论"></a>7.1 贝叶斯决策论</h2><p><strong>贝叶斯决策论</strong>（Bayesian decision theory）：是在概率框架下实施决策的基本方法。对于分类任务来说，在所有相关概率都已知的理想情况下，贝叶斯决策论考虑如何基于这些概率核误判损失来选择最优的类别标记。</p>
<p>以多分类任务为例：</p>
<p>假设有$N$种可能的类别标记，即$\gamma={c<em>1,c_2,\dots,c_N}$，$\lambda</em>{ij}$是将一个真实的$c_j$样本误分类为$c_j$所产生的损失，基于后验概率$P(c_i|x)$可获得将样本$x$分类为$c_i$所产生的<em>期望损失</em>（expected loss），即在样本$x$上的“<em>条件风险</em>”（conditional risk）</p>
<script type="math/tex; mode=display">R(c_i|x) = \sum^N_{j=1}\lambda_{ij}P(c_i|x)</script><p>目标：找到一个最小化总体风险的判定准则：$\chi \mapsto \gamma$</p>
<script type="math/tex; mode=display">R(h) = \mathbb{E}_x[R(h(x)|x)]</script><p>由此，即可得到<em>贝叶斯判定准则</em>（Bayes decision rule）：为最小化总体风险，只需在每个样本上选择能使条件风险$R(c|x)$最小的类别标记，即</p>
<script type="math/tex; mode=display">h^*(x) = \argmin_{c \in \gamma}R(c|x)</script><p>此时，$h^<em> $称为</em>贝叶斯最优分类器<em>（Bayes optimal classifier），对应的总体风险$R(h^</em> )$称为<em>贝叶斯风险</em>（Bayes risk）。则该模型精度的理论上限为$1-R(h^*)$。误判概率$\lambda_{ij}$为</p>
<script type="math/tex; mode=display">\lambda_{ij}=\begin{cases} 0, &\text{ if } i=j; \\ 1, & \text{ otherwise.}\end{cases}</script><p>于是，条件风险为</p>
<script type="math/tex; mode=display">R(c|x) = 1-P(c|x)</script><p>最小化分类错误的贝叶斯最优分类器为</p>
<script type="math/tex; mode=display">h^*(x) = \argmax_{c \in \gamma} P(c|x)</script><p>即对每个样本$x$，选择能使后验概率$P(c|x)$最大的类别标记。</p>
<p>由此可知，要想通过贝叶斯判定准则得到最小化风险的贝叶斯分类器，需要求得其后验概率$P(c|x)$。所以，此时机器学习的目标在于利用有限的样本集尽可能准确地估计出后验概率$P(c|x)$。对后验概率的估计主要分为两种策略：</p>
<ul>
<li><p><strong>判别式模型</strong>（discriminative models）：给定$x$，可直接建模$P(c|x)$来预测。例：决策树、BP神经网络、支持向量机等；</p>
</li>
<li><p><strong>生成式模型</strong>（generative models）：先对联合概率分布$P(x,c)$建模，然后再由此获得$P(c|x)$。</p>
</li>
</ul>
<p>基于贝叶斯定理，$P(c|x)$可写为</p>
<script type="math/tex; mode=display">P(c|x)= \frac{P(c)P(x|c)}{P(x)}</script><p>其中，$P(c)$是类“<em>先验</em>”（prior）概率；$P(x|c)$是样本$x$相对于类标记$c$的<em>类条件概率</em>（class-conditional probablity），或称为“<em>似然</em>”（likelihood）；$P(x)$是用于归一化的“<em>证据</em>”（evidence）因子。显然，$P(x)$与类标记无关，因此估计$P(c|x)$的问题即转化为基于训练数据$D$估计先验概率$P(c)$和似然$P(x|c)$。</p>
<p>类先验概率$P(c)$直接通过大数定律，用各类样本出现的频率进行估计。</p>
<p>而类条件概率$P(x|c)$，由于其涉及关于$x$全部属性的联合概率，直接根据样本出现的频率来估计将会遇到严重的困难。例如，假设样本有$d$个属性且都为二值，则样本空间将有$2^d$种可能的取值。一般地，样本数$m&lt;2^d$，这就造成很多样本的取值在训练集中未能出现，导致将其概率直接判为0，而事实上通常来说“未被观测到” $\neq$ “出现概率为零”。</p>
<h2 id="7-2-极大似然估计"><a href="#7-2-极大似然估计" class="headerlink" title="7.2 极大似然估计"></a>7.2 极大似然估计</h2><p>估计类的条件概率的一种常用策略为：先假定样本具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。记类别$c$的类条件概率为$P(x|c)$，且$P(x|c)$具有确定的形式被参数向量$\theta_c$唯一确定。因此我们的目标是利用数据集$D$估计$\theta_c$，而$P(x|c)$可记为$P(x|\theta_c)$。</p>
<p>事实上，概率模型的训练过程就是<em>参数估计</em>（parameter estimation）过程。此节中介绍的<em>极大似然估计</em>（Maximum Likelihood Estimation，简称MLE），是频率主义学派$^{[1]}$根据数据采样估计概率分布参数的经典方法。</p>
<p>令$D_c$表示训练集$D$中第$c$类样本组成的集合，假设其为独立同分布，则参数$\theta_c$对于数据集$D_c$的似然是</p>
<script type="math/tex; mode=display">P(D_c|\theta_c) = \prod_{x \in D_c}P(x|\theta_c)</script><p>对$\theta$进行极大似然估计，即寻找能最大化似然$P(D_c|\theta_c)$的参数值$\hat \theta_c$。也就是说，寻找使训练集数据出现概率最大的参数值$\hat \theta_c$。</p>
<p>同时，为避免上式中的连乘操作出现溢出，通常使用<em>对数似然</em>（log-likelihood）</p>
<script type="math/tex; mode=display">LL(\theta_c) = \sum_{x \in D_c} \log P(x|\theta_c)</script><p>此时参数$\theta_c$的极大似然估计$\hat \theta_c$为</p>
<script type="math/tex; mode=display">\hat \theta_c = \argmax_{\theta_c} LL(\theta_c)</script><p>需要注意的是，这种参数化的方法虽然能够使类条件概率估计变得相对简单，但其估计结果严重依赖于假设的总体分布形式是否符合潜在的概率分布形式。</p>
<h2 id="7-3-朴素贝叶斯分类器"><a href="#7-3-朴素贝叶斯分类器" class="headerlink" title="7.3 朴素贝叶斯分类器"></a>7.3 朴素贝叶斯分类器</h2><p>在<strong>7.1 贝叶斯决策论</strong>中发现，基于贝叶斯公式估计后验概率$P(c|x)$的主要困难在于：$P(c|x)$是所有属性上的联合概率，仅使用有限的训练样本集难以直接估计得到。而<em>朴素贝叶斯分类器</em>（naive Bayes classifier）采用“<em>属性条件独立性假设</em>”来避开计算所有属性上的联合概率。</p>
<p><strong>属性独立性假设</strong>（attribute conditional independence assumption）：每个属性独立地对分类结果发生影响。在此假设下，可以将原贝叶斯模型重写为</p>
<script type="math/tex; mode=display">P(c|x) = \frac{P(c)}{P(x)}\prod^d_{i=1} P(x_i|c)</script><p>于是根据属性独立性假设，构造朴素贝叶斯分类器</p>
<script type="math/tex; mode=display">h_{nb}(x) = \argmax_{c \in \gamma} P(c) \prod^d_{i=1} P(x_i|c)</script><p>令$D_c$表示训练集$D$中第$c$类样本组成的集合，其中</p>
<script type="math/tex; mode=display">P(c) = \frac{|D_c|}{|D|}</script><ul>
<li>对离散属性而言，令$D<em>{c,x_i}$表示$D_c$中在第$i$个属性上取值为$x_i$的样本组成的集合，则有 $$P(x_i|c) = \frac{|D</em>{c,x_i}|}{|D_c|}$$</li>
<li>对连续属性而言，可用概率密度函数代替$P(x<em>i|c)$，假定$p(x_i|c) \sim \Nu(\mu</em>{c,i}, \sigma^2_{c,i})$</li>
</ul>
<p>值得注意的是，只有在样本量足够多时，才能作出有意义的估计。</p>
<p>在试验中可以发现，可能会出现某些未出现过的属性值组合，其概率将会被直接判别为零，显然这是个不合理的现象。为了避免其他属性携带的信息直接被未出现过的属性“抹去”，需要在估计概率值时进行“<em>平滑</em>”（smoothing）处理，而“<em>拉普拉斯修正</em>”是比较常用的方法之一。</p>
<p><strong>拉普拉斯修正</strong>（Laplacian correction）：令$N$表示训练集$D$中可能的类别数，$N_i$表示第$i$个属性可能的取值数，则$P(c)$和$P(x_i|c)$分别修正为</p>
<script type="math/tex; mode=display">\hat P(c) = \frac{|D_c|+1}{|D|+N},</script><script type="math/tex; mode=display">\hat P(x_i|c) = \frac{|D_{c,x_i}|+1}{|D_c|+N_i}.</script><p>在较大的训练集中，拉普拉斯修正所造成的偏差将变得可忽略，使估计值更加接近实际值。</p>
<p>在现实使用中，朴素贝叶斯分类器有多种使用方式：</p>
<ul>
<li>任务对预测速度有要求，利用给定的训练集，事先计算并储存好要使用到的概率估计，在使用时以查表的方式调用即可；</li>
<li>任务数据更替频繁，采用“<em>懒惰学习</em>”（lazy learning）方式，即在预测时再对模型进行训练；</li>
<li>任务数据不断增加，采用增量学习的方式，即在现有估值的基础上，仅对新增样本涉及的属性进行概率值的计数修正。</li>
</ul>
<h2 id="7-4-半朴素贝叶斯分类器"><a href="#7-4-半朴素贝叶斯分类器" class="headerlink" title="7.4 半朴素贝叶斯分类器"></a>7.4 半朴素贝叶斯分类器</h2><p>在<strong>7.3 朴素贝叶斯分类器</strong>一节中提出，使用属性独立性假设避免计算复杂的联合概率密度，但由于属性独立性假设在现实中很难成立，使用时将会损失大量信息。于是，提出了折中的“<em>半朴素贝叶斯分类器</em>”学习方法。</p>
<p><strong>半朴素贝叶斯分类器</strong>（semi-naive Bayes classifier）：基本思想在于，对考虑完全的联合概率密度和属性独立之间进行折中，即考虑一部分属性间的相互依赖。“<em>独依赖估计</em>”（One-Dependent Estimator，简称ODE）是半朴素贝叶斯分类器最常用的一种策略，即假设每个属性在类别之外最多仅依赖于一个其他属性</p>
<script type="math/tex; mode=display">P(c|x) \propto P(c)\prod^d_{i=1}P(x_i|c,pa_i)</script><p>其中，$pa_i$为属性$x_i$所依赖的属性，称为$x_i$的父属性。针对某一属性，确定父属性之后即可确定该属性的估计概率值$P(x_i|c,pa_i)$。而对父属性的选取有多种方法，不同的方法将产生不同的独依赖分类器。</p>
<p><img src="https://i.loli.net/2019/09/09/Le2QEzWRKsHxYdo.png" alt="朴素贝叶斯与两种半朴素的比较"></p>
<p><strong>SPODE方法</strong>（Super-Parent ODE）：最直接的方法。直接假设所有属性都依赖于同一个属性，称作“<em>超父</em>”（super-parent），然后通过交叉验证等模型选择方法确定超父属性。</p>
<p><strong>TAN方法</strong>（Tree Argumented naive Bayes）：在<em>最大带权生成树</em>（maximum weighted spanning tree）算法的基础上，通过下列步骤将属性间的依赖关系约简为树形结构：</p>
<ol>
<li>计算任意两个属性之间的条件互信息（conditional mutual information）<script type="math/tex">I(x_i,x_j|y) = \sum_{x_i,x_j;c \in \gamma}P(x_i,x_j|c) \log \frac{P(x_i,x_j|c)}{P(x_i|c)P(x_j|c)};</script></li>
<li>以属性为结点构建完全图，任意两结点间的权重设置为$I(x_i,x_j|y)$；</li>
<li>构建此完全图的最大带权生成树，挑选根变量，将边设置为有向；</li>
<li>加入类别结点$y$，增加从$y$到每个属性的有向边。</li>
</ol>
<p>其中，条件互信息$I(x_i,x_j|y)$刻画了属性$x_i$和$x_j$在已知类别情况下的相关性，因此，通过最大生成树算法，TAN实际上仅保留了强相关属性之间的依赖性。</p>
<p><strong>AODE</strong>（Average One-Dependence Estimator）：一种基于集成学习机制、更为强大的独依赖分类器，尝试将每个属性作为超父来构建SPODE，然后将那些具有足够训练数据支撑的SPODE集成起来作为最终结果，即</p>
<script type="math/tex; mode=display">P(c|x) \propto \sum^d_{i=1,|D_{x_i}| \geqslant m'} P(c,x_i) \prod^d_{j=1} P(x_j|c,x_i)</script><p>其中，$D_{x_i}$是在第$i$个属性上取值为$x_i$的样本的集合，$m’$为阈值常数。显然，AODE需估计$P(c,x_i)$和$P(x_j|c,x_i)$，则有</p>
<script type="math/tex; mode=display">\hat P(c,x_i) = \frac{|D_{c,x_i}|+1}{|D|+ N \times N_i}</script><script type="math/tex; mode=display">\hat P(x_j|c,x_i) = \frac{|D_{c,x_i,x_j}|+1}{|D_{c,x_i}|+N_j}</script><p>其中，$N$是$D$中可能的类别数，$N<em>i$是第$i$个属性可能的取值数，$D</em>{c,x<em>i}$是类别为$c$且在第$i$个属性上取值为$x_i$的样本集合，$D</em>{c,x_i,x_j}$是类别为c且在第$i$和第$j$个属性取值分别为$x_i$和$x_j$的样本集合。</p>
<p>同理，还可将独依赖进一步放松到多个属性依赖的“高阶依赖”，即将$pa_i$替换为同时包含多个属性的$pa_i$向量，从而将ODE拓展为kDE。而随着k的增加，准确估计概率$P(x_i|y,pa_i)$所需的训练样本将以指数级增加。</p>
<h2 id="7-5-贝叶斯网"><a href="#7-5-贝叶斯网" class="headerlink" title="7.5 贝叶斯网"></a>7.5 贝叶斯网</h2><p>贝叶斯网（Bayesian network）又称<em>信念网</em>（belief network），借助<em>有向无环图</em>（Directed Acyclic Graph，简称DAG）来刻画属性之间的依赖关系，并使用<em>条件概率表</em>（Conditional Probability Table，简称CPT）描述属性的联合概率分布。</p>
<p>贝叶斯网$B$由两部分构成：结构$G$和参数$\Theta$，即有$B = \left \langle G,\Theta \right \rangle$。网格结构$G$是一个有向无环图，其每个结点对应一个属性，连线表示二者之间具有直接依赖关系；参数$\Theta$为依赖关系的量化，对应依赖的权重。假设属性$x_i$在$G$中的结点为$\pi_i$，则$\Theta$包含了每个属性的条件概率表</p>
<script type="math/tex; mode=display">\theta_{x_i|\pi_i} = P_B(x_i|\pi_i)</script><h3 id="7-5-1-结构"><a href="#7-5-1-结构" class="headerlink" title="7.5.1 结构"></a>7.5.1 结构</h3><p>贝叶斯网的结构有效地表达了属性间的独立性。给定父结点集，贝叶斯网假设每个属性与它的非后裔属性独立，则对$B = \left \langle G,\Theta \right \rangle$，属性$x_1,x_2,\dots,x_d$的联合概率密度为</p>
<script type="math/tex; mode=display">P_B(x_1,x_2,\dots,x_d) = \prod^d_{i=1}P(x_i|\pi_i) = \prod^d_{i=1}\theta_{x_i|pi_i}</script><p>其中，$x_2$和$x_3$在$x_1$的值确定时相互独立，被称为条件独立性，记作$x_2 \perp x_3 | x_1$。此外存在<em>边际独立性</em>（marginal independence）的情况，例如在V型结构中，$x_4$的值未知时，$x_1$和$x_2$相互独立；在$x_4$的值已知时，$x_1$和$x_2$必不独立，这种情况记作$x_1 \rVert x_2$。下为验证：</p>
<script type="math/tex; mode=display">\begin{aligned} P(x_1,x_2) &= \sum_{x_4}P(x_1,x_2,x_4) \\ &= \sum_{x_4}P(x_4|x_1,x_2)P(x_1)P(x_2) \\ &= P(x_1)P(x_2)\end{aligned}</script><p>在贝叶斯网中有三种典型的依赖关系：同父结构、V型结构、顺序结构。</p>
<p><img src="https://i.loli.net/2019/09/10/H7msyJlokT64UxQ.jpg" alt="贝叶斯网典型依赖关系"></p>
<p>常用“<em>有向分离</em>”（D-seperation），对有向图中的依赖关系进行分析：</p>
<ul>
<li>找出所有V型结构，在V型结构的两个父结点间加一条无向边；</li>
<li>将有向边转化为无向边.</li>
</ul>
<p>由此产生“<em>道德图</em>”（moral graph），连接父结点的过程被称为<em>道德化</em>（moralization）。</p>
<p>假定道德图中存在变量$x$，$y$和变量集合$z={z_i}$，如果$x$，$y$在$z$被去除后，分属两个连通的分支，则可说明$x \perp y|z$。需要注意的是，在划分前需要对图剪枝，仅保留有向图中$x$，$y$，$z$及它们的父结点。</p>
<h3 id="7-5-2-学习"><a href="#7-5-2-学习" class="headerlink" title="7.5.2 学习"></a>7.5.2 学习</h3><p>贝叶斯网的学习首要目标即寻找结构最恰当的贝叶斯网。常使用“<em>评分搜索</em>”实现：</p>
<p>首先，定义<em>评分函数</em>（score function），以评估贝叶斯网与训练数据的契合程度。评分函数中包含了对希望构造的贝叶斯网的偏好。</p>
<p>常用评分函数一般基于信息论准则，此类准则将学习问题看作数据压缩的过程，所以最优的贝叶斯网结构应对应最小的综合编码长度，这种方法被称为“<em>最小描述长度</em>”（Minimum Description Length，简称MDL）准则。</p>
<p>给定训练集$D={x_1,x_2,\dots,x_m}$，贝叶斯网$B = \left \langle G,\Theta \right \rangle$在$D$上的评分函数可写为</p>
<script type="math/tex; mode=display">S(B|D) = f(\theta)|B| - LL(B|D)</script><p>其中，$B$是贝叶斯网的参数个数；$f(\theta)$表示描述每个参数$\theta$所需的编码位数；而</p>
<script type="math/tex; mode=display">LL(B|D) = \sum^m_{i=1} \log P_B(x_i)</script><p>是贝叶斯网$B$的对数似然。其评分函数第一项是计算编码贝叶斯网$B$所需的编码位数，第二项是计算$B$所对应的概率分布$P_B$对$D$的描述有多好。从统计学角度理解，一项为结构风险，另一项为经验风险。</p>
<p><strong>AIC评分函数</strong>（Akaike Information Criterion score function）：使$f(\theta) = 1$，即可得到AIC评分函数</p>
<script type="math/tex; mode=display">\text{AIC}(B|D) = |B| - LL(B|D)</script><p><strong>BIC评分函数</strong>（Beyesian Information Criterion score function）：使$f(\theta) = \frac{1}{2}\log m$，即可得到BIC评分函数</p>
<script type="math/tex; mode=display">\text{BIC}(B|D) = \frac{\log m}{2}|B| - LL(B|D)</script><p>特殊地，使$f(\theta) = 0$时，评分函数退化为负对数似然，同时，学习任务退化为极大似然估计。</p>
<p>最后，只要确定网络结构$G$，评分函数第一项为常数，即将最小化$s(B|D)$转化为对参数$\Theta$的极大似然估计。而参数$\Theta$可以通过训练集数据的经验估计获得</p>
<script type="math/tex; mode=display">\theta_{x_i|\pi_i} = P(x_i|\pi_i)</script><p>因此，仅需对结构网络进行搜索，即可在训练集上计算得到最优参数。但是，在所有可能的结构中搜索最优贝叶斯网结构是NP难问题，所以需要特殊方法求近似解：</p>
<ul>
<li>贪心算法，从某个网络结构出发，每次调整一条边，直到评分函数值不再降低；</li>
<li>给网络结构增加约束来削减搜索空间，如TAN中限制为树结构。</li>
</ul>
<h3 id="7-5-3-推断"><a href="#7-5-3-推断" class="headerlink" title="7.5.3 推断"></a>7.5.3 推断</h3><p>在训练好贝叶斯网模型之后，就能用来回答“<em>查询</em>”（query），即通过一些属性变量的观测值来推测其他属性变量的取值。而通过已知变量的观测值来推测待查询变量的过程被称为“<em>推断</em>”（inference），已知变量观测值称为“<em>证据</em>”（evidence）。</p>
<p>在理想状态下，可以直接根据贝叶斯网定义的联合概率分布精确计算其后验概率，但这样的“精确推断”是NP难的。因此需要作出“近似推断”，在实际应用中，常用<em>吉布斯采样</em>（Gibbs sampling）来完成（另常用<em>变分推断</em>）。</p>
<p><strong>吉布斯采样</strong>（Gibbs sampling）：一种随机抽样方法。</p>
<p>有待查询变量$Q={Q_1,Q_2,\dots,Q_n}$，证据变量$E={E_1,E_2,\dots,E_n}$取值为$e={e_1,e_2,\dots,e_n}$。目标是计算后验概率$P(Q=q|E=e)$，其中$q={q_1,q_2,\dots,q_n}$。</p>
<p>吉布斯算法首先产生一个与证据$E=e$一致的样本$q^0$作为初始点，然后每步从当前样本出发产生下一个样本。即先设$q^t=q^{t-1}$，然后逐个改变其中的非证据变量并抽样取值，采样概率根据贝叶斯网和其他变量的当前取值（即$Z=z$）计算获得。经过$T$次采样得到的与$q$一致的样本共有$n_q$个，则可近似估算出后验概率：</p>
<script type="math/tex; mode=display">P(Q=q|E=e) \simeq \frac{n_q}{T}</script><p>实质上，可以看出，在吉布斯采样中，每一步都仅依赖于前一步的状态，即符合<em>马尔科夫链</em>（Markov chain）模型，其在贝叶斯网所有变量的联合状态空间与证据$E=e$一致的子空间中。在一定条件下，无论从什么状态开始，马尔可夫链在第$t$步的状态分布在$t \to \infin$时必收敛于一个<em>平稳分布</em>（stationary distribution）；对于吉布斯采样来说，这个分布恰好是$P(Q|E=e)$。因此，在$T$很大时，吉布斯采样相当于根据$P(Q|E=e)$采样，从而保证了上式收敛于$P(Q=q|E=e)$。以下为具体流程：</p>
<figure class="highlight vb"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">输入：贝叶斯网B=&lt;G,Theta&gt;；</span><br><span class="line">      采样次数T；</span><br><span class="line">      证据变量E及其取值e；</span><br><span class="line">      待查询变量Q及其取值q；</span><br><span class="line">过程：</span><br><span class="line">nq = <span class="number">0</span></span><br><span class="line">q0 = 对Q随机赋值</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span>,<span class="number">2</span>,...,T <span class="keyword">do</span></span><br><span class="line">    <span class="keyword">for</span> Qi <span class="keyword">in</span> Q <span class="keyword">do</span></span><br><span class="line">        Z = E U Q \ &#123;Qi&#125;</span><br><span class="line">        z = e U q(t-<span class="number">1</span>) \ &#123;qi(t-<span class="number">1</span>)&#125;</span><br><span class="line">        根据B计算分布PB(Qi|Z=z)</span><br><span class="line">        qi(t) = 根据PB(Qi|Z=z)采样所获Qi取值</span><br><span class="line">        q(t) = 将q(t-<span class="number">1</span>)中的qi(t-<span class="number">1</span>)用qi(t)替换</span><br><span class="line">    <span class="keyword">end</span> <span class="keyword">for</span></span><br><span class="line">    <span class="keyword">if</span> qt = q <span class="keyword">then</span></span><br><span class="line">        nq = nq + <span class="number">1</span></span><br><span class="line">    <span class="keyword">end</span> <span class="keyword">if</span></span><br><span class="line"><span class="keyword">end</span> <span class="keyword">for</span></span><br><span class="line">输出：P(Q=q|E=e) ≈ nq/T</span><br></pre></td></tr></table></figure>
<p>需要注意的是，马尔可夫链趋于平稳分布的过程很慢，导致吉布斯采样算法的收敛速度较慢。此外，如果贝叶斯网中存在极端概率，即0或1时，不能保证马尔可夫链一定能收敛，会导致吉布斯采样给出错误的估计。</p>
<h2 id="7-6-EM算法"><a href="#7-6-EM算法" class="headerlink" title="7.6 EM算法"></a>7.6 EM算法</h2><p>为解决在现实中需要使用未观测到的数据的训练样本，对模型的参数进行估计的问题，引入“<em>隐变量</em>”（latent variable），即未观察的变量的概念。</p>
<p>在已观测变量集$X$，隐变量集$Z$，模型参数$\Theta$的假设下，对$\Theta$进行极大似然估计，函数</p>
<script type="math/tex; mode=display">LL(\Theta |X,Z)=\ln P(X,Z|\Theta )</script><p>显然，由于$Z$是隐变量，上式无法求解。此时可以考虑通过计算$Z$的期望，来最大化已观察数据的对数“<em>边际似然</em>”（marginal likelihood）</p>
<script type="math/tex; mode=display">LL(\Theta,X) = \ln P(X|\Theta) = \ln \sum_Z P(X,Z|\Theta)</script><p>对参数隐变量进行估计常用的方法是EM（Expectation-Maximization）算法，它是一种迭代的方法。</p>
<p>EM算法原型：</p>
<p>以初始值$\Theta^0$为起点，对边际似然函数，可迭代执行以下步骤直至收敛：</p>
<ul>
<li>基于$\Theta^t$推断隐变量$Z$的期望，记为$Z^t$；</li>
<li>基于已观测变量$X$和$Z^t$对参数$\Theta$做极大似然估计，记为$\Theta ^{t+1}$.</li>
</ul>
<p>进一步，若我们不是取$Z$的期望，而是基于$\Theta^t$计算隐变量$Z$的概率分布$P(Z|X,\Theta^t)$，则EM算法的两个步骤是：</p>
<ul>
<li>E步（Expectation）：以当前参数$\Theta^t$推断隐变量分布$P(Z|X,\Theta^t)$，并计算对数似然$LL(\Theta|X,Z)$关于$Z$的期望；</li>
<li>M步（Maximization）：寻找最大化期望似然，即<script type="math/tex">\Theta^{t+1} = \argmax_\Theta Q(\Theta|\Theta^t).</script></li>
</ul>
<p>直观而言，最终EM算法将收敛到局部最优解，因此也可用梯度下降等优化算法对隐变量估计问题求解，但由于求和的项数将随着隐变量的数目以指数级上升，会给梯度计算带来麻烦；而EM算法则可以看作一种非梯度优化方法。</p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="1-参数估计"><a href="#1-参数估计" class="headerlink" title="[1] 参数估计"></a>[1] 参数估计</h3><p>统计学界的两个学派对参数估计提供不同的方案：</p>
<ul>
<li>频率主义学派：参数虽然未知，但却是客观存在的固定值，因此，可以通过优化似然函数等准则确定参数值；</li>
<li>贝叶斯学派：参数是未观察到的随机变量，其本身也存在分布，因此，可以通过假定参数服从一个先验分布，然后基于训练集计算参数的后验分布。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.zzforgood.top/2019/09/20/Chapter7%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" data-id="ckvaxveuo000d48ojc3eb25w3" class="article-share-link">
        Share
      </a>
      
<ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B/" rel="tag">《机器学习》</a></li></ul>

    </footer>

  </div>

  
  
<nav class="article-nav">
  
  <a href="/2019/10/02/Chapter8%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" class="article-nav-link">
    <strong class="article-nav-caption">Newer</strong>
    <div class="article-nav-title">
      
      Chapter8 集成学习
      
    </div>
  </a>
  
  
  <a href="/2019/09/20/Chapter6%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" class="article-nav-link">
    <strong class="article-nav-caption">Older</strong>
    <div class="article-nav-title">Chapter6 支持向量机</div>
  </a>
  
</nav>

  

  
  
  
  

</article>
</section>
    <footer class="footer">
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
  <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span></li>
  
  <li><i class="fe fe-bookmark"></i> <span id="busuanzi_value_page_pv"></span></li>
  
</ul>
    </div>
    <ul class="list-inline">
      <li>GeniusGrass&#39;s Blog &copy; 2021</li>
      
        <li>ZHWANGART</li>
      
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
      <li>theme  <a target="_blank" rel="noopener" href="https://github.com/zhwangart/hexo-theme-ocean">Ocean</a></li>
    </ul>
  </div>
</footer>
  </main>
  <aside class="sidebar">
    <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/hexo.svg" alt="GeniusGrass&#39;s Blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">Home</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">Archives</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/gallery">Gallery</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">About</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="Search">
        <i class="fe fe-search"></i>
        Search
      </a>
    </li>
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
  <i class="fe fe-rocket"></i>
</div>
    </li>
    <li class="nav-item">
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="fe fe-feed"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
  </aside>
  
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>



<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/copybtn.js"></script>





<script src="/js/tocbot.min.js"></script>

<script>
  // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto',
  });
</script>



<script src="/js/ocean.js"></script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

</html>