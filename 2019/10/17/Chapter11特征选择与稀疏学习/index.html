<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  <title>
    Chapter11 特征选择与稀疏学习 |
    
    GeniusGrass&#39;s Blog
  </title>
  
    <link rel="shortcut icon" href="/favicon.ico">
    
  
<link rel="stylesheet" href="/css/style.css">

  
  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<script src="/js/pace.min.js"></script>

<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <main class="content">
    <section class="outer">
  <article id="post-Chapter11特征选择与稀疏学习" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
      

<h1 class="article-title" itemprop="name">
  Chapter11 特征选择与稀疏学习
</h1>



    </header>
    

    
    <div class="article-meta">
      <a href="/2019/10/17/Chapter11%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" class="article-date">
  <time datetime="2019-10-17T11:13:25.000Z" itemprop="datePublished">2019-10-17</time>
</a>
      
    </div>
    

    
    
<div class="tocbot"></div>

    

    <div class="article-entry" itemprop="articleBody">
      
      
      
      <p>本文内容：</p>
<ul>
<li>子集搜索与评价<ul>
<li>子集搜索</li>
<li>子集评价</li>
</ul>
</li>
<li>过滤式选择</li>
<li>包裹式选择</li>
<li>嵌入式选择与正则化</li>
<li>稀疏表示与字典学习</li>
<li>压缩感知<ul>
<li>限定等距性</li>
</ul>
</li>
<li>附录<ul>
<li>L-Lipschitz条件</li>
<li>$L_0$范数</li>
</ul>
</li>
</ul>
<span id="more"></span>
<h2 id="11-1-子集搜索与评价"><a href="#11-1-子集搜索与评价" class="headerlink" title="11.1 子集搜索与评价"></a>11.1 子集搜索与评价</h2><p>对一个学习任务而言，给定数据集，其中包含一种或多种属性，我们将属性称为“<em>特征</em>”（feature），对当前学习任务有用的属性称为“<em>相关特征</em>”（relevant feature）、无用的称为“<em>无关特征</em>”（irrelevant feature）。从给定的特征集合中选择出相关特征子集的过程，称为“<em>特征选择</em>”（feature selection）</p>
<p>特征选择的作用可大概分为两方面：一方面，选择出重要特征，可以在一定程度上避免维数灾难；另一方面，去除不相关因素可以降低学习任务的难度。因此，特征选择是一个重要的<em>数据预处理</em>，通常在获得数据之后，训练学习器之前进行。</p>
<p>但需要注意的是，特征选择过程必须确保不丢失重要特征，否则后续学习过程会因为缺失重要信息而无法获得好的性能。对给定的数据集，如果学习任务不同，相关特征也就不尽相同，所以，所以谓的“相关特征”和“无关特征”的概念都是相对的。</p>
<p>此外，有一类特征称为“<em>冗余特征</em>”（redundant feature），它们所包含的信息能从其他特征中推演得到，即具有完全相关的特征。一般情况下，去除冗余特征不会对结果产生影响，还可以减轻学习过程的负担；但在冗余属性是结果的“中间概念”时，则会降低学习任务的难度。为简化讨论，本文假定数据中不涉及冗余特征，且初始数据集中包含了所有重要特征。</p>
<p>要从初始的特征集中选取出一个包含所有重要信息的特征子集，如果没有任何领域知识作为先验假设，只能对所有特征子集进行遍历。但在计算上确是不可行的，因为一旦面对特征个数较多的情况，就会遇到组合爆炸的问题。</p>
<p>可行的方法是，寻找一个特征子集，对其进行评价。根据评价结果生成下一个特征子集直到无法找到更好的特征子集。</p>
<p>因此，可将特征子集选择拆分为两个环节：1. 子集搜索；2. 子集评价。</p>
<h3 id="子集搜索"><a href="#子集搜索" class="headerlink" title="子集搜索"></a>子集搜索</h3><p>从单特征子集开始，选择最优的特征，每轮增加一个当前情况下的最优特征，即贪心算法。在没有优于上一轮特征子集时停止生成候选子集，并选取上一轮的特征子集为特征选择结果。这样逐渐增加的策略被称为“<em>前向</em>”（forward）搜索。类似的，从后向前，逐渐减少特征的策略称为“<em>后向</em>”（backward）搜索。此外，可结合前向和后向搜索，每一轮逐渐增加选定相关特征，同时减少无关特征，这样的策略被称为“<em>双向</em>”（bidirectional）搜索。（分别对应于回归分析中的前向回归、后向回归和逐步回归）显然，这种贪心策略有可能会陷入局部最小，但若不进行穷举这种情况就无法避免。</p>
<h3 id="子集评价"><a href="#子集评价" class="headerlink" title="子集评价"></a>子集评价</h3><p>假定给定样本属性均为离散型的数据集$D$，且$D$中第$i$类样本所占比例为$p_i = (i=1,2,\dots,|\gamma|)$。对属性集$A$，假定根据$A$的取值将$D$分成了$V$个子集${D^1,D^2,\dots,D^V}$，每个子集中的样本在$A$上的取值相同，于是我们可以计算$A$的信息增益</p>
<script type="math/tex; mode=display">\text{Gain}(A) = \text{Ent}(D) - \sum^{V}_{v=1}\frac{|D^v|}{|D|} \text{Ent}(D^v)</script><p>其中信息熵定义为</p>
<script type="math/tex; mode=display">\text{Ent}(D) = -\sum^{|\gamma|}_{k=1} p_k\log p_k</script><p>信息增益$\text{Gain(A)}$越大，意味着特征子集$A$包含的有助于分类的信息越多。于是，可以使用信息增益作为特征子集的评价准则。</p>
<p>如果仅考察属性集$A$对$D$的划分能力，难以反映出其划分对学习任务的特殊性。于是，更一般地，对属性子集$A$对$D$的划分，和样本标记信息$Y$对$D$的真实划分，计算这两者之间的划分差异，就能对$A$进行评价。这是利用信息熵对特征子集作出的评价，对任意可以判断二者之间划分差异的机制都可以用于特征评价。（例如 8.4.2节中的多样性度量方法）</p>
<p>结合子集搜索与子集评价，即可得到特征选择方法，特别是前向搜索与信息熵结合，这就与决策树算法非常相似。</p>
<p>常见的特征选择方法可大致分为三类：<em>过滤式</em>（filter）、<em>包裹式</em>（wrapper）、<em>嵌入式</em>（embedding）。</p>
<h2 id="11-2-过滤式选择"><a href="#11-2-过滤式选择" class="headerlink" title="11.2 过滤式选择"></a>11.2 过滤式选择</h2><p>过滤式方法先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关。这相当于先用特征选择过程对初始特征进行过滤，再用过滤后的特征来训练模型。</p>
<p><strong>Relief</strong>（Relevant Feature）：是一种著名的过滤式特征选择方法，该方法设计了一个“相关统计量”来度量特征的重要性。该统计量是一个向量，其每个分量分别对应于一个初始特征，而特征子集的重要性则是由子集中每个特征所对应的相关统计量分量之和来决定。于是，最终只需指定一个阈值$\tau$，然后选择比$\tau$大的相关统计量分量对应的特征即可；也可指定欲选取的特征个数$k$，然后选择相关统计量分量最大的$k$个特征。</p>
<p>显然，Relief的关键是如何确定相关统计量。给定训练集${(x<em>1,y_1),(x_2,y_2),\dots,(x_m,y_m)}$，对每个示例$x_i$，Relief先在$x_i$的同类样本中寻找其最近邻$x</em>{i,\text{nh}}$，称为“<em>猜中近邻</em>”（near-hit），再从$x<em>i$的异类样本中寻找其最近邻$x</em>{i,\text{nm}}$，称为“<em>猜错近邻</em>”（near-miss）。然后，相关统计量对应于属性$j$的分量为</p>
<script type="math/tex; mode=display">\delta^j = \sum_i -\text{diff}(x_i^j,x_{i,nh}^j)^2 + \text{diff}(x_i^j,x_{i,nm}^j)^2</script><p>其中，$x_a^j$表示样本$x_a$在属性$j$上的取值，$\text{diff}(x_a^j,x_b^j)$取决于属性$j$的类型。若属性$j$是离散型，则$x^j_a = x^j_b$时$\text{diff}(x_a^j,x_b^j)=0$，否则为1；若属性$j$是连续型，则$\text{diff}(x_a^j,x_b^j) = |x^j_a-x^j_b|$，注意$x_a^j$和$x_b^j$已规范化到$[0,1]$区间。</p>
<p>结合上式分析，如果$x<em>i$与猜中近邻$x</em>{i,nh}$在属性$j$上的距离小于$x<em>i$与猜错近邻$x</em>{i,nm}$的距离，则说明属性$j$对区分同类与异类样本是有益的，于是增大属性$j$所对应的统计量分量；反之，若$x<em>i$与猜中近邻$x</em>{i,nh}$在属性$j$上的距离大于$x<em>i$与猜错近邻$x</em>{i,nm}$的距离，则说明属性$j$起负面作用，于是减小属性$j$所对应的统计量分量。最后，对基于不同样本得到的估计结果进行平均，就得到各属性的相关统计量分量，分量值越大，则对应属性的分类能力就越强。</p>
<p>上式中的$i$指出了用于平均的样本下标。实际上Relief只需在数据集的采样上而不必在整个数据集上估计相关统计量。显然，Relief的时间开销随采样次数以及原始特征数线性增长，因此是一个运行效率很高的过滤式特征算法。</p>
<p>Relief是为二分类问题设计的，其扩展变体Relief-F能处理多分类问题。假定数据集$D$中的样本来自$|\gamma|$各类别。对示例$x<em>i$，若它属于第$k$类（$k \in {1,2,\dots,|\gamma|}$），则Relief-F先在第$k$类之外的每个类中找到一个$x_i$的最近邻示例作为猜错近邻，记为$x</em>{i,l,m}$（$l = 1,2,\dots,|\gamma|$；$l \neq k$）。于是，相关统计量对应于属性$j$的分量为</p>
<script type="math/tex; mode=display">\delta^j = \sum_i -\text{diff}(x_i^j,x_{i,nh}^j)^2 + \sum_{l \neq k} \left (p_l \times \text{diff}(x_i^j,x_{i,nm}^j)^2 \right )</script><p>其中，$p_l$为第$l$类样本在数据集$D$中所占的比例。</p>
<h2 id="11-3-包裹式选择"><a href="#11-3-包裹式选择" class="headerlink" title="11.3 包裹式选择"></a>11.3 包裹式选择</h2><p>包裹式特征选择直接把最终将要使用的学习器的性能作为特征子集的评价准则，即目的就是为给定的学习器选择最有利于性能、量身定做的特征子集。</p>
<p>一方面，由于包裹式选择直接对给定学习器的性能进行优化，其性能表现要优于过滤式；另一方面，包裹式选择需要对学习器进行多次训练，所以时间复杂度较高。</p>
<p><strong>LVW</strong>（Las Vegas Wrapper）：一个典型的包裹式特征选择方法，其利用拉斯维加斯方法来进行子集搜索，以最终分类器的误差为特征子集评价准则。以下为LVW算法的描述</p>
<hr>
<p>输入：数据集$D$;<br>$~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~$特征集A;
$~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~$学习算法$\pounds$;<br>$~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~$停止条件控制参数$T$;
过程：
$E = \infty$;
$d = |A|$;
$A^* = A$;
$t = 0$;
**while** $t < T$ **do**
$~</script>~<script type="math/tex">~</script>~$随机产生特征子集$A’$;<br>$~<script type="math/tex">~</script>~<script type="math/tex">~</script>d’ = |A’|$;<br>$~<script type="math/tex">~</script>~<script type="math/tex">~</script>E’ = \text{CrossValidation}(\pounds(D^{A’}))$;<br>$~<script type="math/tex">~</script>~<script type="math/tex">~$**if**($E' < E$) $\vee$ (($E' = E$) $\wedge$ (d' < d)) **then**
$~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">t=0$;
$~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">E=E'$;
$~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">d=d'$;
$~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">A^*=A'$;
$~</script>~<script type="math/tex">~</script>~$<strong>else</strong><br>$~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~</script>~<script type="math/tex">~$t=t+1;
$~</script>~<script type="math/tex">~</script>~$<strong>end if</strong><br><strong>end while</strong></p>
<hr>
<p>需要注意的是，LVW的每次评价都需要重新训练学习器，且其特征子集搜索是随机的。所以一般需要为其设置一个最大未改变最优选择的轮数$T$，但如果在初始特征数很多、$T$设置很大时，若存在时间限制，则有可能无法给出解。</p>
<h2 id="10-4-嵌入式选择与正则化"><a href="#10-4-嵌入式选择与正则化" class="headerlink" title="10.4 嵌入式选择与正则化"></a>10.4 嵌入式选择与正则化</h2><p>区别于过滤式和包裹式清楚地将特征选择和学习器训练过程分开，嵌入式特征选择和学习器训练过程融为一体。</p>
<p>对给定的数据集$D={(x_1, y_1), (x_2, y_2), \dots, (x_m, y_m)}$，其中$x \in R$，$y \in R$。考虑最简单的线性回归模型，以平方误差为损失函数，则优化回归目标为</p>
<script type="math/tex; mode=display">\min_w \sum^m_{i=1} (y_i - w^Tx_i)^2</script><p>在样本特征很多而样本数相对较少时，上式易陷入过拟合，为缓解过拟合问题，为其引入正则化项。若使用$L_2$范数正则化，则有</p>
<script type="math/tex; mode=display">\min_w \sum^m_{i=1}(y_i - w^Tx_i)^2 + \lambda \left \| w \right \|^2_2</script><p>其中正则化参数$\lambda &gt; 0$，上式称为“<em>岭回归</em>”（ridge regression），引入$L_2$范数正则化了以后，能显著地降低过拟合的风险。</p>
<p>如果将$L_2$范数替换为$L_1$范数，即</p>
<script type="math/tex; mode=display">\min_w \sum(y_i - w^Tx_i)^2 +\lambda \left \| w \right \|_1</script><p>此时得到的式子被称为<em>LASSO</em>（Least Absolute Shrinkage and Selection Operation）</p>
<p>并且，$L_1$范数比起$L_2$范数还有另外的优势：它更易于获得“<em>稀疏</em>”（sparse）解，即所得解$w$具有更少的非零分量。可通过图像来理解这一点：</p>
<p>在取得稀疏解$w$之后，可以发现，此时仅使用了一部分初始特征。所以，也就实现了嵌入式的特征选择方法。</p>
<p>对$L_1$正则化问题的求解，可使用<em>近端梯度下降</em>（Proximal Gradient Descent）。令$\triangledown$表示微分算子，对优化目标</p>
<script type="math/tex; mode=display">\min_x f(x) + \lambda\left \| x \right \|_1</script><p>$f(x)$可导，$\triangledown$满足L-Lipschitz条件$^{[1]}$，即存在常数$L &gt; 0$，使得</p>
<script type="math/tex; mode=display">\left \| \triangledown f(x') - \triangledown f(x) \right \|^2_2 \ \ (\forall x, x')</script><p>在$x_k$附近将$f(x)$通过泰勒展开式近似为</p>
<script type="math/tex; mode=display">\begin{aligned} \hat f(x) &= f(x_k) + \left \| x-x_k \right \|f'(x_k) + \frac{\left \|x-x_k \right \|^2}{2}f''(x_k) \\ &\simeq f(x_k) + \left \langle \triangledown f(x_k), x - x_k \right \rangle + \frac{L}{2}\left \| x-x_k \right \|^2 \\ &= \frac{L}{2}\left \| x - \left ( x_k - \frac{1}{L}\triangledown f(x_k) \right )\right \|^2_2 + \text{const} \end{aligned}</script><p>其中$\text{const}$是与$x$无关的常数，$\left \langle \cdot, \cdot \right \rangle$表示内积。显然上式的最小值在如下$x_{k+1}$获得：</p>
<script type="math/tex; mode=display">x_{k+1} = x_k - \frac{1}{L}\triangledown f(x_k)</script><p>于是，通过梯度下降法对$f(x)$进行最小化时，每一步梯度下降迭代实际上等价于最小化二次函数$\hat f(x)$，将这个思想扩展到我们的优化目标中，可以得到</p>
<script type="math/tex; mode=display">x_{k+1} = \argmin_x \frac{L}{2}\left \| x-\left ( x_k - \frac{1}{L}\triangledown f(x_k)\right ) \right \|^2_2 + \lambda \left \|x \right \|_1</script><p>即在每一步对$f(x)$进行梯度下降迭代的同时，考虑对$L_1$范数的最小化。</p>
<p>对于上式，可先计算$z = x_k - \frac{1}{L}\triangledown f(x_k)$，然后求解</p>
<script type="math/tex; mode=display">x_{k+1} = \argmin_x \frac{L}{2}\left \| x-z \right \|^2_2 + \lambda \left \|x \right \|_1</script><p>令$x^i$表示$x$的第$i$个分量，将上式按分量展开可看出，其中不存在$x^ix^j(i \neq j)$这样的项，即$x$的各个分量互不影响，于是有闭式解</p>
<script type="math/tex; mode=display">x^i_{k+1}\begin{cases} z^i - \lambda / L, & \lambda / L < z^i; \\ 0, & |z^i| \leqslant \lambda / L; \\ z^i + \lambda / L, & z^i < -\lambda / L \end{cases}</script><p>其中$x^i<em>{k+1}$与$z^i$分别是$x</em>{k+1}$与$z$的第$i$个分量。因此，通过PGD能使LASSO和其他基于$L_1$范数最小化的方法得以快速求解。</p>
<h2 id="11-5-稀疏表示与字典学习"><a href="#11-5-稀疏表示与字典学习" class="headerlink" title="11.5 稀疏表示与字典学习"></a>11.5 稀疏表示与字典学习</h2><p>将数据集$D$考虑成一个矩阵，每行对应于一个样本，每列对应于一个特征。特征选择的目的是，将学习任务限制在仅与学习任务有关的列构成的矩阵上，以降低学习任务的难度，可以涉及的计算和存储开销，学得模型的可解释性也会提高。</p>
<p>针对于类似文档中汉字的稀疏矩阵，矩阵每一行都有大量零元素，并且针对不同的文档，零元素的出现往往很不相同。</p>
<p>样本具有这样的稀疏形式时，将使大多数问题变得线性可分。同时，由于稀疏矩阵有一些高效的存储方法，也就不会造成存储上的巨大负担。</p>
<p>对给定数据集$D$，此时$D$稠密的，将其转化为<em>稀疏表示</em>（sparse representation）形式时，需要注意做到“恰当稀疏”，避免“过度稀疏”。而在一般的学习任务中并没有将稠密矩阵转化为稀疏表示的“字典”或“<em>码书</em>”（codebook），因而需要学习出一个这样的字典，这个过程被称为“<em>字典学习</em>”（dictionary learning）或“<em>码书学习</em>”（codebook learning），也被称为<em>稀疏编码</em>（sparse coding）。（二者在侧重上略有不同）</p>
<p>给定数据集${x_1, x_2, \dots, x_m}$，字典学习最简单的形式为</p>
<script type="math/tex; mode=display">\min_{B, \alpha_i} \sum^m_{i=1} \left \| x_i - B\alpha_i \right \|^2_2 + \lambda \sum^m_{i=1}\left \| \alpha_i \right \|_1</script><p>其中，$B \in R^{d \times k}$为字典矩阵，$k$称为字典的词汇量，通常由用户指定，$\alpha_i \in R^k$是样本$x_i \in R^d$的稀疏表示。对上式可采用变量交替优化的方式求解。显然，第一项是对重构的正则化，第二项是对稀疏性的正则化：</p>
<ol>
<li>固定字典$B$，将上式按分量展开，可看出其中不包含$a<em>i^ua_i^v$这样的交叉项，因此，类似对LASSO的求解，可求出对应于$x_i$的$\alpha_i$ $$\min</em>{\alpha_i} \left | x_i - B \alpha_i \right |^2_2 + \lambda\left | \alpha_i \right |_1$$</li>
<li>以$\alpha_i$为初值更新字典$B$（$\left | \cdot \right |_F$是Frobenius范数，即矩阵上的$L_2$范数）<script type="math/tex">\min_B \left \| X - BA \right \|^2_F</script></li>
</ol>
<p>有多种方法可对上式求解，常用的有基于逐列更新策略的KSVD。令$b_i$表示字典矩阵$B$的第$i$列，$\alpha^i$表示稀疏矩阵$A$的第$i$行，从而将上式重写为</p>
<script type="math/tex; mode=display">\begin{aligned} \min_B \left \| X-BA \right \|^2_F &= \min_{b_i} \left \| X-\sum^k_{j=1}b_j\alpha^j \right \|^2_F \\ &= \min_{b_i} \left \| \left ( X - \sum_{j\neq i}b_j \alpha^j \right ) - b_i \alpha^i \right \|^2_F \\ &= \min_{b_i} \left \| E_i - b_i \alpha^i \right \|^2_F \end{aligned}</script><p>进行逐列更新至第$i$列时，其他列是固定的，所以$E<em>i = X - \sum</em>{j \neq i} b_j \alpha^j$是固定的。于是，求解上式仅需对$E_i$进行奇异值分解以取得最大奇异值对应的正交向量。但是，如果直接对$E_i$进行奇异值分解会改变$b_i$和$\alpha^i$，将可能破坏第一步中$A$的稀疏性，因此KSVD对$E_i$和$\alpha^i$进行专门处理：$\alpha_i$仅保留非零元素，$E_i$仅保留$b_i$与$\alpha^i$的非零元素的乘积项，再进行奇异值分解，从而保持其稀疏性。</p>
<p>初始化字典矩阵$B$之后反复迭代上述两步，最终即可求得字典$B$和样本$x_i$的稀疏表示。过程中可通过设置词汇量$k$的大小来控制字典规模，从而影响稀疏程度。</p>
<h2 id="11-6-压缩感知"><a href="#11-6-压缩感知" class="headerlink" title="11.6 压缩感知"></a>11.6 压缩感知</h2><p><strong>奈奎斯特采样定理</strong>：令采样频率达到模拟信号最高频率的两倍，则采样后的数字信号就保留了模拟信号的全部信息。（意味着这是信号恢复的充分条件而非必要条件）</p>
<p>在实践中，为方便存储、传输，往往需要对数字信号进行压缩，这可能会损失一定的信息，同时传输时也容易出现丢包问题，从而进一步损失信息。这时，需要通过接收到的戴护具精确地重构出原信号，<em>压缩感知</em>（compressed sensing）为该类提供新思路。</p>
<p>假定有长度为$m$的离散信号$x$，以远小于奈奎斯特采样定理要求的采样率进行采样，得到长度为$n$的采样后信号$y$，$n \ll m$，即</p>
<script type="math/tex; mode=display">y = \Phi x</script><p>其中$\Phi \in R^{n \times m}$是对信号$x$的测量矩阵，确定采样的频率和如何将采样样本组成采样后的信号。在将测量值$y$和测量矩阵$\Phi$传输出去后，在一般情况下，无法仅通过它们还原出原始信号$x$。</p>
<p>那么，不妨设存在某线性变换$\Psi \in R^{m \times m}$，使得$x$可表示为$\Psi s$，$y$可表示为</p>
<script type="math/tex; mode=display">y = \Phi \Psi s = As</script><p>其中$A=\Phi \Psi \in R^{n \times m}$，所以只要能通过$y$恢复出$s$，即可通过$x = \Psi s$恢复出信号$x$。看起来问题没有得到简化，因为，$y = \Phi x$和$y = \Phi \Psi s$都是欠定的，无法轻易求出数值解。但对于后者而言，若其中$s$具有稀疏性，则这个问题就能得以很好地解决，此时式中的$\Psi$称为稀疏基，$A$就类似于字典。</p>
<p>压缩感知关注的是如何利用信号本身所具有的稀疏性，从部分观测样本中恢复原信号。通常认为，压缩感知分为“感知测量”和“重构恢复”两个阶段：</p>
<p><strong>感知测量</strong> 关注如何对原始信号进行处理以获得稀疏样本表示，涉及傅里叶变换、小波变换、字典学习和稀疏编码等。</p>
<p><strong>重构恢复</strong> 关注如何基于稀疏性从少量观测中恢复原信号，当谈压缩感知时，通常指该部分。</p>
<p>压缩感知相关理论比较复杂，下面仅简要介绍“<em>限定等距性</em>”（Restricted Isometry Property，简称RIP）。</p>
<h3 id="限定等距性"><a href="#限定等距性" class="headerlink" title="限定等距性"></a>限定等距性</h3><p>对大小为$n \times m$（$n \ll m$）的矩阵$A$，若存在常数$\delta_k \in (0,1)$，使得对任意向量$s$和$A$的所有子矩阵$A_k \in R^{n \times k}$有</p>
<script type="math/tex; mode=display">(1-\delta_k) \left \| s \right \|^2_2 \leqslant \left \| A_ks \right \|^2_2 \leqslant (1+\delta_k)\left \| s \right \|^2_2</script><p>则称$A$满足$k$限定等距性($k$-RIP)。此时可通过下面的优化问题近乎完美地从$y$中恢复出稀疏信号$s$，进而恢复出$x$：</p>
<script type="math/tex; mode=display">\begin{aligned} \min_s \left \| s \right \|_0 \\ \text{s.t.  } \ y = As \end{aligned}</script><p>但上式中涉及$L_0$范数$^{[2]}$最小化，$L_0$范数非凸，这是NP难问题。不过，在一定条件下，$L_0$范数最小化与$L_1$范数最小化具有共解，因此只需要关注</p>
<script type="math/tex; mode=display">\begin{aligned} \min_s \left \| s \right \|_1 \\ \text{s.t.  } \ y = As \end{aligned}</script><p>这样，压缩感知问题就可通过$L_1$范数最小化问题求解，例如上式可转化为LASSO的等价形式再通过近端梯度下降法求解，即使用“<em>基寻踪去噪</em>”（Basis Pursuit De-Noising）。</p>
<p>对“<em>协同过滤</em>”（colalborative filtering）任务，通过压缩感知恢复欠采样信号（即含缺失值样本）的前提条件之一是信号有稀疏表示。</p>
<p><em>矩阵补全</em>（matrix completion）技术可用于解决这个问题，其形式为</p>
<script type="math/tex; mode=display">\begin{aligned} &\min_x \text{rank}(X) \\ &\text{s.t.  } \ (X)_{ij} = (A)_{ij}, \ \ (i,j)\in \Omega \end{aligned}</script><p>其中$X$是需要恢复的稀疏信号，$\text{rank}(X)$表示矩阵的秩。$A$是已观测信号，$\Omega$是$A$中已观测元素$(A)_{ij}$的下标集合。上式的约束中明确指出，恢复出的$X$矩阵与已观测到的对应元素相同。</p>
<p>但上式依旧是NP难问题，而$\text{rank}(X)$在集合${x \in R^{m \times n}: \left | X \right |^2_F \leqslant 1}$上的凸包是$X$的“<em>核范数</em>”（nuclear norm）：</p>
<script type="math/tex; mode=display">\left \| X \right \|_* = \sum^{\min\{m,n\}}_{j=1} \sigma_j(X)</script><p>其中$\sigma_j(X)$表示$X$的奇异值，即矩阵的核范数为矩阵的奇异值之和，于是可通过最小化矩阵核范数来近似求解原问题，即</p>
<script type="math/tex; mode=display">\begin{aligned} &\min_X \left \| X \right \|_*  \\ &\text{s.t.  } (X)_{ij} = (A)_{ij}, \ \ (i,j) \in \Omega \end{aligned}</script><p>上式是一个凸优化问题，可通过<em>半正定规划</em>（Semi-Definite Programming，简称SDP）求解。理论研究表明，在满足一定条件时，若$A$的秩为$r$，$n \ll m$，则只需观察到$O(mrlog^2m)$个元素就能完美恢复出$A$。</p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="1-L-Lipschitz条件"><a href="#1-L-Lipschitz条件" class="headerlink" title="[1] L-Lipschitz条件"></a>[1] L-Lipschitz条件</h3><p>如果函数$\phi(x)$，在有限区间$[a,b]$上满足：</p>
<ul>
<li>当$x \in [a,b]$时，$\phi(x) \in [a,b]$</li>
<li>对任意的$x_1$，$x_2 \in [a,b]$，恒成立：$|\phi(x_1) - \phi(x_2)| \leqslant L|x_1 - x_2|$</li>
</ul>
<p>则称函数$\phi(x)$在区间$[a,b]$上满足Lipschitz条件，其中$L$被称为Lipschitz常数。</p>
<p>利普希茨连续条件是一个比一致连续更强的光滑性条件，直观而言，利普希茨连续函数限制了函数改变的速度，是函数斜率必小于利普希茨常数。</p>
<p>在微分方程理论中，利普希茨条件是初值条件下解的存在唯一性定理中的一个核心条件。</p>
<h3 id="2-L-0-范数"><a href="#2-L-0-范数" class="headerlink" title="[2] $L_0$范数"></a>[2] $L_0$范数</h3><p>$L_0$范数表示向量中非零元素的个数，即稀疏度。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.zzforgood.top/2019/10/17/Chapter11%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" data-id="ckvqbnyce0004e4oj0ipmgm4o" class="article-share-link">
        Share
      </a>
      
<ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B/" rel="tag">《机器学习》</a></li></ul>

    </footer>

  </div>

  
  
<nav class="article-nav">
  
  <a href="/2021/10/26/%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95%E8%87%AA%E5%8A%A8%E5%81%A5%E5%BA%B7%E5%A1%AB%E6%8A%A5%E8%84%9A%E6%9C%AC/" class="article-nav-link">
    <strong class="article-nav-caption">Newer</strong>
    <div class="article-nav-title">
      
      模拟登录自动健康填报脚本
      
    </div>
  </a>
  
  
  <a href="/2019/10/05/Chapter10%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" class="article-nav-link">
    <strong class="article-nav-caption">Older</strong>
    <div class="article-nav-title">Chapter10 降维与度量学习</div>
  </a>
  
</nav>

  

  
  
  
<div class="gitalk" id="gitalk-container"></div>

<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">


<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>


<script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>

<script type="text/javascript">
  var gitalk = new Gitalk({
    clientID: 'b9cfa880780f6dc246f0',
    clientSecret: 'eece6422d853f25460f68edcaa0506ce9f0b30a8',
    repo: 'lyzsj114.github.io',
    owner: 'lyzsj114',
    admin: ['lyzsj114'],
    // id: location.pathname,      // Ensure uniqueness and length less than 50
    id: md5(location.pathname),
    distractionFreeMode: false,  // Facebook-like distraction free mode
    pagerDirection: 'last'
  })

  gitalk.render('gitalk-container')
</script>

  

</article>
</section>
    <footer class="footer">
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
  <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span></li>
  
  <li><i class="fe fe-bookmark"></i> <span id="busuanzi_value_page_pv"></span></li>
  
</ul>
    </div>
    <ul class="list-inline">
      <li>GeniusGrass&#39;s Blog &copy; 2021</li>
      
        <li>GENIUSGRASS</li>
      
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
      <li>theme  <a target="_blank" rel="noopener" href="https://github.com/zhwangart/hexo-theme-ocean">Ocean</a></li>
    </ul>
  </div>
</footer>
  </main>
  <aside class="sidebar">
    <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/hexo.svg" alt="GeniusGrass&#39;s Blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">Home</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">Archives</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/gallery">Gallery</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">About</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="Search">
        <i class="fe fe-search"></i>
        Search
      </a>
    </li>
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
  <i class="fe fe-rocket"></i>
</div>
    </li>
    <li class="nav-item">
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="fe fe-feed"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
  </aside>
  
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>



<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/copybtn.js"></script>





<script src="/js/tocbot.min.js"></script>

<script>
  // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto',
  });
</script>



<script src="/js/ocean.js"></script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

</html>